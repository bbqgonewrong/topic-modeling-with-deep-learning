{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicLayer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "csW_UCbQIJYh"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1WzzRRmWDgw"
      },
      "source": [
        "##Importing all the libraries required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA-tVEKI_pCS"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.models import Sequential,Model\n",
        "from tensorflow.python.keras.layers import Dense,AveragePooling2D,Dropout,Flatten,Dense,Input,Concatenate,GlobalAveragePooling2D\n",
        "from google.colab.patches import cv2_imshow,cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import shutil\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from google.colab import files\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,TensorBoard,LambdaCallback\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from glob import glob\n",
        "import shutil\n",
        "import json\n",
        "from json import JSONEncoder\n",
        "import seaborn as sn\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "#import tensorflow_addons as tfa\n",
        "#from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_K5hdoIfVH0"
      },
      "source": [
        "%pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSC7U1IbOdIm"
      },
      "source": [
        "\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5AI_I5NbLdt"
      },
      "source": [
        "## Mounting onto google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDCnT7wYSxKH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkedkIuJCCAf"
      },
      "source": [
        "#New implementation for model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJAy9pwvF2Oq"
      },
      "source": [
        "Full code below .. dont run this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpvS0214oYa"
      },
      "source": [
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(128,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "y = model.output\n",
        "k_1 = Dense(1,name = 'topic_1')(y)\n",
        "k_2 = Dense(1,name = 'topic_2')(y)\n",
        "k_3 = Dense(1,name = 'topic_3')(y)\n",
        "k_4 = Dense(1,name = 'topic_4')(y)\n",
        "conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "#Adding the Dense layers now\n",
        "topic = Dense(256,activation='relu')(flat_2)\n",
        "topic = Dropout(0.2)(topic)\n",
        "topic = Dense(64,activation='relu')(topic)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "cent_model_1 = Model(inputs = base_model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuH3J2mxFJD5"
      },
      "source": [
        "cent_model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKWZtpuGGIDL"
      },
      "source": [
        "train_meta = pd.read_csv('/content/gdrive/MyDrive/train_names.csv')\n",
        "val_meta = pd.read_csv('/content/gdrive/MyDrive/val_names.csv')\n",
        "test_meta = pd.read_csv('/content/gdrive/MyDrive/test_names.csv')\n",
        "train_meta.columns = ['ID','Path']\n",
        "train_met = train_meta.Path\n",
        "val_meta.columns = ['ID','Path']\n",
        "val_met = val_meta.Path\n",
        "train_met = pd.DataFrame(train_met)\n",
        "val_met = pd.DataFrame(val_met)\n",
        "tv = [train_met,val_met]\n",
        "tnv = pd.concat(tv)\n",
        "tnv = tnv.sample(frac=1,random_state = 42)\n",
        "\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir('/content/gdrive/MyDrive/data/test')]))\n",
        "len(class_names)\n",
        "\n",
        "def kfold_label_maker(meta):\n",
        "\n",
        "  meta['PicName'] = meta['Path'].apply(lambda x: x.split('/')[-1])\n",
        "  meta['Label'] = meta['Path'].apply(lambda x: x.split('/')[-2])\n",
        "  #meta['Image'] = meta['Image'].str.replace('/content/gdrive/MyDrive','/data/s4133366')\n",
        "  kfold = meta[['Path','Label']]\n",
        "  return kfold\n",
        "\n",
        "train = kfold_label_maker(tnv)\n",
        "train = kfold_label_maker(tnv)\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "\n",
        "\n",
        "Y = train['Label']\n",
        "X = train['Path']\n",
        "num_folds = 4\n",
        "batch_size = 64\n",
        "skfold = StratifiedKFold(n_splits=num_folds, shuffle=True,random_state=42)\n",
        "fold_no = 1\n",
        "history_path = '/content/gdrive/MyDrive/history'\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "save_dir = '/content/gdrive/MyDrive/saved_models'\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  training_data = train.iloc[train_index]\n",
        "  valid_data = train.iloc[val_index]\n",
        "  print(len(training_data))\n",
        "  print(len(valid_data))\n",
        "  train_data = training_data['Path']\n",
        "  train_labels = training_data['Label']\n",
        "  val_labels = valid_data['Label']\n",
        "  val_data = valid_data['Path']\n",
        "  train_tensor = tf.data.Dataset.from_tensors(train_data)\n",
        "  val_tensor = tf.data.Dataset.from_tensors(val_data)\n",
        "  train_tensor.shuffle(len(train_data))\n",
        "  val_tensor.shuffle(len(val_data))\n",
        "  print(train_tensor)\n",
        "  tra_tens = train_tensor.unbatch().map(process_TL)\n",
        "  val_tens = val_tensor.unbatch().map(process_TL)\n",
        "  print(tra_tens,' ',tf.data.experimental.cardinality(tra_tens))\n",
        "  train_cache = tra_tens.cache()\n",
        "  val_cache = val_tens.cache()\n",
        "  #normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "  val_batch = val_tens.batch(batch_size)\n",
        "  train_batch= tra_tens.batch(batch_size)\n",
        "  train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  print(train_ds)\n",
        "  \n",
        "\n",
        "  # class_weights = class_weight.compute_class_weight(\n",
        "  #          'balanced',\n",
        "  #           np.unique(ktrainGen.classes), \n",
        "  #           ktrainGen.classes)\n",
        "\n",
        "  # train_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "  ##############                                    #################\n",
        "  #####                   TOPIC MODEL WITH 1 UNIT     #################\n",
        "  ###################################################################\n",
        "  ##############                                    #################\n",
        "  ##############                                    ################# \n",
        "\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "  x = base_model.output\n",
        "  x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "  x = Flatten(name =\"flatten\")(x)\n",
        "  x = Dense(128,activation=\"relu\")(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "  #Activating the model\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  y = model.output\n",
        "  k_1 = Dense(1,name = 'topic_1')(y)\n",
        "  k_2 = Dense(1,name = 'topic_2')(y)\n",
        "  k_3 = Dense(1,name = 'topic_3')(y)\n",
        "  k_4 = Dense(1,name = 'topic_4')(y)\n",
        "  conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "  flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "  #Adding the Dense layers now\n",
        "  topic = Dense(256,activation='relu')(flat_2)\n",
        "  topic = Dropout(0.2)(topic)\n",
        "  topic = Dense(64,activation='relu')(topic)\n",
        "  topic = Dropout(0.2)(topic)\n",
        "  final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "  cent_model_1 = Model(inputs = base_model.input,outputs = final)\n",
        "  lr = 1e-1\n",
        "  num_epochs = 1\n",
        "  opti = tf.keras.optimizers.Adadelta(learning_rate=lr, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "  cent_model_1.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')\n",
        "\n",
        "  callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        \n",
        "        filepath=os.path.join(save_dir,'top_m_n1_'+str(fold_no)),\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"accuracy\",\n",
        "        verbose=1,\n",
        "        save_freq = 442,mode = 'max'\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    ),\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=3, min_lr=1e-5,verbose=1)\n",
        "    ]\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  history = cent_model_1.fit(train_ds,epochs=num_epochs,callbacks=callbacks,verbose=1,workers=8,use_multiprocessing = False,shuffle=True)\n",
        "  cent_model_1.save('/content/gdrive/MyDrive/saved_models/topic_1_neuro_kfold_'+str(fold_no))\n",
        "  report = history.history\n",
        "  np.save(os.path.join(history_path,'history'+str(fold_no)+'.npy'), history)\n",
        "  print('Results saved to disk ...')\n",
        "  print('Evaluating the model ...') \n",
        "  cent_model_1 = keras.models.load_model(\"/content/gdrive/MyDrive/saved_models/topic_1_neuro_kfold_\"+str(fold_no))\n",
        "  results = cent_model_1.evaluate(val_ds,workers = 4)\n",
        "  results = dict(zip(cent_model_1.metrics_names,results))\n",
        "  np.save(os.path.join(history_path,'results_'+str(fold_no)+'.npy'), results)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  # tf.keras.backend.clear_session()\n",
        "\n",
        "  #fold_no += 1\n",
        "\n",
        "  print('Going to next fold................')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYMsDofEi54j"
      },
      "source": [
        " def return_dataframes(file): \n",
        "  test_labels = pd.read_csv(file)\n",
        "  df_list = []\n",
        "  df_list_names = []\n",
        "  class_names_meta = test_labels['Label'].unique()\n",
        "  class_names_meta\n",
        "  len(class_names_meta)\n",
        "  sorted(class_names_meta)\n",
        "  for class_name in class_names_meta:\n",
        "    class_name_rep = class_name.replace(\" \",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\"/\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\" \",\"\")\n",
        "    class_name_rep = class_name_rep.replace(\",_\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\",\",\"_\")\n",
        "    #print(class_name_rep)\n",
        "    h = \"df_\" + str(class_name_rep)\n",
        "    df_name = h\n",
        "    \n",
        "    vars()[h] = test_labels[test_labels['Label'] == class_name]\n",
        "    print(\"Created: \",h,' ',len(vars()[h]))\n",
        "    print()\n",
        "    df_list.append(vars()[h])\n",
        "    df_list_names.append(df_name)\n",
        "  return df_list,df_list_names\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGPe-UFTYp_"
      },
      "source": [
        "stuff for small data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Se-SUXLU0LE"
      },
      "source": [
        " def return_dataframes(file): \n",
        "  test_labels = pd.read_csv(file)\n",
        "  df_list = []\n",
        "  df_list_names = []\n",
        "  class_names_meta = test_labels['Label'].unique()\n",
        "  class_names_meta\n",
        "  len(class_names_meta)\n",
        "  sorted(class_names_meta)\n",
        "  for class_name in class_names_meta:\n",
        "    class_name_rep = class_name.replace(\" \",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\"/\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\" \",\"\")\n",
        "    class_name_rep = class_name_rep.replace(\",_\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\",\",\"_\")\n",
        "    #print(class_name_rep)\n",
        "    h = \"df_\" + str(class_name_rep)\n",
        "    df_name = h\n",
        "    \n",
        "    vars()[h] = test_labels[test_labels['Label'] == class_name]\n",
        "    print(\"Created: \",h,' ',len(vars()[h]))\n",
        "    print()\n",
        "    df_list.append(vars()[h])\n",
        "    df_list_names.append(df_name)\n",
        "  return df_list,df_list_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dCoyqXXTYBV"
      },
      "source": [
        "def get_small_ds(path):\n",
        "  df,name = return_dataframes(path)\n",
        "  \n",
        "  df_Office_updated = df_Office.sample(3000)\n",
        "  df_Transportation_updated = df_Transportation.sample(3000)\n",
        "  df_Education_science_updated = df_Education_science.sample(3000)\n",
        "  df_Pathways_updated = df_Pathways.sample(3000)\n",
        "  df_Restaurant_Bar_updated = df_Restaurant_Bar.sample(3000)\n",
        "\n",
        "  df_list = [df_Bedroom,df_Office_updated,df_Other_rooms,df_Living_room,df_Noise,df_Bathroom,df_Pathways_updated,df_Shop,df_Buildings,df_Others,df_Sport_fields,df_Kitchen,df_Garden,df_Balcony,df_Beach,df_Recreation,df_Museum,df_Hospital,df_Water,df_Forest_field_jungle,df_Mountains_hills_desert_sky,df_Transportation_updated,df_Education_science_updated,df_Restaurant_Bar_updated]\n",
        "  train_df = pd.concat(df_list)\n",
        "  len(train_df)\n",
        "\n",
        "  tr_df = train_df \n",
        "  num_epochs = 10\n",
        "  tr_df['Image'] = tr_df['Image'].str.replace('/data/s4133366/data','/content/gdrive/MyDrive/data')\n",
        "  tr_df = shuffle(tr_df)\n",
        "  return tr_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk-jZdrWeXhM"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "test_labels = pd.read_csv('/content/val.csv')\n",
        "class_names_meta = test_labels['Label'].unique()\n",
        "class_names_meta\n",
        "len(class_names_meta)\n",
        "sorted(class_names_meta)\n",
        "for class_name in class_names_meta:\n",
        "  class_name_rep = class_name.replace(\" \",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\"/\",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\" \",\"\")\n",
        "  class_name_rep = class_name_rep.replace(\",_\",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\",\",\"_\")\n",
        "  #print(class_name_rep)\n",
        "  h = \"df_\" + str(class_name_rep)\n",
        "  vars()[h] = test_labels[test_labels['Label'] == class_name]\n",
        "  print(\"Created: \",h,' ',len(vars()[h]))\n",
        "df_Office_updated = df_Office.sample(1500)\n",
        "df_Transportation_updated = df_Transportation.sample(1500)\n",
        "df_Education_science_updated = df_Education_science.sample(1500)\n",
        "df_Pathways_updated = df_Pathways.sample(1500)\n",
        "df_Restaurant_Bar_updated = df_Restaurant_Bar.sample(1500)\n",
        "\n",
        "df_list = [df_Bedroom,df_Office_updated,df_Other_rooms,df_Living_room,df_Noise,df_Bathroom,df_Pathways_updated,df_Shop,df_Buildings,df_Others,df_Sport_fields,df_Kitchen,df_Garden,df_Balcony,df_Beach,df_Recreation,df_Museum,df_Hospital,df_Water,df_Forest_field_jungle,df_Mountains_hills_desert_sky,df_Transportation_updated,df_Education_science_updated,df_Restaurant_Bar_updated]\n",
        "train_df = pd.concat(df_list)\n",
        "print(len(train_df))\n",
        "\n",
        "\n",
        "#num_epochs = 10\n",
        "tr_df['Image'] = tr_df['Image'].str.replace('/data/s4133366/data','/content/gdrive/MyDrive/data')\n",
        "#tr_df = shuffle(tr_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9hgRYn1_v3a"
      },
      "source": [
        "train_df = train_df.sample(frac=1)\n",
        "len(train_df)\n",
        "train_df.to_csv('val_short_collab.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XN3aZEHe22l"
      },
      "source": [
        "train_df['Image'] = train_df['Image'].str.replace('/data/s4133366/data','/content/gdrive/MyDrive/data')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGrHlQwB4FhB"
      },
      "source": [
        "##Topic model with small data_set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy2WlnabguI5"
      },
      "source": [
        "###################\n",
        "#Using small df####\n",
        "###################\n",
        "cache_file_t = '/content/gdrive/MyDrive/cache_train'\n",
        "cache_file_v = '/content/gdrive/MyDrive/cache_val'\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir('/content/gdrive/MyDrive/data/test')]))\n",
        "num_folds = 4\n",
        "batch_size = 64\n",
        "skfold = StratifiedKFold(n_splits=num_folds, shuffle=True,random_state=42)\n",
        "fold_no = 1\n",
        "\n",
        "history_path = '/content/gdrive/MyDrive/history'\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "save_dir = '/content/gdrive/MyDrive/saved_models'\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "\n",
        "\n",
        "Y = tr_df['Label']\n",
        "X = tr_df['Image']\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  training_data = tr_df.iloc[train_index]\n",
        "  valid_data = tr_df.iloc[val_index]\n",
        "  print(len(training_data))\n",
        "  print(len(valid_data))\n",
        "  train_data = training_data['Image']\n",
        "  train_labels = training_data['Label']\n",
        "  val_labels = valid_data['Label']\n",
        "  val_data = valid_data['Image']\n",
        "  train_tensor = tf.data.Dataset.from_tensors(train_data)\n",
        "  val_tensor = tf.data.Dataset.from_tensors(val_data)\n",
        "  train_tensor.shuffle(len(train_data))\n",
        "  val_tensor.shuffle(len(val_data))\n",
        "  print(train_tensor)\n",
        "  tra_tens = train_tensor.unbatch().map(process_TL)\n",
        "  val_tens = val_tensor.unbatch().map(process_TL)\n",
        "  print(tra_tens,' ',tf.data.experimental.cardinality(tra_tens))\n",
        "  train_cache = tra_tens.cache(cache_file_t)\n",
        "  val_cache = val_tens.cache(cache_file_v)\n",
        "  #normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "  val_batch = val_tens.batch(batch_size)\n",
        "  train_batch= tra_tens.batch(batch_size)\n",
        "  train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  print(train_ds)\n",
        "  \n",
        "\n",
        "  # class_weights = class_weight.compute_class_weight(\n",
        "  #          'balanced',\n",
        "  #           np.unique(ktrainGen.classes), \n",
        "  #           ktrainGen.classes)\n",
        "\n",
        "  # train_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "  ##############\n",
        "  ##############\n",
        "  #####TOPIC MODEL\n",
        "  ###########################################################\n",
        "  ##############\n",
        "  ##############\n",
        "\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "  x = base_model.output\n",
        "  x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "  x = Flatten(name =\"flatten\")(x)\n",
        "  x = Dense(128,activation=\"relu\")(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "  #Activating the model\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  y = model.output\n",
        "  k_1 = Dense(1,name = 'topic_1')(y)\n",
        "  k_2 = Dense(1,name = 'topic_2')(y)\n",
        "  k_3 = Dense(1,name = 'topic_3')(y)\n",
        "  k_4 = Dense(1,name = 'topic_4')(y)\n",
        "  conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "  flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "  #Adding the Dense layers now\n",
        "  topic = Dense(256,activation='relu')(flat_2)\n",
        "  topic = Dropout(0.2)(topic)\n",
        "  topic = Dense(64,activation='relu')(topic)\n",
        "  topic = Dropout(0.2)(topic)\n",
        "  final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "  cent_model_1 = Model(inputs = base_model.input,outputs = final)\n",
        "  lr = 1e-1\n",
        "  num_epochs = 1\n",
        "  opti = tf.keras.optimizers.Adadelta(learning_rate=lr, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "  cent_model_1.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')\n",
        "\n",
        "  callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        \n",
        "        filepath=os.path.join(save_dir,'top_m_n1_'+str(fold_no)),\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"categorical_accuracy\",\n",
        "        verbose=1,\n",
        "        save_freq = 442,mode = 'max'\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    ),\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=3, min_lr=1e-5,verbose=1)\n",
        "    ]\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  history = cent_model_1.fit(train_ds,epochs=num_epochs,callbacks=callbacks,verbose=1,workers=8,use_multiprocessing = False,shuffle=True)\n",
        "  cent_model_1.save('/content/gdrive/MyDrive/saved_models/topic_1_neuro_kfold_'+str(fold_no))\n",
        "  report = history.history\n",
        "  np.save(os.path.join(history_path,'history'+str(fold_no)+'.npy'), report)\n",
        "  print('Results saved to disk ...')\n",
        "  print('Evaluating the model ...') \n",
        "  cent_model_1 = keras.models.load_model(\"/content/gdrive/MyDrive/saved_models/topic_1_neuro_kfold_\"+str(fold_no))\n",
        "  results = cent_model_1.evaluate(val_ds,workers = 4)\n",
        "  results = dict(zip(cent_model_1.metrics_names,results))\n",
        "  np.save(os.path.join(history_path,'results_'+str(fold_no)+'.npy'), results)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  fold_no += 1\n",
        "\n",
        "  print('Going to next fold................')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM_TVRszT4et"
      },
      "source": [
        "#data_set = get_small_ds('/content/train.csv')\n",
        "for i, j in zip(name,df):\n",
        "    i = pd.DataFrame(j)\n",
        "    #print(len(i))\n",
        "    print(vars()[j])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYfDqyO4b8gS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CFRCHcUYAVf"
      },
      "source": [
        "/content/history1.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgO8eia-Xof8"
      },
      "source": [
        "# Plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-AbVXl-ilqR"
      },
      "source": [
        "train_list = ['history_3_1.npy','history_3_2.npy','history_3_3.npy','history_3_4.npy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I2J9FlBgSL1"
      },
      "source": [
        "read_train_1 = np.load('/content/history_3_1.npy',allow_pickle=True).item()\n",
        "read_train_2 = np.load('/content/history_3_2.npy',allow_pickle=True).item()\n",
        "read_train_3 = np.load('/content/history_3_3.npy',allow_pickle=True).item()\n",
        "read_train_2 = np.load('/content/history_3_4.npy',allow_pickle=True).item()\n",
        "read_test_1 = np.load('/content/results_3_1.npy',allow_pickle=True).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWnrgMKjNsj"
      },
      "source": [
        "read_train_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvmQkhlmXtiW"
      },
      "source": [
        "train_list = ['history_3_1.npy','history_3_2.npy','history_3_3.npy','history_3_4.npy']\n",
        "for train in train_list:\n",
        "  train = train.replace('.npy','')\n",
        "  fold = 'read_train_fold_'+str(train)\n",
        "  vars()[fold] = np.load('/content/history_3_1.npy',allow_pickle='TRUE').item()\n",
        "  print(fold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycLVTWxnfSSO"
      },
      "source": [
        "read_train_fold_history_3_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKgWqM80YGbt"
      },
      "source": [
        "a = unzip(read_test.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2CuaQkN0o20"
      },
      "source": [
        "## Use this declaration to save plots in file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FckCoW60YGZT"
      },
      "source": [
        "plot_dir = '/content/gdrive/MyDrive/plots'\n",
        "#For peregrine\n",
        "#plot_dir = '/data/s4133366/saved_models/history/plots'\n",
        "def plot_metrics(history_path,results_path):\n",
        "  read_train = np.load(history_path,allow_pickle=True).item()\n",
        "  read_test = np.load(results_path,allow_pickle=True).item()\n",
        "  history_path = os.path.split(history_path)[-1]\n",
        "  get_fold = history_path[-5]\n",
        "  history_path = history_path.replace('.npy','')\n",
        "  if not os.path.isdir(plot_dir):\n",
        "    print('Plot directory '+plot_dir+' does not exist at the moment. Creating ....')\n",
        "    os.mkdir(plot_dir)\n",
        "    print(plot_dir+' created ...')\n",
        "  if not os.path.isdir(os.path.join(plot_dir,history_path)) :\n",
        "    os.mkdir(os.path.join(plot_dir,history_path))\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path+' created...')\n",
        "  else:\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path,' exists ...')  \n",
        "  print('Adding the metric plots to ',folder_path)\n",
        "  \n",
        "  #summarize history for accuracy\n",
        "  \n",
        "  plt.plot(read_train['categorical_accuracy'])\n",
        "  plt.axhline(read_test['categorical_accuracy'], color='g', linestyle='--')\n",
        "  plt.plot(read_test['categorical_accuracy'])\n",
        "  plt.title('model accuracy for fold_'+str(get_fold))\n",
        "  plt.ylabel('Categorical Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'accuracy_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "  \n",
        "  # summarize history for loss\n",
        "  \n",
        "  plt.plot(read_train['loss'])\n",
        "  plt.axhline(read_test['loss'], color='g', linestyle='--')\n",
        "  plt.title('model loss for fold_'+str(get_fold))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  #plt.show()\n",
        "  plt.savefig(os.path.join(folder_path,'loss_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "  \n",
        "  #summarize history for Precision \n",
        "  plt.plot(read_train['precision'])\n",
        "  plt.axhline(read_test['precision'], color='g', linestyle='--')\n",
        "  plt.title('model precision for fold_'+str(get_fold))\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'precision_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "  \n",
        "  #summarize history for Recall \n",
        "  \n",
        "  plt.plot(read_train['recall'])\n",
        "  plt.axhline(read_test['recall'], color='g', linestyle='--')\n",
        "  plt.title('Model Recall for fold_'+str(get_fold))\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'recall_'+str(history_path)+'.png'))\n",
        "  plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP_1lFr7zBxQ"
      },
      "source": [
        "\n",
        "\n",
        "shutil.rmtree('/content/gdrive/MyDrive/plots')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psawkkhQjYq9"
      },
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cioH1vLojZti"
      },
      "source": [
        "cache_file_t = '/content/gdrive/MyDrive/cache_train'\n",
        "cache_file_v = '/content/gdrive/MyDrive/cache_val'\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir('/content/gdrive/MyDrive/data/test')]))\n",
        "num_folds = 4\n",
        "batch_size = 64\n",
        "skfold = StratifiedKFold(n_splits=num_folds, shuffle=True,random_state=42)\n",
        "fold_no = 1\n",
        "\n",
        "history_path = '/content/gdrive/MyDrive/history'\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "save_dir = '/content/gdrive/MyDrive/saved_models'\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "\n",
        "\n",
        "Y = tr_df['Label']\n",
        "X = tr_df['Image']\n",
        "print(len(X),len(Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4Txr7jjkza"
      },
      "source": [
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  training_data = tr_df.iloc[train_index]\n",
        "  valid_data = tr_df.iloc[val_index]\n",
        "  print('train:',len(training_data))\n",
        "  print('val:',len(valid_data))\n",
        "  train_data = training_data['Image']\n",
        "  train_labels = training_data['Label']\n",
        "  val_labels = valid_data['Label']\n",
        "  val_data = valid_data['Image']\n",
        "  print('train:',len(train_data))\n",
        "  print('val:',len(valid_data))\n",
        "  train_tensor = tf.data.Dataset.from_tensors(train_data)\n",
        "  val_tensor = tf.data.Dataset.from_tensors(val_data)\n",
        "  train_tensor.shuffle(len(train_data))\n",
        "  val_tensor.shuffle(len(val_data))\n",
        "  print('train_tensor',train_tensor)\n",
        "  print('val_tensor',val_tensor)\n",
        "  tra_tens = train_tensor.unbatch().map(process_TL)\n",
        "  val_tens = val_tensor.unbatch().map(process_TL)\n",
        "  print(tra_tens,'mapped train tensor ',tf.data.experimental.cardinality(tra_tens))\n",
        "  print(val_tens,' mapped val tensor',tf.data.experimental.cardinality(val_tens))\n",
        "  train_cache = tra_tens.cache(cache_file_t)\n",
        "  val_cache = val_tens.cache(cache_file_v)\n",
        "  #normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "  val_batch = val_tens.batch(batch_size)\n",
        "  train_batch= tra_tens.batch(batch_size)\n",
        "  train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  print(train_ds)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyvY6u1AZ-GS"
      },
      "source": [
        "path = '/content/results_sds_ns_4.npy'\n",
        "results = np.load(path,allow_pickle=True).item()\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQp7VFMzkm7"
      },
      "source": [
        "##Trying out weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP2lAMNclcJ1"
      },
      "source": [
        "###testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIPh4ThsldhQ"
      },
      "source": [
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  '/content/gdrive/MyDrive/data/train',\n",
        "  seed=123,\n",
        "  image_size=(224, 224),\n",
        "  batch_size=64,color_mode = 'rgb',shuffle = True,label_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wh1XBxWh3uz"
      },
      "source": [
        "class_names = test_dataset.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h6qVNuGm-71"
      },
      "source": [
        "for image_batch, labels_batch in test_dataset.take(2):\n",
        "  #print(image_batch)\n",
        "  print(labels_batch)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq3P3VumnxQw"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "#image_batch, labels_batch = next(iter(test_dataset))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU3aIyVvvUsw"
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOOSfIOotP34"
      },
      "source": [
        "iterator = test_dataset.make_one_shot_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4dHm2qkoMy-"
      },
      "source": [
        "train_ds = test_dataset.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTH53VcPwcTl"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMl23seBwE6I"
      },
      "source": [
        "history = cent_model_c.fit(train_ds,epochs=5,verbose=1,steps_per_epoch=100,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETYxCNKEwDJ-"
      },
      "source": [
        "cent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU41-QoEudbT"
      },
      "source": [
        "top_1 = cent_model_c.get_layer(name='topic_1', index=None)\n",
        "top_2 = cent_model_c.get_layer(name='topic_2', index=None)\n",
        "top_3 = cent_model_c.get_layer(name='topic_3', index=None)\n",
        "top_4 = cent_model_c.get_layer(name='topic_4', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E2P8Zm5vt5c"
      },
      "source": [
        "#top_3 =  0.00572042,  0.07700813,  0.00037249, ...,  0.01569479,\n",
        "#         -0.04370036,  0.00142679\n",
        "#top_4 = 0.07224628,  0.04283633,  0.08743759, ...,  0.12188006,\n",
        "#         -0.01654566,  0.0358057 \n",
        "# top_1 -0.00504995,  0.05073379, -0.01360468, ..., -0.01115265,\n",
        "#          -0.03401065, -0.00033989],\n",
        "#         [-0.0618718 , -0.03967685,  0.03000757, ...,  0.08042301,\n",
        "#          -0.04552665, -0.07524551],\n",
        "#         [-0.02856736,  0.01151073,  0.0035671 , ...,  0.00878615,\n",
        "#          -0.00205357,  0.01009549],\n",
        "# top_2 = 0.16858643,  0.06966868, -0.02712329, ...,  0.07811368,\n",
        "#           0.06262797,  0.00686309],\n",
        "#         [ 0.15656647, -0.03285585,  0.05510008, ...,  0.03848132,\n",
        "#           0.00037507, -0.00980724],\n",
        "#         [ 0.13819823,  0.02557753,  0.0020411 , ..., -0.01188827,\n",
        "#          -0.02261599, -0.0185649\n",
        "top_2.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apSnsHAIwlQ4"
      },
      "source": [
        "weig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_aKiv_eVXwy"
      },
      "source": [
        "train_num_files=len([file for file in glob(str(train_dir + '/*/*'))])\n",
        "val_num_files=len([file for file in glob(str(val_dir + '/*/*'))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONsU4OoTSNjp"
      },
      "source": [
        "train_files=[file for file in glob(str(train_dir + '/*/*'))]\n",
        "val_files=[file for file in glob(str(val_dir + '/*/*'))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS9XFLNRPFU6"
      },
      "source": [
        "len(train_files),train_num_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCxdQUTu2okK"
      },
      "source": [
        "tra_ten = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "val_ten = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "tra_ten = tra_ten.shuffle(train_num_files)\n",
        "val_ten = val_ten.shuffle(val_num_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNmZjnor0AF"
      },
      "source": [
        "tr = tra_ten.take(1)\n",
        "for el in tr:\n",
        "  print(el)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrSEieW9ZHI_"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWz82gCGdZz0"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qou2m9FyfcIk"
      },
      "source": [
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1g6WKuMW3sh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3YV_jIRS2tQ"
      },
      "source": [
        "tra_ten_test = tra_ten.map(process_TL)\n",
        "val_ten_test = val_ten.map(process_TL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVpu7pXXRVCA"
      },
      "source": [
        "for image,label in tra_ten_test.take(1):\n",
        "  print(image)\n",
        "  print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_3y9XhBTBKV"
      },
      "source": [
        "tf.data.experimental.cardinality(tra_ten_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY0nws6FTTOW"
      },
      "source": [
        "train_fi\n",
        "for item in train_fi:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqkDnfplITKZ"
      },
      "source": [
        "train_fi = tra_ten_test.cache()\n",
        "val_fi = val_ten_test.cache()\n",
        "#test_ds = train_fi.map(process_TL).shuffle(buffer_size = train_num_files,seed = 42,reshuffle_each_iteration=True).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s77KhihFTeBc"
      },
      "source": [
        "train_fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egs4BMkxIpgA"
      },
      "source": [
        "print(tf.data.experimental.cardinality(test_ds))\n",
        "for it in test_ds:\n",
        "  print(it)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXXA9ZszUUwz"
      },
      "source": [
        "for images, labels in tra_ten.take(1):\n",
        "  print('images.shape: ', images)\n",
        "  print('labels.shape: ', labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4wTbgmJVpYZ"
      },
      "source": [
        "train_files = tf.data.Dataset.list_files([file for file in glob(str(train_dir + '/*/*'))], shuffle=False)\n",
        "val_files = tf.data.Dataset.list_files([file for file in glob(str(val_dir + '/*/*'))], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ8gh5iZADNb"
      },
      "source": [
        "len(train_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcRoY-48fkdc"
      },
      "source": [
        "train_ds = train_files.interleave(lambda x: tf.data.Dataset.list_files([file for file in glob(str(train_dir + '/*/*'))], shuffle=True),cycle_length=4 ).map(process_TL,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_files.interleave(lambda x: tf.data.Dataset.list_files([file for file in glob(str(val_dir + '/*/*'))], shuffle=True),cycle_length=4 ).map(process_TL,num_parallel_calls = tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYDII2PaoZ4m"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj04edOIiZZ6"
      },
      "source": [
        "train_ds = tra_ten.cache()\n",
        "val_ds = val_ten.cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0BTO0cVIH39"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQe7Sbo-rgd1"
      },
      "source": [
        "def plot_batch_sizes(ds):\n",
        "  batch_sizes = [batch.shape[0] for batch in ds]\n",
        "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
        "  plt.xlabel('Batch number')\n",
        "  plt.ylabel('Batch size')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D04JG-2hodI5"
      },
      "source": [
        "test = train_ds.take(1)\n",
        "test\n",
        "for image, label in test:\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPtEOYkBLiPQ"
      },
      "source": [
        "batch_size = 128\n",
        "num_epochs = 1\n",
        "steps = int(train_num_files/batch_size)\n",
        "steps_val = int(val_num_files/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dydivnJ5jiTp"
      },
      "source": [
        "tf.data.experimental.cardinality(train_fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX_RdXm9jPPd"
      },
      "source": [
        "t_ds = train_fi.repeat(num_epochs).batch(batch_size)\n",
        "#v_ds = val_ten_test.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw8pwa6orYxs"
      },
      "source": [
        "tf.data.experimental.cardinality(t_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J_z9QgMogEH"
      },
      "source": [
        "test = t_ds.take(1)\n",
        "tf.data.experimental.cardinality(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFrothrYkjDX"
      },
      "source": [
        "train_prefetch_ds = t_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "#val_prefetch_ds = v_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fca4DKr0F3P0"
      },
      "source": [
        "tf.data.experimental.cardinality(train_prefetch_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gflkF96oVQM"
      },
      "source": [
        "t_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mja_S2aIiGZ"
      },
      "source": [
        "import datetime\n",
        "save_dir = '/content/gdrive/MyDrive/topic_m'\n",
        "log_dir = \"/content/gdrive/MyDrive/topic_m/logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEl7E1l2CBXU"
      },
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=os.path.join(save_dir,'topic'),\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"categorical_accuracy\",\n",
        "        verbose=1,\n",
        "        save_freq='epoch',mode = 'max'\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    ),\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=1, min_lr=1e-6),\n",
        "    TensorBoard(log_dir=log_dir, histogram_freq=1)         \n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIxoLG1-9OXR"
      },
      "source": [
        "TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iK4gYMAh6Gf"
      },
      "source": [
        "cent_model = tf.keras.models.load_model('/content/gdrive/MyDrive/topic_m/top')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9QsIxZ0ZBj"
      },
      "source": [
        "%tensorboard --logdir /content/gdrive/MyDrive/topic_m/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZxLFyLFnLSL"
      },
      "source": [
        "history = cent_model.fit(t_ds,epochs=num_epochs,steps_per_epoch=steps,callbacks=callbacks,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F62LlQIwkUiC"
      },
      "source": [
        "cent_model_c.save(os.path.join(save_dir,'top_1'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMGdUY5aKpqN"
      },
      "source": [
        "###TESTING MULTI GPU WORKFLOW\n",
        "\n",
        "!pip install d2l==0.16.6\n",
        "!pip install -U mxnet-cu101==1.7.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygrlte9VfeTC"
      },
      "source": [
        "test = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "test = test.repeat().shuffle(buffer_size = 500,seed = 42).batch(batch_size, drop_remainder=True)\n",
        "t = test.take(1)\n",
        "for t1 in t:\n",
        "  print(t1)\n",
        "#20161202_151053_000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg_VbjW2PCri"
      },
      "source": [
        "#Parameterized function for reading and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otFG-ANurqCb"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.models import Sequential,Model\n",
        "from tensorflow.python.keras.layers import Dense,AveragePooling2D,Dropout,Flatten,Dense,Input,Concatenate\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import shutil\n",
        "from sklearn.metrics import multilabel_confusion_matrix,classification_report\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.utils import shuffle\n",
        "import json\n",
        "import os, re\n",
        "\n",
        "\n",
        "#HYPER PARAMS #########\n",
        "num_folds = 4\n",
        "t_batch_size = 128\n",
        "v_batch_size = 128\n",
        "num_epochs =1\n",
        "model_name = 'train_tf_kfold_sds_noshuff_2_'\n",
        "learning_rate = 0.4\n",
        "#Data directory stuff ###############################################\n",
        "cache_dir = '/data/s4133366/data_cache'\n",
        "train_csv = ''\n",
        "train_dir = #'/data/s4133366/data/train'\n",
        "save_dir = '/data/s4133366/saved_models'\n",
        "data_dir = '/data/s4133366/data'\n",
        "####################################################################\n",
        "train_csv_file = 'short_df_collab.csv'\n",
        "val_csv_file = 'val_short_collab.csv'\n",
        "####################################################################\n",
        "cache_t = os.path.join(cache_dir,model_name+'_t')\n",
        "cache_v = os.path.join(cache_dir,model_name+'_v')\n",
        "\n",
        "regu_t = os.path.split(cache_t)[-1]+'*'\n",
        "regu_v = os.path.split(cache_v)[-1]+'*'\n",
        "print(regu_v,regu_t)\n",
        "\n",
        "#Change paths \n",
        "model_dir = os.path.join(save_dir,model_name)\n",
        "\n",
        "\n",
        "plot_dir = os.path.join(model_dir,'plots')\n",
        "history_dir = os.path.join(model_dir,'history')\n",
        "\n",
        "\n",
        "train_metrics_path = os.path.join(history_dir,model_name+'.npy')\n",
        "res_path = os.path.join(history_path,'results_'+model_name+'.npy')\n",
        "chk_path = os.path.join(model_dir,model_name+'_chk')\n",
        "#Saving weights for retraining ....\n",
        "weights_path = os.path.join(model_dir,model_name+'_weights')\n",
        "#######################\n",
        "\n",
        "#Function for clearing cache\n",
        "def purge(dir, pattern):\n",
        "  for f in os.listdir(dir):\n",
        "    if re.search(pattern, f):\n",
        "      os.remove(os.path.join(dir, f))\n",
        "#Function for making and storing the plots\n",
        "\n",
        "def plot_metrics(history_path,results_path):\n",
        "  read_train = np.load(history_path,allow_pickle=True).item()\n",
        "  read_test = np.load(results_path,allow_pickle=True).item()\n",
        "  history_path = os.path.split(history_path)[-1]\n",
        "  get_fold = history_path[-5]\n",
        "  history_path = history_path.replace('.npy','')\n",
        "  if not os.path.isdir(plot_dir):\n",
        "    print('Plot directory '+plot_dir+' does not exist at the moment. Creating ....')\n",
        "    os.mkdir(plot_dir)\n",
        "  print(plot_dir+' created ...')\n",
        "  if not os.path.isdir(os.path.join(plot_dir,history_path)) :\n",
        "    os.mkdir(os.path.join(plot_dir,history_path))\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path+' created...')\n",
        "  else:\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path,' exists ...')  \n",
        "    print('Adding the metric plots to ',folder_path)\n",
        "\n",
        "  #summarize history for accuracy\n",
        "\n",
        "  plt.plot(read_train['categorical_accuracy'])\n",
        "  plt.axhline(read_test['categorical_accuracy'], color='g', linestyle='--')\n",
        "  plt.title('model accuracy for fold_'+str(get_fold))\n",
        "  plt.ylabel('Categorical Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'accuracy_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  # summarize history for loss\n",
        "\n",
        "  plt.plot(read_train['loss'])\n",
        "  plt.axhline(read_test['loss'], color='g', linestyle='--')\n",
        "  plt.title('model loss for fold_'+str(get_fold))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  #plt.show()\n",
        "  plt.savefig(os.path.join(folder_path,'loss_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Precision \n",
        "  plt.plot(read_train['precision'])\n",
        "  plt.axhline(read_test['precision'], color='g', linestyle='--')\n",
        "  plt.title('model precision for fold_'+str(get_fold))\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'precision_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Recall \n",
        "\n",
        "  plt.plot(read_train['recall'])\n",
        "  plt.axhline(read_test['recall'], color='g', linestyle='--')\n",
        "  plt.title('Model Recall for fold_'+str(get_fold))\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'recall_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "print('########## BASELINE MODEL TRAINING FOR SHORT DS USING STRATIFIED K FOLD ... ######################')\n",
        "print('Loading CSVs')\n",
        "\n",
        "train_csv = pd.read_csv('/content/short_df_collab.csv')\n",
        "#train_csv = pd.read_csv(os.path.join(data_dir,train_csv_file))\n",
        "train_csv = train_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "val_csv = pd.read_csv('/content/val.csv')\n",
        "#val_csv = pd.read_csv(os.path.join(data_dir,val_csv_file))\n",
        "val_csv = val_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "print('Data read and loading onto tensors.....')\n",
        "\n",
        "def get_label(file_path):\n",
        "# convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "# convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir(train_dir)]))\n",
        "\n",
        "\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "\n",
        "\n",
        "\n",
        "print('Cleaning prior cache ...')\n",
        "#purge(cache_dir,regu_v)   \n",
        "print('value cache cleared ....')\n",
        "#purge(cache_dir,regu_t)\n",
        "print('train cache cleared ....')\n",
        "training_data = train_csv\n",
        "valid_data = val_csv\n",
        "print(len(training_data))\n",
        "print(len(valid_data))\n",
        "train_data = training_data['Image']\n",
        "train_labels = training_data['Label']\n",
        "print(len(train_data))\n",
        "val_labels = valid_data['Label']\n",
        "val_data = valid_data['Image']\n",
        "print(len(val_data))\n",
        "steps = 1 #int(len(train_data)/t_batch_size)\n",
        "steps_val = 1 #int(len(val_data)/v_batch_size)\n",
        "\n",
        "train_tensor = tf.data.Dataset.from_tensors(train_data)\n",
        "val_tensor = tf.data.Dataset.from_tensors(val_data)\n",
        "train_tensor = train_tensor.unbatch()\n",
        "val_tensor = val_tensor.unbatch()\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "train_tensor = train_tensor.shuffle(len(train_data))\n",
        "val_tensor = val_tensor.shuffle(len(val_data))\n",
        "\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "print('Shuffling the dataframes internally for more randomness...')\n",
        "\n",
        "tra_tens = train_tensor.map(process_TL)\n",
        "val_tens = val_tensor.map(process_TL)\n",
        "print('tra_tens:',tf.data.experimental.cardinality(tra_tens))\n",
        "print('tra_tens:',tf.data.experimental.cardinality(val_tens))\n",
        "train_cache = tra_tens.cache()\n",
        "val_cache = val_tens.cache()\n",
        "print(tf.data.experimental.cardinality(train_cache))\n",
        "print(tf.data.experimental.cardinality(val_cache))\n",
        "print('Created cache ...')\n",
        "#normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "val_batch = val_cache.batch(v_batch_size)\n",
        "train_batch= train_cache.repeat(num_epochs).batch(t_batch_size)\n",
        "print(tf.data.experimental.cardinality(train_batch))\n",
        "print(tf.data.experimental.cardinality(val_batch))\n",
        "print(train_batch)\n",
        "print(train_batch)\n",
        "train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "print(tf.data.experimental.cardinality(train_ds))\n",
        "print(tf.data.experimental.cardinality(val_ds))\n",
        "print(train_ds)\n",
        "\n",
        "\n",
        "#Create model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(128,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(24,activation=\"softmax\")(x)\n",
        "\n",
        "# #Activating the model\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "opti = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "model.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')\n",
        "\n",
        "callbacks = [\n",
        "  ModelCheckpoint(\n",
        "    \n",
        "    filepath=os.path.join(model_dir,model_name),\n",
        "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "    monitor=\"categorical_accuracy\",\n",
        "    verbose=1,\n",
        "    save_freq = 'epoch',mode = 'max'\n",
        "  ),\n",
        "  EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "  ),\n",
        "  ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                          patience=3, min_lr=1e-5,verbose=1)\n",
        "]\n",
        "\n",
        "\n",
        "#print('Loading model ...')\n",
        "#model = keras.models.load_model(os.path.join(save_dir,'train_tf_kfold_sds_noshuff_'+str(fold_no)))    \n",
        "print('Model loaded starting training...')\n",
        "print('Accessing the model directory. creating if needed....')\n",
        "if not os.path.isdir(model_dir):\n",
        "  print(os.path.join(save_dir,model_dir)+' not found. Creating ....')\n",
        "  os.mkdir(model_dir)\n",
        "  if not os.path.isdir(history_dir):\n",
        "    print('Creating '+history_dir +'...')\n",
        "    os.mkdir(history_dir)\n",
        "  if not os.path.isdir(plots_dir):\n",
        "    os.mkdir(plots_dir)\n",
        "    print('Creating '+plots_dir+'...')\n",
        "\n",
        "print('Folders checked. Training model ...')\n",
        "history = model.fit(train_ds,epochs=num_epochs,callbacks=callbacks,verbose=1,workers=8,use_multiprocessing = False,shuffle=True,steps_per_epoch = steps)\n",
        "model.save(os.path.join(model_dir,model_name))\n",
        "report = history.history\n",
        "his_path = train_metrics_path\n",
        "np.save(os.path.join(history_path,train_metrics_path), report)\n",
        "history_filename = os.path.join(history_dir,train_metrics_path)\n",
        "print('Results saved to disk ...')\n",
        "print('Evaluating the model ...') \n",
        "model = keras.models.load_model(os.path.join(model_dir,model_name))\n",
        "\n",
        "results = model.evaluate(val_ds,workers = 4,steps = steps_val)\n",
        "results = dict(zip(model.metrics_names,results))\n",
        "\n",
        "np.save(os.path.join(history_path,res_path), results)\n",
        "results_filename = os.path.join(history_dir,res_path)\n",
        "model.save(os.path.join(model_dir,model_name))\n",
        "model.save_weights(weights_path)\n",
        "print('Storing the plots in '+plot_dir+'....')\n",
        "\n",
        "print('Trying to plot ...')\n",
        "try:\n",
        "  plot_metrics(history_filename,results_filename)\n",
        "except : \n",
        "  print('Something went wrong while plotting skipping ...')\n",
        "\n",
        "print('Metrics have been plotted ...')\n",
        "tf.keras.backend.clear_session()\n",
        "print('Clearning cache ...')\n",
        "#purge(cache_dir,regu_v)   \n",
        "print('value cache cleared ....')\n",
        "#purge(cache_dir,regu_t)\n",
        "print('train cache cleared ....')\n",
        "\n",
        "print('Cache cleared')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Job ended ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfNBJIxt0AuC"
      },
      "source": [
        "train_dir = '/data/s4133366/data/train'\n",
        "save_dir = '/data/s4133366/saved_models'\n",
        "model_name = 'train_tf_kfold_sds_noshuff_2_'\n",
        "model_dir = os.path.join(save_dir,model_name)\n",
        "\n",
        "\n",
        "plot_dir = os.path.join(model_dir,'plots')\n",
        "history_dir = os.path.join(model_dir,'history')\n",
        "\n",
        "\n",
        "train_metrics_path = os.path.join(history_dir,model_name+'.npy')\n",
        "res_path = os.path.join(history_path,'results_'+model_name+'.npy')\n",
        "chk_path = os.path.join(model_dir,model_name+'_chk')\n",
        "#Saving weights for retraining ....\n",
        "weights_path = os.path.join(model_dir,model_name+'_weights')\n",
        "\n",
        "train_dir,plot_dir,history_path,res_path,train_metrics_path,chk_path,model_dir,weights_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84J6WJae0UP4"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('/content/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny_yg-_CyCzt"
      },
      "source": [
        "#Making predictions on the topic layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYj7xTKwcsj"
      },
      "source": [
        "###Using image data generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRol9uufzJZ"
      },
      "source": [
        "model = keras.models.load_model(\"/content/gdrive/MyDrive/saved_models/topic_full_ds_eval\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqDbvA4AmaI9"
      },
      "source": [
        "lr = 0.1\n",
        "num_epochs = 20\n",
        "opti = Adam(learning_rate= lr,decay = 0)\n",
        "model.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sUjNfN9Ftc6"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAip2gwFp8j"
      },
      "source": [
        "testGen = datagen.flow_from_directory('/content/gdrive/MyDrive/data/test',target_size=(224,224),\n",
        "                     color_mode='rgb',batch_size=116,class_mode=None,shuffle=False,seed=42)\n",
        "num_test_samples = len(testGen.filenames)\n",
        "num_test_classes = len(testGen.class_indices)\n",
        "STEP_SIZE_TEST = testGen.n//testGen.batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO6c-8h4JZD7"
      },
      "source": [
        "testGen.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXUd8QrPSj6"
      },
      "source": [
        "print(\"Predicting the model\")\n",
        "testGen.reset()\n",
        "predictions = model.predict(testGen,steps=STEP_SIZE_TEST,verbose=1,workers = 4,use_multiprocessing = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWrIMcHOd6_i"
      },
      "source": [
        "pd.DataFrame(predictions).to_csv(\"topic_full_ds_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXW8SMJ_5jSQ"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNrhjm_ub1sa"
      },
      "source": [
        "predictions_test = np.argmax(predictions,axis=1)\n",
        "#print(testGen[0].shape)\n",
        "len(testGen.labels)\n",
        "#First 128 images\n",
        "#testing = testGen[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mse2KQqyUqR"
      },
      "source": [
        "print(\"Predicting the model\")\n",
        "testGen.reset()\n",
        "predictions = model.predict(testGen,steps=STEP_SIZE_TEST,verbose=1,workers = 4,use_multiprocessing = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAJkqn7OyUqR"
      },
      "source": [
        "pd.DataFrame(predictions).to_csv(\"predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKv_F8abyUqS"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkhkIDLWyUqS"
      },
      "source": [
        "predictions_test = np.argmax(predictions,axis=1)\n",
        "#print(testGen[0].shape)\n",
        "len(testGen.labels)\n",
        "#First 128 images\n",
        "#testing = testGen[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zqyPC_95vw8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p7-lBHkm7w"
      },
      "source": [
        "test_labels = testGen.labels\n",
        "test_labels = test_labels[:128]\n",
        "test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re9Qcw3mjRsH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7MyvkZ6yd4X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7T68TT1GBhA"
      },
      "source": [
        "###Making the confusion matrix and deriving the classification report with data generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uno_qfYKXypm"
      },
      "source": [
        "#preds = np.argmax(predictions,axis=1)\n",
        "#len(predictions)\n",
        "#preds\n",
        "print(classification_report(testGen.classes, predictions_test,target_names=testGen.class_indices.keys()))\n",
        "#ecode_predictions(predictions, top=3)[0]\n",
        "#print(\"[INFO] saving model...\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KF_N9fFEFSH"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true = testGen.classes\n",
        "y_pred = predictions_test\n",
        "\n",
        "#print(list(labels))\n",
        "cm = confusion_matrix(y_true,y_pred,labels =  list(range(0,24)) )\n",
        "df_cm = pd.DataFrame(cm,index = ['Balcony','Bathroom','Beach','Bedroom','Buildings','Education,science','Forest, field, jungle', 'Garden', 'Hospital','Kitchen','Living room','Mountains, hills, desert, sky','Museum','Noise','Office','Other rooms','Others','Pathways','Recreation','Restaurant,Bar','Shop','Sport fields','Transportation','Water'] ,columns =  ['Balcony','Bathroom','Beach','Bedroom','Buildings','Education,science','Forest, field, jungle', 'Garden', 'Hospital','Kitchen','Living room','Mountains, hills, desert, sky','Museum','Noise','Office','Other rooms','Others','Pathways','Recreation','Restaurant,Bar','Shop','Sport fields','Transportation','Water'] )\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=False, annot_kws={\"size\": 20},cmap=\"BuPu\") # font size\n",
        "plt.title('Confusion matrix for Topic Full DS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFNjHNinCHsE"
      },
      "source": [
        "cm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4H-cUJ_uzM"
      },
      "source": [
        "labels = ['Balcony','Bathroom','Beach','Bedroom','Buildings','Education,science','Forest, field, jungle', 'Garden', 'Hospital','Kitchen','Living room','Mountains, hills, desert, sky','Museum','Noise','Office','Other rooms','Others','Pathways','Recreation','Restaurant,Bar','Shop','Sport fields','Transportation','Water']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ntVeO5iMIM1"
      },
      "source": [
        "# New Topic Layer Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuUNa04h0Xko"
      },
      "source": [
        "##Centralized approach (c) {Looks closest to the schematic on the slides}\n",
        "\n",
        "* Taking the output of the resnet soft max into k_1,k_2,k_3 and k_4 \n",
        "* Concatenating the layers in order to get 4000 neurons with the 1000 objects for each topic(k=4)\n",
        "* Flattening the different topics into Dense layer of 4000 neurons(4 topics*1000 objects)\n",
        "* Adding a dense layer of 4000 neurons with tanh activation\n",
        "* Adding 50% Dropout rate\n",
        "* Using softmax in order to get the places(p = 24 i.e num_classes)\n",
        "\n",
        "**Issue : Does not give places probabilities for each topic individually as it is being flattened and fed to softmax with only 24 units for each class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00o9bd8W5Tk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnsnZZ2v2ZfM"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(1024,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "# y = Dense(1)(model.output)\n",
        "k_1 = Dense(20,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(20,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(20,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(20,name = 'topic_4')(model.output)\n",
        "wt_add = Wt_Topics(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "flat_2 = Flatten(name = 'flat_c')(sum_layer)\n",
        "# w_1 = tf.Variable(1.0,name='weight_topic_1',trainable=True )\n",
        "# w_2 = tf.Variable(1.0 ,name='weight_topic_2',trainable=True)\n",
        "# w_3 = tf.Variable(1.0 ,name='weight_topic_3',trainable=True )\n",
        "# w_4 = tf.Variable(1.0 ,name='weight_topic_4',trainable=True)\n",
        "# conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "# flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "# #Adding the Dense layers now\n",
        "# topic = Dense(4000,activation='tanh')(flat_2)\n",
        "# topic = Dropout(0.5)(topic)\n",
        "topic = Dense(128,activation='tanh')(flat_2)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRnkFP25N2Bu"
      },
      "source": [
        "# conc2 = Concatenate(name = 'conc_c')([k_1*w_1,k_2*w_2,k_3*w_3,k_4*w_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz4hjYPf3tTW"
      },
      "source": [
        "cent_model_c = Model(inputs = base_model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDZ6l-P132ff"
      },
      "source": [
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewp-EtgVDn83"
      },
      "source": [
        "opti = tf.keras.optimizers.Adadelta(learning_rate=1e-1, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "cent_model_c.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHIFKgdIVW4x"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w1 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_1'\n",
        "    )\n",
        "    self.w2 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_2'\n",
        "    )  \n",
        "    self.w3 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_3'\n",
        "    )       \n",
        "    self.w4 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_4'\n",
        "    )\n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #name = 'tops'\n",
        "    return tf.multiply(input1,self.w1) + tf.multiply(input2, self.w2) + tf.multiply(input3, self.w3)+tf.multiply(input4, self.w4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjAo4nJLusL"
      },
      "source": [
        "top_1 = cent_model_c.get_layer( index=-5)\n",
        "topic_layer_weights = top_1.get_weights()\n",
        "topic_1 = cent_model_c.get_layer(name = 'topic_1')\n",
        "topic_1_weights = topic_1.get_weights()\n",
        "topic_1_weights[0].shape\n",
        "topic_layer_weights\n",
        "cent_model_c.layers[-5].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1Iu1Mcg_pW"
      },
      "source": [
        "### 2x2 Topic Testing Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYhoi6Cax4b"
      },
      "source": [
        "\n",
        "\n",
        "#HYPER PARAMS #########\n",
        "num_folds = 4\n",
        "t_batch_size = 128\n",
        "v_batch_size = 128\n",
        "num_epochs =10\n",
        "model_name = 'test_2x2'\n",
        "learning_rate = 0.4\n",
        "#Data directory stuff ###############################################\n",
        "cache_dir = ''\n",
        "train_csv = '/content/gdrive/MyDrive/data/train'\n",
        "train_dir = '/content/gdrive/MyDrive/data/train'#/data/s4133366/data/train'\n",
        "save_dir = '/content/gdrive/MyDrive/saved_models'#'/data/s4133366/saved_models'\n",
        "data_dir = '/content/gdrive/MyDrive/data'\n",
        "####################################################################\n",
        "train_csv_file = '/content/gdrive/MyDrive/data/short_df_collab.csv'\n",
        "val_csv_file = '/content/gdrive/MyDrive/data/val_short_collab.csv'\n",
        "####################################################################\n",
        "#cache_t = os.path.join(cache_dir,model_name+'_t')\n",
        "#cache_v = os.path.join(cache_dir,model_name+'_v')\n",
        "\n",
        "#regu_t = os.path.split(cache_t)[-1]+'*'\n",
        "#regu_v = os.path.split(cache_v)[-1]+'*'\n",
        "#print(regu_v,regu_t)\n",
        "\n",
        "#Change paths \n",
        "model_dir = os.path.join(save_dir,model_name)\n",
        "\n",
        "\n",
        "plot_dir = os.path.join(model_dir,'plots')\n",
        "history_dir = os.path.join(model_dir,'history')\n",
        "\n",
        "\n",
        "train_metrics_path = os.path.join(history_dir,model_name+'.npy')\n",
        "res_path = os.path.join(history_dir,'results_'+model_name+'.npy')\n",
        "chk_path = os.path.join(model_dir,model_name+'_chk')\n",
        "#Saving weights for retraining ....\n",
        "weights_path = os.path.join(model_dir,model_name+'_weights')\n",
        "#######################\n",
        "\n",
        "#Function for clearing cache\n",
        "# def purge(dir, pattern):\n",
        "#   for f in os.listdir(dir):\n",
        "#     if re.search(pattern, f):\n",
        "#       os.remove(os.path.join(dir, f))\n",
        "# #Function for making and storing the plots\n",
        "\n",
        "def plot_metrics(history_path,results_path):\n",
        "  read_train = np.load(history_path,allow_pickle=True).item()\n",
        "  read_test = np.load(results_path,allow_pickle=True).item()\n",
        "  history_path = os.path.split(history_path)[-1]\n",
        "  get_fold = history_path[-5]\n",
        "  history_path = history_path.replace('.npy','')\n",
        "  if not os.path.isdir(plot_dir):\n",
        "    print('Plot directory '+plot_dir+' does not exist at the moment. Creating ....')\n",
        "    os.mkdir(plot_dir)\n",
        "  print(plot_dir+' created ...')\n",
        "  if not os.path.isdir(os.path.join(plot_dir,history_path)) :\n",
        "    os.mkdir(os.path.join(plot_dir,history_path))\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path+' created...')\n",
        "  else:\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path,' exists ...')  \n",
        "    print('Adding the metric plots to ',folder_path)\n",
        "\n",
        "  #summarize history for accuracy\n",
        "\n",
        "  plt.plot(read_train['categorical_accuracy'])\n",
        "  plt.axhline(read_test['categorical_accuracy'], color='g', linestyle='--')\n",
        "  plt.title('model accuracy for fold_'+str(get_fold))\n",
        "  plt.ylabel('Categorical Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'accuracy_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  # summarize history for loss\n",
        "\n",
        "  plt.plot(read_train['loss'])\n",
        "  plt.axhline(read_test['loss'], color='g', linestyle='--')\n",
        "  plt.title('model loss for fold_'+str(get_fold))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  #plt.show()\n",
        "  plt.savefig(os.path.join(folder_path,'loss_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Precision \n",
        "  plt.plot(read_train['precision'])\n",
        "  plt.axhline(read_test['precision'], color='g', linestyle='--')\n",
        "  plt.title('model precision for fold_'+str(get_fold))\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'precision_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Recall \n",
        "\n",
        "  plt.plot(read_train['recall'])\n",
        "  plt.axhline(read_test['recall'], color='g', linestyle='--')\n",
        "  plt.title('Model Recall for fold_'+str(get_fold))\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'recall_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH8bGUCob9Se"
      },
      "source": [
        "print('########## BASELINE MODEL TRAINING FOR SHORT DS USING STRATIFIED K FOLD ... ######################')\n",
        "print('Loading CSVs')\n",
        "\n",
        "train_csv = pd.read_csv('/content/gdrive/MyDrive/data/short_df_collab.csv')\n",
        "#train_csv = pd.read_csv(os.path.join(data_dir,train_csv_file))\n",
        "train_csv = train_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "val_csv = pd.read_csv('/content/gdrive/MyDrive/data/val_short_collab.csv')\n",
        "#val_csv = pd.read_csv(os.path.join(data_dir,val_csv_file))\n",
        "val_csv = val_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "print('Data read and loading onto tensors.....')\n",
        "\n",
        "def get_label(file_path):\n",
        "# convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "# convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  #img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.image.per_image_standardization(img)\n",
        "  #img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "def process_Test(file_path):\n",
        "  #img = Image.open(file_path)\n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.image.per_image_standardization(img)\n",
        "  #img = tf.cast(img/255. ,tf.float32)\n",
        "  return img\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir(train_dir)]))\n",
        "\n",
        "\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "\n",
        "\n",
        "\n",
        "print('Cleaning prior cache ...')\n",
        "#purge(cache_dir,regu_v)   \n",
        "print('value cache cleared ....')\n",
        "#purge(cache_dir,regu_t)\n",
        "print('train cache cleared ....')\n",
        "training_data = train_csv\n",
        "valid_data = val_csv\n",
        "print(len(training_data))\n",
        "print(len(valid_data))\n",
        "########## USING CLASS WEIGHTS ########\n",
        "from sklearn.utils import class_weight\n",
        "class_weight = class_weight.compute_class_weight('balanced'\n",
        "                                               ,np.unique(training_data['Label'])\n",
        "                                               ,training_data['Label'])\n",
        "\n",
        "\n",
        "###############################\n",
        "\n",
        "\n",
        "train_data = training_data['Image']\n",
        "train_labels = training_data['Label']\n",
        "print(len(train_data))\n",
        "val_labels = valid_data['Label']\n",
        "val_data = valid_data['Image']\n",
        "print(len(val_data))\n",
        "steps = int(len(train_data)/t_batch_size)\n",
        "steps_val = int(len(val_data)/v_batch_size)\n",
        "\n",
        "train_tensor = tf.data.Dataset.from_tensors(train_data)\n",
        "val_tensor = tf.data.Dataset.from_tensors(val_data)\n",
        "train_tensor = train_tensor.unbatch()\n",
        "val_tensor = val_tensor.unbatch()\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "train_tensor = train_tensor.shuffle(len(train_data))\n",
        "val_tensor = val_tensor.shuffle(len(val_data))\n",
        "\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "print('Shuffling the dataframes internally for more randomness...')\n",
        "\n",
        "tra_tens = train_tensor.map(process_TL)\n",
        "val_tens = val_tensor.map(process_TL)\n",
        "print('tra_tens:',tf.data.experimental.cardinality(tra_tens))\n",
        "print('tra_tens:',tf.data.experimental.cardinality(val_tens))\n",
        "train_cache = tra_tens.cache()\n",
        "val_cache = val_tens.cache()\n",
        "print(tf.data.experimental.cardinality(train_cache))\n",
        "print(tf.data.experimental.cardinality(val_cache))\n",
        "print('Created cache ...')\n",
        "#normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "val_batch = val_cache.batch(v_batch_size)\n",
        "train_batch= train_cache.repeat(num_epochs).batch(t_batch_size)\n",
        "print(tf.data.experimental.cardinality(train_batch))\n",
        "print(tf.data.experimental.cardinality(val_batch))\n",
        "print(train_batch)\n",
        "print(train_batch)\n",
        "train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "print(tf.data.experimental.cardinality(train_ds))\n",
        "print(tf.data.experimental.cardinality(val_ds))\n",
        "print(train_ds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp8acn10Yalf"
      },
      "source": [
        "####Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRT0EU8sYaNK"
      },
      "source": [
        "tra_test = train_tensor.map(process_Test)\n",
        "#val_test = val_tensor.map(process_TL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL_JshZ2XQlM"
      },
      "source": [
        "for el in tra_test.take(1):\n",
        "  print(el)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5eF1SsDqiXJ"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weight = class_weight.compute_class_weight('balanced'\n",
        "                                               ,np.unique(training_data['Label'])\n",
        "                                               ,training_data['Label'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl1FLwM_pLTW"
      },
      "source": [
        "\n",
        "weights = {i:el for el,i in zip(class_weight,range(0,24))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgcCMzhLq3nC"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yFbodCdDHJ-"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "      \n",
        "    self.w1 = self.add_weight(name = 'weight_topic_1',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "    self.w2 = self.add_weight(name = 'weight_topic_2',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())  \n",
        "    \n",
        "    self.w3 = self.add_weight(name = 'weight_topic_3',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())       \n",
        "    \n",
        "    self.w4 = self.add_weight(name = 'weight_topic_4',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #Sum of the softmax output*topic weights\n",
        "    return tf.multiply(input1,self.w1) , tf.multiply(input2, self.w2) , tf.multiply(input3, self.w3),tf.multiply(input4, self.w4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i09159ixgcIv"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w1 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\"),trainable=True,name = 'weight_topic_1',initializer = 'random_normal'\n",
        "    \n",
        "   \n",
        "    self.w2 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_2',initializer = 'random_normal'\n",
        "    )  \n",
        "    self.w3 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_3',initializer = 'random_normal'\n",
        "    )       \n",
        "    self.w4 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_4',initializer = 'random_normal'\n",
        "    )\n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #name = 'tops'\n",
        "    return tf.multiply(input2,self.w1) + tf.multiply(input1, self.w2) + tf.multiply(input3, self.w3)+tf.multiply(input4, self.w4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQzrVNk2FMJZ"
      },
      "source": [
        "k_1 = Dense(1,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(1,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(1,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(1,name = 'topic_4')(model.output)\n",
        "wt_add = Wt_Topics(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "topic = Dense(256,activation='relu')(sum_layer)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfek9LS-ghXr"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(1024,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "# y = Dense(1)(model.output)\n",
        "k_1 = Dense(1,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(1,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(1,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(1,name = 'topic_4')(model.output)\n",
        "wt_add = Wt_Topics(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "topic = Dense(256,activation='relu')(sum_layer)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWlGhiaug6mh"
      },
      "source": [
        "cent_model_c = Model(inputs = base_model.input,outputs = final)\n",
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keZ7qasxvZN8"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_2x2_1.png', show_shapes=True, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        ")\n",
        "\n",
        "#plot_model(cent_model_c, to_file='/content/plot/plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH2ntstggosu"
      },
      "source": [
        "opti = tf.keras.optimizers.Adadelta(learning_rate=1e-1, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "cent_model_c.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3pEOdsDhw4o"
      },
      "source": [
        "Defining the callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eotkQKqdjVhn"
      },
      "source": [
        "cent_model_c.layers[-5].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3pHpjP8hwZN"
      },
      "source": [
        "#if not isfile(json_log):\n",
        "\n",
        "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
        "callbacks = [\n",
        "  ModelCheckpoint(\n",
        "    \n",
        "    filepath=os.path.join(model_dir,model_name),\n",
        "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "    monitor=\"categorical_accuracy\",\n",
        "    verbose=1,\n",
        "    save_freq = 'epoch',mode = 'max'\n",
        "  ),\n",
        "  EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "  ),\n",
        "  ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                          patience=3, min_lr=1e-5,verbose=1),\n",
        "  \n",
        "  LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: json_log.write(\n",
        "                json.dumps({'epoch': epoch, \n",
        "                            'categorical_accuracy': logs['categorical_accuracy'],\n",
        "                            'loss': logs['loss'],\n",
        "                            'topic_layer_weights': list(cent_model_c.layers[-5].get_weights())},cls = NumpyArrayEncoder) + '\\n'),\n",
        "            on_train_end=lambda logs: json_log.close()\n",
        "  )\n",
        "\n",
        "         \n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTfnSu6hVG1"
      },
      "source": [
        "Fitting the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1L1ewjMhGMt"
      },
      "source": [
        "history = cent_model_c.fit(train_ds,epochs=num_epochs,callbacks=callbacks,verbose=1,workers=8,use_multiprocessing = False,shuffle=True,steps_per_epoch = 100,)\n",
        "cent_model_c.save(os.path.join(model_dir,model_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kTWAoGcDwI8"
      },
      "source": [
        "from json import JSONEncoder\n",
        "import numpy\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkOP8SB8B0U"
      },
      "source": [
        "a =cent_model_c.layers[-5].get_weights()\n",
        "\n",
        "# [array([[0.4940912]], dtype=float32),\n",
        "#  array([[-0.6106522]], dtype=float32),\n",
        "#  array([[0.5864157]], dtype=float32),\n",
        "#  array([[-0.41294459]], dtype=float32)]\n",
        "type(a[0])\n",
        "# b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0V65wdeTwhg"
      },
      "source": [
        "## 3x3 Topic Testing Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEWks33Yhz9"
      },
      "source": [
        "###Parameter Definition and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXbRB8JmTwhi"
      },
      "source": [
        "\n",
        "\n",
        "#HYPER PARAMS #########\n",
        "num_folds = 4\n",
        "t_batch_size = 128\n",
        "v_batch_size = 128\n",
        "num_epochs =10\n",
        "model_name = 'test_topic'\n",
        "learning_rate = 0.4\n",
        "#Data directory stuff ###############################################\n",
        "cache_dir = ''\n",
        "train_csv = '/content/gdrive/MyDrive/data/train'\n",
        "train_dir = '/content/gdrive/MyDrive/data/train'#/data/s4133366/data/train'\n",
        "save_dir = '/content/gdrive/MyDrive/saved_models'#'/data/s4133366/saved_models'\n",
        "data_dir = '/content/gdrive/MyDrive/data'\n",
        "####################################################################\n",
        "train_csv_file = '/content/gdrive/MyDrive/data/short_df_collab.csv'\n",
        "val_csv_file = '/content/gdrive/MyDrive/data/val_short_collab.csv'\n",
        "####################################################################\n",
        "#cache_t = os.path.join(cache_dir,model_name+'_t')\n",
        "#cache_v = os.path.join(cache_dir,model_name+'_v')\n",
        "\n",
        "#regu_t = os.path.split(cache_t)[-1]+'*'\n",
        "#regu_v = os.path.split(cache_v)[-1]+'*'\n",
        "#print(regu_v,regu_t)\n",
        "\n",
        "#Change paths \n",
        "model_dir = os.path.join(save_dir,model_name)\n",
        "\n",
        "\n",
        "plot_dir = os.path.join(model_dir,'plots')\n",
        "history_dir = os.path.join(model_dir,'history')\n",
        "\n",
        "\n",
        "train_metrics_path = os.path.join(history_dir,model_name+'.npy')\n",
        "res_path = os.path.join(history_dir,'results_'+model_name+'.npy')\n",
        "chk_path = os.path.join(model_dir,model_name+'_chk')\n",
        "#Saving weights for retraining ....\n",
        "weights_path = os.path.join(model_dir,model_name+'_weights')\n",
        "#######################\n",
        "\n",
        "#Function for clearing cache\n",
        "# def purge(dir, pattern):\n",
        "#   for f in os.listdir(dir):\n",
        "#     if re.search(pattern, f):\n",
        "#       os.remove(os.path.join(dir, f))\n",
        "# #Function for making and storing the plots\n",
        "\n",
        "def plot_metrics(history_path,results_path):\n",
        "  read_train = np.load(history_path,allow_pickle=True).item()\n",
        "  read_test = np.load(results_path,allow_pickle=True).item()\n",
        "  history_path = os.path.split(history_path)[-1]\n",
        "  get_fold = history_path[-5]\n",
        "  history_path = history_path.replace('.npy','')\n",
        "  if not os.path.isdir(plot_dir):\n",
        "    print('Plot directory '+plot_dir+' does not exist at the moment. Creating ....')\n",
        "    os.mkdir(plot_dir)\n",
        "  print(plot_dir+' created ...')\n",
        "  if not os.path.isdir(os.path.join(plot_dir,history_path)) :\n",
        "    os.mkdir(os.path.join(plot_dir,history_path))\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path+' created...')\n",
        "  else:\n",
        "    folder_path = os.path.join(plot_dir,history_path)\n",
        "    print(folder_path,' exists ...')  \n",
        "    print('Adding the metric plots to ',folder_path)\n",
        "\n",
        "  #summarize history for accuracy\n",
        "\n",
        "  plt.plot(read_train['categorical_accuracy'])\n",
        "  plt.axhline(read_test['categorical_accuracy'], color='g', linestyle='--')\n",
        "  plt.title('model accuracy for fold_'+str(get_fold))\n",
        "  plt.ylabel('Categorical Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'accuracy_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  # summarize history for loss\n",
        "\n",
        "  plt.plot(read_train['loss'])\n",
        "  plt.axhline(read_test['loss'], color='g', linestyle='--')\n",
        "  plt.title('model loss for fold_'+str(get_fold))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  #plt.show()\n",
        "  plt.savefig(os.path.join(folder_path,'loss_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Precision \n",
        "  plt.plot(read_train['precision'])\n",
        "  plt.axhline(read_test['precision'], color='g', linestyle='--')\n",
        "  plt.title('model precision for fold_'+str(get_fold))\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'precision_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "  #summarize history for Recall \n",
        "\n",
        "  plt.plot(read_train['recall'])\n",
        "  plt.axhline(read_test['recall'], color='g', linestyle='--')\n",
        "  plt.title('Model Recall for fold_'+str(get_fold))\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(os.path.join(folder_path,'recall_'+str(history_path)+'.png'))\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWiaNPolYpC2"
      },
      "source": [
        "###Data Wrangling and model tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KuY_MK6Twhi"
      },
      "source": [
        "print('########## BASELINE MODEL TRAINING FOR SHORT DS USING STRATIFIED K FOLD ... ######################')\n",
        "print('Loading CSVs')\n",
        "\n",
        "train_csv = pd.read_csv(train_csv_file)\n",
        "#train_csv = pd.read_csv(os.path.join(data_dir,train_csv_file))\n",
        "train_csv = train_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "val_csv = pd.read_csv(val_csv_file)\n",
        "#val_csv = pd.read_csv(os.path.join(data_dir,val_csv_file))\n",
        "val_csv = val_csv.sample(frac=1,random_state = 42)\n",
        "\n",
        "print('Data read and loading onto tensors.....')\n",
        "\n",
        "def get_label(file_path):\n",
        "# convert the path to a list of path components separated by sep\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.cast(one_hot, tf.int32)\n",
        "\n",
        "def decode_img(img):\n",
        "# convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)  \n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [224, 224])\n",
        "\n",
        "def process_TL(file_path):\n",
        "  label = get_label(file_path) \n",
        "  img = tf.io.read_file(file_path) \n",
        "  img = decode_img(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = tf.cast(img/255. ,tf.float32)\n",
        "  return img, label\n",
        "\n",
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir(train_dir)]))\n",
        "\n",
        "\n",
        "print('LOADING THE DATA ON TO TENSORS......')\n",
        "\n",
        "\n",
        "\n",
        "print('Cleaning prior cache ...')\n",
        "#purge(cache_dir,regu_v)   \n",
        "print('value cache cleared ....')\n",
        "#purge(cache_dir,regu_t)\n",
        "print('train cache cleared ....')\n",
        "training_data = train_csv\n",
        "valid_data = val_csv\n",
        "print(len(training_data))\n",
        "print(len(valid_data))\n",
        "train_data = training_data['Image']\n",
        "train_labels = training_data['Label']\n",
        "print(len(train_data))\n",
        "val_labels = valid_data['Label']\n",
        "val_data = valid_data['Image']\n",
        "print(len(val_data))\n",
        "steps = int(len(train_data)/t_batch_size)\n",
        "steps_val = int(len(val_data)/v_batch_size)\n",
        "\n",
        "train_tensor_1 = tf.data.Dataset.from_tensors(train_data)\n",
        "val_tensor_1 = tf.data.Dataset.from_tensors(val_data)\n",
        "train_tensor = train_tensor_1.unbatch()\n",
        "val_tensor = val_tensor_1.unbatch()\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('un-shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "train_tensor = train_tensor.shuffle(len(train_data))\n",
        "val_tensor = val_tensor.shuffle(len(val_data))\n",
        "\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(train_tensor))\n",
        "print('shuffled_tensor:',tf.data.experimental.cardinality(val_tensor))\n",
        "\n",
        "print('Shuffling the dataframes internally for more randomness...')\n",
        "\n",
        "tra_tens = train_tensor.map(process_TL)\n",
        "val_tens = val_tensor.map(process_TL)\n",
        "print('tra_tens:',tf.data.experimental.cardinality(tra_tens))\n",
        "print('tra_tens:',tf.data.experimental.cardinality(val_tens))\n",
        "train_cache = tra_tens.cache()\n",
        "val_cache = val_tens.cache()\n",
        "print(tf.data.experimental.cardinality(train_cache))\n",
        "print(tf.data.experimental.cardinality(val_cache))\n",
        "print('Created cache ...')\n",
        "#normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "val_batch = val_cache.batch(v_batch_size)\n",
        "train_batch= train_cache.repeat(num_epochs).batch(t_batch_size)\n",
        "print(tf.data.experimental.cardinality(train_batch))\n",
        "print(tf.data.experimental.cardinality(val_batch))\n",
        "print(train_batch)\n",
        "print(train_batch)\n",
        "train_ds = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "print(tf.data.experimental.cardinality(train_ds))\n",
        "print(tf.data.experimental.cardinality(val_ds))\n",
        "print(train_ds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAm-NPHPSquN"
      },
      "source": [
        "for element in train_cache.take(1):\n",
        "  print(element)\n",
        "train_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVFU0XWJYuGE"
      },
      "source": [
        "###Defining the Topic Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GANvXAKTwhj"
      },
      "source": [
        "#3x3\n",
        "class Wt_Topics3x3(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics3x3, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "    w_init = tf.random_normal_initializer()\n",
        "    #1\n",
        "    self.w1 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_1'\n",
        "    )\n",
        "    self.w2 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_2'\n",
        "    )  \n",
        "    self.w3 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_3'\n",
        "    ) \n",
        "    #2      \n",
        "    self.w4 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_4'\n",
        "    )\n",
        "    self.w5 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_5'\n",
        "    )\n",
        "    self.w6 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_6'\n",
        "    )\n",
        "    #3\n",
        "    self.w7 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_7'\n",
        "    )\n",
        "    self.w8 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_8'\n",
        "    )\n",
        "    self.w9 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_9'\n",
        "    )\n",
        "    #self.bias = tf.Variable(tf.zeros([self.units]), name=\"weight_biases\",dtype = 'float32',trainable=True)\n",
        "\n",
        "  \n",
        "\n",
        "  def call(self, input1, input2, input3,input4,input5,input6,input7,input8,input9):\n",
        "    #name = 'tops'\n",
        "    return tf.multiply(input1,self.w1) + tf.multiply(input2, self.w2) + tf.multiply(input3, self.w3)+tf.multiply(input4, self.w4)+tf.multiply(input5,self.w5)+tf.multiply(input6,self.w6)+tf.multiply(input7,self.w7)+tf.multiply(input8,self.w8)+tf.multiply(input9,self.w9)#+self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "435ZyVg5ZE1j"
      },
      "source": [
        "###Defining the base model here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcy9Z4OiZEbh"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D( )(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(1024,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTOAQ8miZ0Et"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcG7WeMyauJD"
      },
      "source": [
        "###Declaring the rest of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiG-WcW8Twhj"
      },
      "source": [
        "\n",
        "k_1 = Dense(20,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(20,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(20,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(20,name = 'topic_4')(model.output)\n",
        "k_5 = Dense(20,name = 'topic_5')(model.output)\n",
        "k_6 = Dense(20,name = 'topic_6')(model.output)\n",
        "k_7 = Dense(20,name = 'topic_7')(model.output)\n",
        "k_8 = Dense(20,name = 'topic_8')(model.output)\n",
        "k_9 = Dense(20,name = 'topic_9')(model.output)\n",
        "wt_add = Wt_Topics3x3(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4,k_5,k_6,k_7,k_8,k_9)\n",
        "flat_2 = Flatten(name = 'flat_c')(sum_layer)\n",
        "# w_1 = tf.Variable(1.0,name='weight_topic_1',trainable=True )\n",
        "# w_2 = tf.Variable(1.0 ,name='weight_topic_2',trainable=True)\n",
        "# w_3 = tf.Variable(1.0 ,name='weight_topic_3',trainable=True )\n",
        "# w_4 = tf.Variable(1.0 ,name='weight_topic_4',trainable=True)\n",
        "# conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "# flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "# #Adding the Dense layers now\n",
        "# topic = Dense(4000,activation='tanh')(flat_2)\n",
        "# topic = Dropout(0.5)(topic)\n",
        "topic = Dense(256,activation='relu')(flat_2)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT4RBiIITwhk"
      },
      "source": [
        "cent_model_c = Model(inputs = base_model.input,outputs = final)\n",
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Jip9DnTwhk"
      },
      "source": [
        "cent_model_c.layers[-5].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5QD9nRwevo1"
      },
      "source": [
        "###Callbacks and compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXN6WK0cTwhk"
      },
      "source": [
        "opti = tf.keras.optimizers.Adadelta(learning_rate=1e-1, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "cent_model_c.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF9oKGbdTwhk"
      },
      "source": [
        "Defining the callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbLO6MipTwhk"
      },
      "source": [
        "#if not isfile(json_log):\n",
        "\n",
        "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
        "callbacks = [\n",
        "  ModelCheckpoint(\n",
        "    \n",
        "    filepath=os.path.join(model_dir,model_name),\n",
        "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "    monitor=\"categorical_accuracy\",\n",
        "    verbose=1,\n",
        "    save_freq = 'epoch',mode = 'max'\n",
        "  ),\n",
        "  EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "  ),\n",
        "  ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                          patience=3, min_lr=1e-5,verbose=1),\n",
        "  \n",
        "  LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: json_log.write(\n",
        "                json.dumps({'epoch': epoch, \n",
        "                            'categorical_accuracy': logs['categorical_accuracy'],\n",
        "                            'loss': logs['loss'],\n",
        "                            'topic_layer_weights': list(cent_model_c.layers[-5].get_weights())},cls = NumpyArrayEncoder) + '\\n'),\n",
        "            on_train_end=lambda logs: json_log.close()\n",
        "  )\n",
        "\n",
        "         \n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp8O-o94Twhk"
      },
      "source": [
        "Fitting the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThWWJZLtTwhk"
      },
      "source": [
        "history = cent_model_c.fit(train_ds,epochs=num_epochs,callbacks=callbacks,verbose=1,workers=8,use_multiprocessing = False,shuffle=True,steps_per_epoch = 100)\n",
        "cent_model_c.save(os.path.join(model_dir,model_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXr93CQlTwhk"
      },
      "source": [
        "from json import JSONEncoder\n",
        "import numpy\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0mIJsuDTwhk"
      },
      "source": [
        "a =cent_model_c.layers[-5].get_weights()\n",
        "\n",
        "# [array([[0.4940912]], dtype=float32),\n",
        "#  array([[-0.6106522]], dtype=float32),\n",
        "#  array([[0.5864157]], dtype=float32),\n",
        "#  array([[-0.41294459]], dtype=float32)]\n",
        "type(a[0])\n",
        "# b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTJ5or3L70wO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VamwXAVP0Vm"
      },
      "source": [
        "## Transferring weights of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZhZc9QJ7aWc"
      },
      "source": [
        "###This is performed in for Places365 dataset in order to load the trained baseline model for faster training of the Places365 Topic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwcaDM-LSqM1"
      },
      "source": [
        "model1 = keras.models.load_model('/content/gdrive/MyDrive/saved_models/baseline_p365_without_top')\n",
        "for layer in model1.layers:\n",
        "  layer.trainable = False\n",
        "  x = model1.output\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "model = Model(model1.input,preds)\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "k_1 = Dense(1,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(1,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(1,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(1,name = 'topic_4')(model.output)\n",
        "wt_add = Wt_Topics(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "topic = Dense(1024,activation='relu')(sum_layer)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(365,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = Model(inputs = model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9qT986sQIXA"
      },
      "source": [
        "model1 = keras.models.load_model('/content/gdrive/MyDrive/saved_models/baseline_p365_without_top')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WcvMFI8Qh2p"
      },
      "source": [
        "for layer in model1.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgvbS3VVQ7k3"
      },
      "source": [
        "x = model1.output\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "model = Model(model1.input,preds)\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj7UQP1TRYvG"
      },
      "source": [
        "k_1 = Dense(1,name = 'topic_1')(model.output)\n",
        "k_2 = Dense(1,name = 'topic_2')(model.output)\n",
        "k_3 = Dense(1,name = 'topic_3')(model.output)\n",
        "k_4 = Dense(1,name = 'topic_4')(model.output)\n",
        "wt_add = Wt_Topics(1)\n",
        "sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "topic = Dense(1024,activation='relu')(sum_layer)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(365,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = Model(inputs = model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOx_oje8RqM-"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGo7CZRtuwM"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuLOnGNTt5nS"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO-496yTkmfb"
      },
      "source": [
        "train = pd.read_csv('train_mod.csv')\n",
        "train = train[['Image','Label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpQRgTm1lRep"
      },
      "source": [
        "train.to_csv('train_new.csv')\n",
        "files.download('train_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23PL2ASTP5xX"
      },
      "source": [
        "#Topic Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naVNOZmm47F0"
      },
      "source": [
        "topic_model = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models/topic_weight_k4_places365_1_eval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnI4kpphIGXs"
      },
      "source": [
        "topic_model_2 = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models/topic_weight_k4_places365_2_eval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3OWjO496n70"
      },
      "source": [
        "topic_model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts8TTQoa6unK"
      },
      "source": [
        "topic_model.get_layer('resnet_out').get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W87WbrkG61fE"
      },
      "source": [
        "a = topic_model.get_layer('resnet_out').get_weights()\n",
        "weights = a[0]\n",
        "biases = a[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JMTqxoz7N8C"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdUo4a5l8iEJ"
      },
      "source": [
        "b= topic_model.get_layer('wt__topics').\n",
        "#weights_out = b[0]\n",
        "#biases_out = b[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1VWjWs8-ch"
      },
      "source": [
        "topic_model.get_layer('wt__topics').output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSvISHEC9X-D"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wZpmgD19Z3-"
      },
      "source": [
        "c = base_model.get_layer('predictions').get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9CHn42r9faC"
      },
      "source": [
        "c[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B7WL46NDsYw"
      },
      "source": [
        "img = keras.preprocessing.image.load_img(\n",
        "    '/content/gdrive/MyDrive/places365_sampled/val/airfield/Places365_val_00000435.jpg', target_size=(224,224,3)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "x= img_array\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEagd06QEI6X"
      },
      "source": [
        "x = x/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzq0JxK1cknx"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsBSLfp4_9Lj"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# with a Sequential model\n",
        "get_3rd_layer_output = K.function([topic_model_2.input],\n",
        "                                  [topic_model_2.get_layer('predictions').output])\n",
        "layer_output = get_3rd_layer_output([x])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8q8gfK-Wmrh"
      },
      "source": [
        "layer_output\n",
        "# array([[9.28297084e-08, 4.01620838e-07, 9.37058394e-06, 8.11473183e-06,\n",
        "#         8.00038033e-05, 4.70403074e-06, 3.78478546e-07, 3.41940085e-06,\n",
        "#         1.02050910e-06, 3.02595481e-07, 4.38917569e-09, 3.00811962e-08,\n",
        "#         1.02478033e-07, 2.52272923e-08, 2.13475346e-06, 5.36712790e-08,\n",
        "#         3.58117632e-07, 4.86063186e-07, 1.58908813e-06, 6.16022930e-08,\n",
        "#         1.55278556e-07, 7.01976148e-08, 3.24510040e-07, 1.70968656e-06,\n",
        "#         1.58559942e-07, 7.26027665e-08, 1.70016961e-08, 5.52901334e-08,\n",
        "#         1.41197640e-08, 1.67404338e-07, 3.79966174e-08, 1.40522147e-07,\n",
        "#         5.02371122e-08, 2.05255446e-07, 7.79303662e-07, 1.30195019e-08,\n",
        "#         3.17171782e-07, 9.17453690e-07, 9.98555993e-09, 2.40200990e-07,\n",
        "#         1.12698416e-07, 8.08853216e-08, 5.35295790e-08, 5.57538620e-08,\n",
        "#         6.81153125e-08, 8.33016571e-08, 1.08284425e-07, 5.90509011e-08,\n",
        "#         1.09753626e-06, 4.82614375e-08, 1.20412790e-06, 1.01259111e-05,\n",
        "#         2.72258660e-07, 2.23813696e-08, 2.93538953e-08, 3.92554938e-07,\n",
        "#         1.06888436e-07, 2.76315024e-08, 9.75771144e-08, 4.03090070e-07,\n",
        "#         1.00771410e-06, 1.73865510e-06, 1.05170876e-07, 8.13673893e-08,\n",
        "#         8.72278108e-08, 2.65132314e-07, 3.06861416e-08, 3.52348195e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amlJ9LIiNc6o"
      },
      "source": [
        "layer_output\n",
        "topic_lab = np.argmax(layer_output) \n",
        "topic_list = np.argsort(layer_output[0])[::-1][:10]\n",
        "#place_lab = np.argmax(layer_output) \n",
        "#place_lab\n",
        "topic_lab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZARenVE6ULRV"
      },
      "source": [
        "topic_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHcOe38MEcvq"
      },
      "source": [
        "#3.66683019e-15 for topic 1 for first label\n",
        "#9.282971e-08 from resnet 50 first element\n",
        "\n",
        "# layer_output\n",
        "checking = np.array(layer_output)\n",
        "# np.argmax(layer_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwKWSpS7NuK9"
      },
      "source": [
        "topic_model_2.get_layer('wt__topics').get_weights() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWPqG2kDOGb6"
      },
      "source": [
        "-6.754724e-08*9.282971e-08"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62MOf-4jM665"
      },
      "source": [
        "first_top_el1 = checking[0][0][0][0]\n",
        "second_top_el1 = checking[0][1][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZqfZx5hO6HJ"
      },
      "source": [
        "second_top_el1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0hSimfsJBX-"
      },
      "source": [
        "first_out = layer_output[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8VLNPklJdfp"
      },
      "source": [
        "top_1 = checking[0]\n",
        "top_1[0][0]\n",
        "# top_2 = checking[1][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBGBCE1TWYt"
      },
      "source": [
        "###Testing class weights and sample weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swnY1ZFKTdn0"
      },
      "source": [
        "class_weight = class_weight.compute_class_weight('balanced',np.unique(training_data['Label']),training_data['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvwQ1LHTxL2"
      },
      "source": [
        "sample_weights = class_weight/np.max(class_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eBGvuj_Xhjd"
      },
      "source": [
        "weights = compute_sample_weight(class_weight='balanced', y=training_data['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBIOKhlcYN2i"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26-cch3KYcok"
      },
      "source": [
        "weights_test = np.ones(shape=(len(training_data['Label']),))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C6en_AmYnSP"
      },
      "source": [
        "len(weights_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i8eG0cfTfE3"
      },
      "source": [
        "# class_weight = array([  9.42866162,   3.24956484,   2.17965558,   1.02266502,\n",
        "#          8.70337995,   0.41486111,   2.13845934,   3.19944302,\n",
        "#         30.35569106,   0.92191358,   0.45142667,   2.12386234,\n",
        "#         17.04908676,   0.37691803,   0.41486111,   1.2725801 ,\n",
        "#          5.76195988,   0.41486111,   2.05376788,   0.41486111,\n",
        "#          1.09462035,   3.08829611,   0.41486111, 311.14583333])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygaRDspKNbK3"
      },
      "source": [
        "##Defining callback to return the model intermediate outputs to JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-2NSfb-N0aH"
      },
      "source": [
        "###Class to obtain the output from ResNet-50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmSJdkw2NzTW"
      },
      "source": [
        "from keras import backend as K\n",
        "# def model_output(model,layer):\n",
        "# # with a Sequential model\n",
        "#   get_3rd_layer_output = K.function([model.input],\n",
        "#                                     [model.get_layer(layer).output])\n",
        "#   layer_output = get_3rd_layer_output([x])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ4C3-cXcsoX"
      },
      "source": [
        "img = keras.preprocessing.image.load_img(\n",
        "    '/content/gdrive/MyDrive/places365_sampled/val/airfield/Places365_val_00000435.jpg', target_size=(224,224,3)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "x= img_array\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTPdR3uzUlsb"
      },
      "source": [
        "####For implementations without custom layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDH3hwd4XZAg"
      },
      "source": [
        "Writing the JSON file to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPeK2_UFXYJE"
      },
      "source": [
        "json_out = open('test_out.json', mode='wt', buffering=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohH2JVpecAUN"
      },
      "source": [
        "get_t1_output = K.function([cent_model.input],[cent_model.get_layer('topic_1').output])\n",
        "test = get_t1_output(x)[0]\n",
        "test.shape\n",
        "get_t2_output = K.function([cent_model.input],[cent_model.get_layer('topic_2').output])\n",
        "get_t2_output(x)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr24CCYEhC_x"
      },
      "source": [
        "###Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZTwNZ24Nf0S"
      },
      "source": [
        "class OutputCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def on_train_begin(self,logs):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "        \n",
        "    \n",
        "    def on_train_batch_end(self,batch,logs = {}):\n",
        "        \n",
        "        keys = list(logs.keys())\n",
        "        get_pred_layer_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('predictions').output])\n",
        "        #Getting Resnet-50 softmax output\n",
        "        print('check_1')\n",
        "        print('Input:',self.model.input)\n",
        "      \n",
        "        resnet50_output = get_pred_layer_output([self.model.input])\n",
        "        print('Resnet:',resnet50_output)\n",
        "        #Getting the Intermediate topic outputs\n",
        "        get_t1_output = K.function(inputs=[self.model.input],outputs=[self.model.get_layer('topic_1').output])\n",
        "        get_t2_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('topic_2').output])\n",
        "        get_t3_output = K.function(inputs = [self.model.input],outputs=[self.model.get_layer('topic_3').output])\n",
        "        get_t4_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('topic_4').output])\n",
        "        print('check_2')\n",
        "        print('Calling the values before writing to file')\n",
        "        #Calling the values before writing to file\n",
        "        t1_out = list(get_t1_output([self.model.input]))#get_t1_output.outputs\n",
        "        t2_out = list(get_t2_output([self.model.input]))\n",
        "        t3_out = list(get_t3_output([self.model.input]))#get_t3_output.outputs\n",
        "        t4_out = list(get_t4_output([self.model.input]))#get_t4_output.outputs#\n",
        "        print('Topics: ',t1_out,t2_out,t3_out,t4_out)\n",
        "        print('check')\n",
        "        # json_out.write(\n",
        "        #         json.dumps({'batch': batch, \n",
        "        #                     'ResNet': list(resnet50_output),\n",
        "        #                     'Topic 1': t1_out,\n",
        "        #                     'Topic 2': t2_out,\n",
        "        #                     'Topic 3': t3_out,\n",
        "        #                     'Topic 4': t4_out,\n",
        "\n",
        "        #                     #,'model_output' : list(self.model.layers[-1].output)\n",
        "        #         },cls = NumpyArrayEncoder) + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpNVfrAu7RY"
      },
      "source": [
        "class OutputCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def on_train_begin(self,logs):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "        \n",
        "    \n",
        "    def on_train_batch_end(self,batch,logs = {}):\n",
        "        \n",
        "        keys = list(logs.keys())\n",
        "        get_pred_layer_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('predictions').output])\n",
        "        #Getting Resnet-50 softmax output\n",
        "        \n",
        "        #Getting the Intermediate topic outputs\n",
        "        get_t1_output = K.function(inputs=[self.model.input],outputs=[self.model.get_layer('topic_1').output])\n",
        "        get_t2_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('topic_2').output])\n",
        "        get_t3_output = K.function(inputs = [self.model.input],outputs=[self.model.get_layer('topic_3').output])\n",
        "        get_t4_output = K.function(inputs = [self.model.input],outputs = [self.model.get_layer('topic_4').output])\n",
        "        print('check_1')\n",
        "        print('Input:',self.model.input)\n",
        "        images = self.model.input\n",
        "        images = images.unbatch()\n",
        "        print('Images',images)\n",
        "        i=0\n",
        "        for image in self.model.input:\n",
        "          \n",
        "          print('image',i)\n",
        "          resnet50_output = get_pred_layer_output([image])\n",
        "          print('Resnet:',resnet50_output)\n",
        "          print('check_2')\n",
        "          print('Calling the values before writing to file')\n",
        "          #Calling the values before writing to file\n",
        "          t1_out = list(get_t1_output([image]))#get_t1_output.outputs\n",
        "          t2_out = list(get_t2_output([image]))\n",
        "          t3_out = list(get_t3_output([image]))#get_t3_output.outputs\n",
        "          t4_out = list(get_t4_output([image]))#get_t4_output.outputs#\n",
        "          print('Topics: ',t1_out,t2_out,t3_out,t4_out)\n",
        "          i = i+1\n",
        "        print('check done ...')\n",
        "        # json_out.write(\n",
        "        #         json.dumps({'batch': batch, \n",
        "        #                     'ResNet': list(resnet50_output),\n",
        "        #                     'Topic 1': t1_out,\n",
        "        #                     'Topic 2': t2_out,\n",
        "        #                     'Topic 3': t3_out,\n",
        "        #                     'Topic 4': t4_out,\n",
        "\n",
        "        #                     #,'model_output' : list(self.model.layers[-1].output)\n",
        "        #         },cls = NumpyArrayEncoder) + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKoEaaiq0tN7"
      },
      "source": [
        "#Trying out variations of model implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Viym6g0xAL"
      },
      "source": [
        "##Implementation 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug7BkXcM1Nzc"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "      \n",
        "    self.w1 = self.add_weight(name = 'weight_topic_1',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "    self.w2 = self.add_weight(name = 'weight_topic_2',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())  \n",
        "    \n",
        "    self.w3 = self.add_weight(name = 'weight_topic_3',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())       \n",
        "    \n",
        "    self.w4 = self.add_weight(name = 'weight_topic_4',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #Sum of the softmax output*topic weights\n",
        "    return tf.multiply(input1,self.w1), tf.multiply(input2, self.w2), tf.multiply(input3, self.w3),tf.multiply(input4, self.w4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVOgagypLDby"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        preprocessing.RandomFlip(\"horizontal\"),\n",
        "        preprocessing.RandomRotation(0.1),\n",
        "        preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a model that includes the augmentation stage\n",
        "input_shape = (224, 224, 3)\n",
        "classes = 24\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "# Augment images\n",
        "x = data_augmentation(inputs)\n",
        "# Rescale image values to [0, 1]\n",
        "x = preprocessing.Rescaling(1.0 / 255)(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy5gsjMCv3AH"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "wt_add = Wt_Topics(1000) #Calling the Topic Layer\n",
        "sum_layer = wt_add(base_model.output,base_model.output,\n",
        "                    base_model.output,base_model.output)\n",
        "conc = keras.layers.Concatenate()(sum_layer)\n",
        "flat_2 = keras.layers.Flatten(name = 'flat_2')(conc)\n",
        "topic_flat = Dense(2000,name = 'first_dense')(flat_2)\n",
        "topic = Dropout(0.45,name = 'drop_1')(topic_flat)\n",
        "topic = Dense(256,activation='relu')(topic)\n",
        "topic = Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "cent_model = Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QT6iLpD00vJ"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "#for layer in base_model.layers:\n",
        "#  layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#  layer.trainable = True\n",
        "\n",
        "wt_add = Wt_Topics(1)\n",
        "#new_tensor = tf.reshape(wt_add, shape=[tf.shape(wt_add)[0]*tf.shape(wt_add)[1],4])\n",
        "#sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "sum_layer = wt_add(base_model.output,base_model.output,base_model.output,base_model.output)\n",
        "\n",
        "conc = keras.layers.Concatenate()(sum_layer)\n",
        "#flat = keras.layers.GlobalAveragePooling1D()(conc)\n",
        "#test = keras.layers.GlobalAveragePooling1D()(flat)\n",
        "flat_2 = keras.layers.Flatten(name = 'flat_2')(conc)\n",
        "topic_flat = Dense(2000,name = 'first_dense')(flat_2)\n",
        "topic = Dropout(0.45,name = 'drop_1')(topic_flat)\n",
        "topic = Dense(256,activation='relu')(topic)\n",
        "topic = Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgrdkfRTflUa"
      },
      "source": [
        "opti = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, name=\"Adam\")\n",
        "cent_model.compile(optimizer=opti,loss='categorical_crossentropy',metrics=[keras.metrics.Precision(),keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4oQx7N-1fV0"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-HAhuY3WHK9"
      },
      "source": [
        "cent_model.fit(train_ds,verbose = 1,use_multiprocessing=False,workers=4,epochs = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnJUJ73BW_dS"
      },
      "source": [
        "cent_model.save('content/drive/myDrive/saved_models/test_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoZfEP-E1sKo"
      },
      "source": [
        "##Implementation 4\n",
        "\n",
        "With Max Pooling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrXUjWPi3zKl"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "      \n",
        "    self.w1 = self.add_weight(name = 'weight_topic_1',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "    self.w2 = self.add_weight(name = 'weight_topic_2',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())  \n",
        "    \n",
        "    self.w3 = self.add_weight(name = 'weight_topic_3',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())       \n",
        "    \n",
        "    self.w4 = self.add_weight(name = 'weight_topic_4',shape = (1, self.units),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #Sum of the softmax output*topic weights\n",
        "    topics = tf.multiply(input1,self.w1), tf.multiply(input2, self.w2), tf.multiply(input3, self.w3),tf.multiply(input4, self.w4)\n",
        "    \n",
        "    return topics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opha3hDl1yZN"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#  layer.trainable = True\n",
        "\n",
        "wt_add = Wt_Topics(1000)\n",
        "sum_layer = tf.stack(wt_add(base_model.output,base_model.output,base_model.output,base_model.output),axis = -1)\n",
        "\n",
        "\n",
        "max_pool = tf.keras.layers.MaxPool1D(pool_size=4)(sum_layer)\n",
        "flat_2 = tf.keras.layers.Flatten(name = 'flat_2')(max_pool)\n",
        "topic_flat = tf.keras.layers.Dense(1000,name = 'first_dense')(flat_2)\n",
        "topic = tf.keras.layers.Dropout(0.45,name = 'drop_1')(topic_flat)\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo3o2ZN1E0mi"
      },
      "source": [
        "###Implementation without custom layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9UM5jl9BhsM"
      },
      "source": [
        "#y = base_model.output\n",
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "k_1 = tf.keras.layers.Dense(1000,name = 'topic_1',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_2 = tf.keras.layers.Dense(1000,name = 'topic_2',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_3 = tf.keras.layers.Dense(1000,name = 'topic_3',use_bias=False,kernel_initializer='random_normal',activation = 'relu')(base_model.output)\n",
        "k_4 = tf.keras.layers.Dense(1000,name = 'topic_4',use_bias=False,kernel_initializer='random_normal',activation = 'relu')(base_model.output)\n",
        "stack = tf.stack([k_1,k_2,k_3,k_4],axis=-1)\n",
        "max_pool = tf.keras.layers.MaxPool1D(pool_size=4)(stack)\n",
        "#conc = tf.keras.layers.Concatenate(name = 'conc')([k_1,k_2,k_3,k_4])\n",
        "flat = tf.keras.layers.Flatten(name = 'flat')(max_pool)\n",
        "# #add = tf.keras.layers.Add(name = 'add_layer')([k_1,k_2,k_3,k_4])\n",
        "topic = tf.keras.layers.Dense(1000,activation='relu',name = 'first_dense')(flat)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "topic = tf.keras.layers.Dense(256,activation='softmax',name = 'second_dense')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax',name = 'places')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtAZ4EN-38SL"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OgmLqJx-xSv"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model,to_file = 'impl_5.png',  show_shapes=True, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9o67_Ac6iLh"
      },
      "source": [
        "tf.rank(cent_model.get_layer('wt__topics_15').output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZihS8DB0fiji"
      },
      "source": [
        "##Implementation 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K02THrw-kPq"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#  layer.trainable = True\n",
        "\n",
        "wt_add = Wt_Topics(1000)\n",
        "sum_layer = tf.stack(wt_add(base_model.output,base_model.output,base_model.output,base_model.output),axis = -1)\n",
        "#Entire sum_layer\n",
        "full = tf.keras.layers.Concatenate(axis = -1)(tf.unstack(sum_layer,axis = -1))\n",
        "\n",
        "dense_full = tf.keras.layers.Dense(1000,name = 'dense_full',activation='relu')(full)\n",
        "#Pool 1\n",
        "max_pool = tf.keras.layers.MaxPool1D(pool_size=4,name = 'pool_1')(sum_layer)\n",
        "flat_2 = tf.keras.layers.Flatten(name = 'flat_2')(max_pool)\n",
        "#Pool 2\n",
        "max_pool_2 = tf.keras.layers.MaxPool1D(pool_size=2,name = 'pool_2')(sum_layer)\n",
        "uns = tf.unstack(max_pool_2,axis = -1)\n",
        "test_layer = tf.keras.layers.Concatenate()(uns)\n",
        "flat_3 = tf.keras.layers.Flatten(name = 'flat_3')(test_layer)\n",
        "dense_pool = tf.keras.layers.Dense(1000,name = 'dense_pool',activation = 'relu')(flat_3)\n",
        "\n",
        "comb_layer = tf.keras.layers.Add(name = 'comb')([dense_full,flat_2,dense_pool])\n",
        "\n",
        "topic_flat = tf.keras.layers.Dense(1000,name = 'first_dense',activation='relu')(comb_layer)\n",
        "topic = tf.keras.layers.Dropout(0.25,name = 'drop_1')(topic_flat)\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Yg5sLgJp79"
      },
      "source": [
        "###Implementation without custom layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BWAzdDJJu3A"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model,to_file = 'impl_5_wo_custom.png',  show_shapes=True, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcXzjsx5E6jS"
      },
      "source": [
        "#y = base_model.output\n",
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "k_1 = tf.keras.layers.Dense(1000,name = 'topic_1',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_2 = tf.keras.layers.Dense(1000,name = 'topic_2',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_3 = tf.keras.layers.Dense(1000,name = 'topic_3',use_bias=False,kernel_initializer='random_normal',activation = 'relu')(base_model.output)\n",
        "k_4 = tf.keras.layers.Dense(1000,name = 'topic_4',use_bias=False,kernel_initializer='random_normal',activation = 'relu')(base_model.output)\n",
        "\n",
        "conc_1 = tf.keras.layers.Concatenate(name = 'full_tops')([k_1,k_2,k_3,k_4])\n",
        "dense_full = tf.keras.layers.Dense(1000,name = 'dense_full')(conc_1)\n",
        "#Creating first pooling layer\n",
        "stack = tf.stack([k_1,k_2,k_3,k_4],axis=-1)\n",
        "max_pool = tf.keras.layers.MaxPool1D(pool_size=4,name = 'pool_1')(stack)\n",
        "flat = tf.keras.layers.Flatten(name = 'flat')(max_pool)\n",
        "\n",
        "#Creating second pooling layer\n",
        "max_pool_2 = tf.keras.layers.MaxPool1D(pool_size=2,name = 'pool_2')(stack)\n",
        "flat_2 = tf.keras.layers.Flatten(name = 'flat_2')(max_pool_2)\n",
        "dense_pool = tf.keras.layers.Dense(1000,name = 'dense_pool',activation = 'relu')(flat_2)\n",
        "\n",
        "#Combining the layers\n",
        "comb_layer = tf.keras.layers.Add(name = 'comb')([dense_full,flat,dense_pool])\n",
        "\n",
        "topic = tf.keras.layers.Dense(1000,activation='relu',name = 'first_dense')(comb_layer)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "topic = tf.keras.layers.Dense(256,activation='softmax',name = 'second_dense')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax',name = 'places')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TutOY5R4hNOn"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaPGBAtoJfaA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQZf-RMtyhc"
      },
      "source": [
        "##Implementation 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU5R4tjklsxn"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXz86igqt7mI"
      },
      "source": [
        "img = keras.preprocessing.image.load_img(\n",
        "    '/content/gdrive/MyDrive/data/train/Museum/20160609_074119_000.jpg', target_size=(224,224,3)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "x= img_array\n",
        "x = preprocess_input(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFdDT1_Vv8nN"
      },
      "source": [
        "prediction_test = base_model.predict(x)\n",
        "pred = np.argmax(prediction_test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IZVAbztwWbk"
      },
      "source": [
        "topic_df_2 = pd.DataFrame(create_topics(prediction_test))\n",
        "topic_df_2.columns = ['Topic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCbRKg94xoTJ"
      },
      "source": [
        "topic_df.iloc[pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r3qFmfT79MO"
      },
      "source": [
        "decode_predictions(prediction_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiQKa7gIw3ZB"
      },
      "source": [
        "topic_df_3.equals(topic_df_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuJXld_eeJEk"
      },
      "source": [
        "topic_df_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB17OEgS_hCn"
      },
      "source": [
        "###Making declaration for storing the topics values for use in making TF-IDF per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wu6nJc2cEWy"
      },
      "source": [
        "def create_topics(predictions):\n",
        "  topics = decode_predictions(predictions,top = 1000)\n",
        "  t = pd.DataFrame(topics)\n",
        "  lis = []\n",
        "  for top in t:\n",
        "    a,b,c = t[top].explode()\n",
        "    lis.append(b)\n",
        "  return lis\n",
        "\n",
        "def get_topic_from_image(image_name):\n",
        "  #topic_list = []\n",
        "  img = keras.preprocessing.image.load_img(image_name, target_size=(224,224,3))\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)\n",
        "  x= img_array\n",
        "  predict = base_model.predict(x)\n",
        "  pred = np.argmax(predict)\n",
        "  img_topic = topic_df['Topic'].iloc[pred]\n",
        "  return img_topic\n",
        "\n",
        "def place_topics(place):\n",
        "  topics_per_class = []\n",
        "  class_name = place\n",
        "  df_class_images = csv_data[csv_data['Label']==class_name]\n",
        "  image_list = list(df_class_images['Image'])\n",
        "  for image in image_list:\n",
        "    print(image)\n",
        "    img_topic = get_topic_from_image(image)\n",
        "    topics_per_class.append(img_topic)\n",
        "  return topics_per_class\n",
        "\n",
        "def topic_place_dict(filename,dir):\n",
        "  t_p_dict = {}\n",
        "  csv_data = pd.read_csv(filename)\n",
        "  places = os.listdir(dir)\n",
        "  for place in places:\n",
        "    topics = place_topics(place)\n",
        "    t_p_dict.update({place:topics})\n",
        "  return t_p_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLOBX97rwVBI"
      },
      "source": [
        "def create_topics(predictions):\n",
        "  topics = decode_predictions(predictions,top = 1000)\n",
        "  t = pd.DataFrame(topics)\n",
        "  lis = []\n",
        "  for top in t:\n",
        "    a,b,c = t[top].explode()\n",
        "    lis.append(b)\n",
        "  return lis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPfQSbky_n_t"
      },
      "source": [
        "def get_topic_from_image(image_name):\n",
        "  #topic_list = []\n",
        "  img = keras.preprocessing.image.load_img(image_name, target_size=(224,224,3))\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)\n",
        "  x= img_array\n",
        "  predict = base_model.predict(x)\n",
        "  pred = np.argmax(predict)\n",
        "  img_topic = topic_df['Topic'].iloc[pred]\n",
        "  return img_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQHTe7FDB1zC"
      },
      "source": [
        "def place_topics(place):\n",
        "  topics_per_class = []\n",
        "   \n",
        "  class_name = place\n",
        "  df_class_images = csv_data[csv_data['Label']==class_name]\n",
        "  image_list = list(df_class_images['Image'])\n",
        "  for image in image_list:\n",
        "    print('Loaded:'+image)\n",
        "    img_topic = get_topic_from_image(image)\n",
        "    topics_per_class.append(img_topic)\n",
        "  return topics_per_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU_cqOZiXIW3"
      },
      "source": [
        "def topic_place_dict(filename,dir):\n",
        "  t_p_dict = {}\n",
        "  csv_data = pd.read_csv(filename)\n",
        "  places = os.listdir(dir)\n",
        "  for place in places:\n",
        "    topics = place_topics(place)\n",
        "    t_p_dict.update({place:topics})\n",
        "  return t_p_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO34xbxdXT8H"
      },
      "source": [
        "topic_place_dict('/content/gdrive/MyDrive/data/short_df_collab.csv','/content/gdrive/MyDrive/data/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dchCbgMjHYtt"
      },
      "source": [
        "office_list = place_topics('Water')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMJ2YT0GV6xy"
      },
      "source": [
        "test = {'Office':office_list,'Label_2':['abc','cde']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AElJEnORWjWY"
      },
      "source": [
        "test['Label_2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW6T_MmkfCwm"
      },
      "source": [
        "office_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBDP8QAI2ybA"
      },
      "source": [
        "## Impelmentation 7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Q_x1sY26_M"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "#x = base_model.output\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "#x = Flatten(name =\"flatten\")(x)\n",
        "#x = Dense(1024,activation=\"relu\")(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "#model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#    layer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idneOsEm7aPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_PyM_RE6UZN"
      },
      "source": [
        "#y = base_model.output\n",
        "k_1 = tf.keras.layers.Dense(1,name = 'topic_1',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_2 = tf.keras.layers.Dense(1,name = 'topic_2',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_3 = tf.keras.layers.Dense(1,name = 'topic_3',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "k_4 = tf.keras.layers.Dense(1,name = 'topic_4',use_bias=False,kernel_initializer='random_normal',activation='relu')(base_model.output)\n",
        "conc = tf.keras.layers.Concatenate(name = 'conc')([k_1,k_2,k_3,k_4])\n",
        "flat = tf.keras.layers.Flatten(name = 'flat')(conc)\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(flat)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ETqQpz8C8-"
      },
      "source": [
        "%pip install numpy==1.19.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoezOp-948sq"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEeGxZFm_l6h"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_imp7.png', show_shapes=True, show_dtype=True,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False,dpi=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwEgpbwd_xaC"
      },
      "source": [
        "cent_model.get_layer('topic_1').get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdconRAkLHfo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s62siVb8LIA4"
      },
      "source": [
        "## Impelmentation 8\n",
        "\n",
        "With skip connections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nJIBl_gLIA4"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "#x = base_model.output\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "#x = Flatten(name =\"flatten\")(x)\n",
        "#x = Dense(1024,activation=\"relu\")(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "#model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#    layer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCH77kBLIA4"
      },
      "source": [
        "k_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hfymD1LIA4"
      },
      "source": [
        "#y = base_model.output\n",
        "k_1 = tf.keras.layers.Dense(1000,name = 'topic_1',use_bias=False,kernel_initializer='random_normal')(base_model.output)\n",
        "k_2 = tf.keras.layers.Dense(1000,name = 'topic_2',use_bias=False,kernel_initializer='random_normal')(base_model.output)\n",
        "k_3 = tf.keras.layers.Dense(1000,name = 'topic_3',use_bias=False,kernel_initializer='random_normal')(base_model.output)\n",
        "k_4 = tf.keras.layers.Dense(1000,name = 'topic_4',use_bias=False,kernel_initializer='random_normal')(base_model.output)\n",
        "#conc = tf.keras.layers.Concatenate(name = 'conc')([k_1,k_2,k_3,k_4])\n",
        "#flat = tf.keras.layers.Flatten(name = 'flat')(conc)\n",
        "add = tf.keras.layers.Add(name = 'add_layer')([k_1,k_2,k_3,k_4])\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(add)\n",
        "topic = tf.keras.layers.Dropout(0.2)(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsPghApKJ1y"
      },
      "source": [
        "k_1.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1CtKXTELIA4"
      },
      "source": [
        "%pip install numpy==1.19.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9CtxZCHLIA5"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJDMLLULIA5"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_imp8.png', show_shapes=True, show_dtype=True,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False,dpi=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNCUR8tvLIA5"
      },
      "source": [
        "opti = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, name=\"Adam\")\n",
        "cent_model.compile(optimizer=opti,loss='categorical_crossentropy',metrics=[keras.metrics.Precision(),keras.metrics.CategoricalAccuracy()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya25poybZkgI"
      },
      "source": [
        "mo = OutputCallback()\n",
        "mo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XfcJuISYzcp"
      },
      "source": [
        "history = cent_model.fit(train_ds,epochs=10,shuffle=True,steps_per_epoch=10)\n",
        "cent_model.save('modelll')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYuomDa5acmV"
      },
      "source": [
        "cent_model.get_layer('predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGtTsZnkBfzW"
      },
      "source": [
        "##Implementation 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lO7ITx4MxEm"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "      \n",
        "    self.w1 = self.add_weight(name = 'weight_topic_1',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "    self.w2 = self.add_weight(name = 'weight_topic_2',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())  \n",
        "    \n",
        "    self.w3 = self.add_weight(name = 'weight_topic_3',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())       \n",
        "    \n",
        "    self.w4 = self.add_weight(name = 'weight_topic_4',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #Sum of the softmax output*topic weights\n",
        "    \n",
        "    return tf.multiply(input1,self.w1), tf.multiply(input2, self.w2),tf.multiply(input3, self.w3),tf.multiply(input4, self.w4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuF7xFnzB4aH"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#  layer.trainable = True\n",
        "\n",
        "wt_add = Wt_Topics(1000)\n",
        "#new_tensor = tf.reshape(wt_add, shape=[tf.shape(wt_add)[0]*tf.shape(wt_add)[1],4])\n",
        "#sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "sum_layer = wt_add(base_model.output,base_model.output,base_model.output,base_model.output)\n",
        "\n",
        "conc = keras.layers.Concatenate()(sum_layer)\n",
        "#flat = keras.layers.GlobalAveragePooling1D()(conc)\n",
        "#test = keras.layers.GlobalAveragePooling1D()(flat)\n",
        "flat_2 = tf.keras.layers.Flatten(name = 'flat_2')(conc)\n",
        "topic_flat = tf.keras.layers.Dense(1000,name = 'first_dense')(flat_2)\n",
        "topic = tf.keras.layers.Dropout(0.45,name = 'drop_1')(topic_flat)\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNIASJ2yCukM"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNTWCYZZDTM4"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_imp9.png', show_shapes=True, show_dtype=True,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False,dpi=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIt9AU48DjNs"
      },
      "source": [
        "abc = np.array(cent_model.get_layer('wt__topics_6').get_weights())\n",
        "abc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXC3JETMu7I"
      },
      "source": [
        "##Implementation 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h_PVWOrBiOC"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    self.sum1 = 0\n",
        "    self.sum2 = 0\n",
        "    self.sum3 = 0\n",
        "    self.sum4 = 0\n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "      \n",
        "    self.w1 = self.add_weight(name = 'weight_topic_1',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "    self.w2 = self.add_weight(name = 'weight_topic_2',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())  \n",
        "    \n",
        "    self.w3 = self.add_weight(name = 'weight_topic_3',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())       \n",
        "    \n",
        "    self.w4 = self.add_weight(name = 'weight_topic_4',shape = (self.units,),trainable = True,initializer = 'random_normal',regularizer = tf.keras.regularizers.l1_l2())\n",
        "    \n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #Sum of the softmax output*topic weights\n",
        "    self.sum1 += tf.multiply(input1,self.w1)\n",
        "    self.sum2 +=  tf.multiply(input2, self.w2)\n",
        "    self.sum3 +=  tf.multiply(input3, self.w3)\n",
        "    self.sum4 += tf.multiply(input4, self.w4)\n",
        "    return self.sum1,self.sum2,self.sum3,self.sum4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2_VXDJSN0Bn"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "#for layer in model.layers:\n",
        "#  layer.trainable = True\n",
        "\n",
        "wt_add = Wt_Topics(1000)\n",
        "#new_tensor = tf.reshape(wt_add, shape=[tf.shape(wt_add)[0]*tf.shape(wt_add)[1],4])\n",
        "#sum_layer = wt_add(k_1,k_2,k_3,k_4)\n",
        "sum_layer = wt_add(base_model.output,base_model.output,base_model.output,base_model.output)\n",
        "\n",
        "conc = keras.layers.Concatenate()(sum_layer)\n",
        "#flat = keras.layers.GlobalAveragePooling1D()(conc)\n",
        "#test = keras.layers.GlobalAveragePooling1D()(flat)\n",
        "flat_2 = tf.keras.layers.Flatten(name = 'flat_2')(conc)\n",
        "topic_flat = tf.keras.layers.Dense(1024,name = 'first_dense')(flat_2)\n",
        "topic = tf.keras.layers.Dropout(0.45,name = 'drop_1')(topic_flat)\n",
        "topic = tf.keras.layers.Dense(256,activation='relu')(topic)\n",
        "topic = tf.keras.layers.Dropout(0.2,name = 'drop_2')(topic)\n",
        "final = tf.keras.layers.Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model = tf.keras.models.Model(inputs = base_model.input,outputs = final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oi4ZufrPtpe"
      },
      "source": [
        "cent_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQsUe-1P9-G"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_imp10.png', show_shapes=True, show_dtype=True,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False,dpi=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFLTfO5eGFe"
      },
      "source": [
        "files.download('/content/topic_model_imp9.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKi1G2FgHbLZ"
      },
      "source": [
        "# Evaluating the Topic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1eS7h1HfW3"
      },
      "source": [
        "##Prediction for Implementation 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubaBr-wsBdYK"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS3f2y6xm9gM"
      },
      "source": [
        "test_data = pd.read_csv('/content/gdrive/MyDrive/data/test (1).csv')\n",
        "test_data = test_data[test_data['Label'] == 'Office']\n",
        "test_data = test_data['Image'].apply(lambda x:x.replace('/data/s4133366/data','/content/gdrive/MyDrive/data'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn7fovFZkgrD"
      },
      "source": [
        "test = pd.read_csv('/content/place.csv')\n",
        "test = test[['Image','Object']]\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0rq3Hefmsiq"
      },
      "source": [
        "test = test[['Object','Image']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucYMBqJGnlIc"
      },
      "source": [
        "total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIFnkZnanrGU"
      },
      "source": [
        "total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuQGwXkYn8tD"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5sdn4SobcM"
      },
      "source": [
        "test.Object.value_counts().sort_values().plot(kind = 'barh')\n",
        "plt.xlabel('Number of Images')\n",
        "plt.ylabel('Objects')\n",
        "plt.title('Top 10 Objects highlighted by Resnet-50 for Place: Water')\n",
        "plt.xticks([])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzIHnNPapidC"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEtx3kxIk7jq"
      },
      "source": [
        "total = test.groupby('Object').count()\n",
        "bar1 = sn.barplot(x=\"Object\", data=total, color='darkblue')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv3uYXzCnx9L"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXWb9wBUl0yX"
      },
      "source": [
        "data = create_numpy_img('/content/gdrive/MyDrive/data/test/Office/20160621_102654_000.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVUTleB_9Lsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg312ifBqMxj"
      },
      "source": [
        "correct = []\n",
        "incorrect = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0X_56h2n7HQ"
      },
      "source": [
        "for test in test_data:\n",
        "  data = create_numpy_img(test)\n",
        "# cent_model = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models/topic_weight_k4_imp_4_4_eval')\n",
        "  prediction = cent_model.predict(data)\n",
        "  preds = np.argmax(prediction)\n",
        "  if preds == 14:\n",
        "    correct.append(test)\n",
        "  else:\n",
        "    print('Predicted Value = ',preds)\n",
        "    incorrect.append(test)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGOHYrWSC8Gm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te-RVAFKvy9I"
      },
      "source": [
        "corr = pd.DataFrame(correct)\n",
        "corr.to_csv('correct.csv')\n",
        "files.download('correct.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrjlwMWxDROv"
      },
      "source": [
        "###Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWZOUG0hDYBJ"
      },
      "source": [
        "cent_model = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models/topic_weight_k4_places_20_imp1_1_eval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H47Vip2McKDS"
      },
      "source": [
        "###Output Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoxN--Abyca"
      },
      "source": [
        "from keras import backend as K\n",
        "get_base_output = K.function([cent_model.input],[cent_model.get_layer('predictions').output])\n",
        "get_t1_output = K.function([cent_model.input],[cent_model.get_layer('topic_1').output])\n",
        "get_t2_output = K.function([cent_model.input],[cent_model.get_layer('topic_2').output])\n",
        "get_t3_output = K.function([cent_model.input],[cent_model.get_layer('topic_3').output])\n",
        "get_t4_output = K.function([cent_model.input],[cent_model.get_layer('topic_4').output])\n",
        "# get_t5_output = K.function([cent_model.input],[cent_model.get_layer('topic_5').output])\n",
        "# get_t6_output = K.function([cent_model.input],[cent_model.get_layer('topic_6').output])\n",
        "# get_t7_output = K.function([cent_model.input],[cent_model.get_layer('topic_7').output])\n",
        "# get_t8_output = K.function([cent_model.input],[cent_model.get_layer('topic_8').output])\n",
        "# get_t9_output = K.function([cent_model.input],[cent_model.get_layer('topic_9').output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxMLpJVKDa_3"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model, to_file='topic_model_imp9_1.png', show_shapes=True, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD7SZdlFLEc5"
      },
      "source": [
        "data = create_numpy_img('/content/gdrive/MyDrive/data/test/Office/20170608_115437_000.jpg')\n",
        "# cent_model = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models/topic_weight_k4_imp_4_4_eval')\n",
        "prediction = cent_model.predict(data)\n",
        "preds = np.argmax(prediction)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut2pVFoINKM3"
      },
      "source": [
        "###Resnet output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84gxdZU2Dc7J"
      },
      "source": [
        "x = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HBSCn14NLzv"
      },
      "source": [
        "from keras import backend as K\n",
        "get_base_output = K.function([cent_model.input],[cent_model.get_layer('predictions').output])\n",
        "resnet = get_base_output(x)[0]\n",
        "#print('Resnet output:',resnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsB0EPELGYHE"
      },
      "source": [
        "resnet_test = np.squeeze(resnet)\n",
        "resnet_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmvS5LOODmBy"
      },
      "source": [
        "n=10\n",
        "best_topics_resnet = np.argsort(resnet_test)[-n:]\n",
        "best_topics_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWpYGEYgeTs-"
      },
      "source": [
        "array([512, 418, 584, 813, 432, 677, 644, 683, 499, 111])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X74d9Nr4IJnb"
      },
      "source": [
        "resnet_topic_t10_values = []\n",
        "for i in best_topics_resnet:\n",
        "  resnet_topic_t10_values.append(resnet_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VHBerhiIgRJ"
      },
      "source": [
        "resnet_topic_t10_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhPrNkCgI1l8"
      },
      "source": [
        "all_resnet_sum = np.sum(resnet_test)\n",
        "top10_resnet_sum = np.sum(resnet_topic_t10_values)\n",
        "print('Top 10 Sum: ',top10_resnet_sum)\n",
        "print('Entire Resnet topics sum: ',all_resnet_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBpcdlCpGp4S"
      },
      "source": [
        "array([[512, 418, 584, 813, 432, 677, 644, 683, 499, 111]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt3awO_pNRqP"
      },
      "source": [
        "###Topic 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsX-Z3zQNOYL"
      },
      "source": [
        "#Topic 1\n",
        "get_t1_output = K.function([cent_model.input],[cent_model.get_layer('topic_1').output])\n",
        "topic_1 = get_t1_output(x)[0]\n",
        "# print('Topic 1:',topic_1)\n",
        "top_1 = np.divide(topic_1,resnet)\n",
        "# print('Topic 1 Values',top_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIz0VDNVL29m"
      },
      "source": [
        "topic_1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u0zXd-xLlb4"
      },
      "source": [
        "n=10\n",
        "topic_1_test = np.squeeze(topic_1)\n",
        "best_topics_topic_1 = np.argsort(topic_1_test)[-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Qbbmg3ciRm"
      },
      "source": [
        "rank_topic_1 = np.argsort(topic_1_test)\n",
        "best_topics_topic_1 = rank_topic_1[::-1][:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86rhs5I9b7oy"
      },
      "source": [
        "best_topics_topic_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT-OoIm8c0eT"
      },
      "source": [
        "array([163, 729, 585, 813,  98, 463, 268,  31, 916, 788])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKH0vof5EDLP"
      },
      "source": [
        "#Without squeeze,Dont run\n",
        "#best_topics_topic_1 = np.argsort(topic_1, axis=1)[:,-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HVTH_TMuEI"
      },
      "source": [
        "####Top 10 Topic Indexes from Topic 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_jbZmkrLkYk"
      },
      "source": [
        "topic1_t10_values = []\n",
        "for i in best_topics_topic_1:\n",
        "  topic1_t10_values.append(topic_1_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQjU2YjEXoa"
      },
      "source": [
        "topic1_t10_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwm_Mt0PM6L0"
      },
      "source": [
        "all_topic_1_sum = np.sum(topic_1_test)\n",
        "top10_topic_1_sum = np.sum(topic1_t10_values)\n",
        "print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "print('Entire Topic 1 topic sum: ',all_topic_1_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uxg-vUPN6BY"
      },
      "source": [
        "####Top 10 Topics for Topic 1 taken from Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt_iQJAvOLWs"
      },
      "source": [
        "best_topics_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRq7MUvbLXaH"
      },
      "source": [
        "topic_1_t10_resnet = []\n",
        "for i in best_topics_resnet:\n",
        "  topic_1_t10_resnet.append(topic_1_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF4JsdcoOPYo"
      },
      "source": [
        "topic_1_t10_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6IrYTkuOdcl"
      },
      "source": [
        "all_topic_1_sum = np.sum(topic_1_test)\n",
        "top10_topic_1_sum_resnet = np.sum(topic_1_t10_resnet)\n",
        "print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "print('Entire Topic 1 topic sum: ',all_topic_1_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6F2XLw13C0W"
      },
      "source": [
        "top_1_hist = top_1.squeeze()\n",
        "top_1_hist = pd.DataFrame(top_1_hist)\n",
        "top_1_hist\n",
        "plt.plot(top_1_hist)\n",
        "plt.title('Topic 1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0fMu8tNbZQ"
      },
      "source": [
        "###Topic 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYASkKtwHvV3"
      },
      "source": [
        "#Topic 2\n",
        "get_t2_output = K.function([cent_model.input],[cent_model.get_layer('topic_2').output])\n",
        "topic_2 = get_t2_output(x)\n",
        "# print('Topic 2:',topic_2)\n",
        "top_2 = np.divide(topic_2,resnet)\n",
        "# print('Topic 2 Values',top_2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjTzNpAoS0Pn"
      },
      "source": [
        "n=10\n",
        "topic_2_test = np.squeeze(topic_2)\n",
        "best_topics_topic_2 = np.argsort(topic_2_test)[-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-TaeCZoEkcl"
      },
      "source": [
        "#Without squeeze .. Dont run\n",
        "best_topics_topic_2 = np.argsort(topic_2, axis=1)[:,-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJ6XNrcUtH4"
      },
      "source": [
        "array([596, 646,  74, 255, 805,  22, 405, 726, 942, 140])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TwxaZR_Eqhc"
      },
      "source": [
        "best_topics_topic_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MJlxr0kRqfs"
      },
      "source": [
        "####Top 10 Topic Indexes from Topic 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDNjNYZZRqft"
      },
      "source": [
        "topic2_t10_values = []\n",
        "for i in best_topics_topic_2:\n",
        "  topic2_t10_values.append(topic_2_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ8uEFKiRqft"
      },
      "source": [
        "topic2_t10_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGi08GQRqft"
      },
      "source": [
        "all_topic_2_sum = np.sum(topic_2_test)\n",
        "top10_topic_2_sum = np.sum(topic2_t10_values)\n",
        "print('Top 10 Sum from Topic 2: ',top10_topic_2_sum)\n",
        "print('Entire Topic 2 topic sum: ',all_topic_2_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rNXSK2I5ABT"
      },
      "source": [
        "top_2_hist = top_2.squeeze()\n",
        "top_2_hist = pd.DataFrame(top_2_hist)\n",
        "# top_2_hist\n",
        "plt.plot(top_2_hist)\n",
        "plt.title('Topic 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxKpgHGCSGEK"
      },
      "source": [
        "####Top 10 Topics for Topic 1 taken from Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcqnFXS4SGEL"
      },
      "source": [
        "best_topics_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M3fXtADSGEL"
      },
      "source": [
        "topic_2_t10_resnet = []\n",
        "for i in best_topics_resnet:\n",
        "  topic_2_t10_resnet.append(topic_2_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbIA19iSGEL"
      },
      "source": [
        "topic_2_t10_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE__00XGSGEM"
      },
      "source": [
        "all_topic_2_sum = np.sum(topic_2_test)\n",
        "top10_topic_2_sum_resnet = np.sum(topic_2_t10_resnet)\n",
        "print('Sum of Top 10 indexes from Resnet: ',top10_topic_2_sum_resnet)\n",
        "print('Entire Topic 2 topic sum: ',all_topic_2_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK6fDsdfNfNO"
      },
      "source": [
        "###Topic 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfzjYBmXNgt-"
      },
      "source": [
        "get_t3_output = K.function([cent_model.input],[cent_model.get_layer('topic_3').output])\n",
        "topic_3 = get_t3_output(x)[0]\n",
        "# print('Topic 3:',topic_3)\n",
        "top_3 = np.divide(topic_3,resnet)\n",
        "# print('Topic 3 Values',top_3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWsluY9fFMY0"
      },
      "source": [
        "#Without squeeze .. Dont run\n",
        "best_topics_topic_3 = np.argsort(topic_3, axis=1)[:,-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI4TsfrnFUo6"
      },
      "source": [
        "n =10\n",
        "topic_3_test = np.squeeze(topic_3)\n",
        "best_topics_topic_3 = np.argsort(topic_3_test)[-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z3jpbXGXmvw"
      },
      "source": [
        "best_topics_topic_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74nsRo0u0tWZ"
      },
      "source": [
        "top_3_hist = top_3.squeeze()\n",
        "top_3_hist = pd.DataFrame(top_3_hist)\n",
        "top_3_hist\n",
        "plt.plot(top_3_hist)\n",
        "plt.title('Topic 3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo-rMjfXRxUg"
      },
      "source": [
        "####Top 10 Topic Indexes from Topic 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5PvjuohRxUg"
      },
      "source": [
        "topic3_t10_values = []\n",
        "for i in best_topics_topic_3:\n",
        "  topic3_t10_values.append(topic_3_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O9x04xiRxUh"
      },
      "source": [
        "topic3_t10_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os3ixrWbRxUh"
      },
      "source": [
        "all_topic_3_sum = np.sum(topic_3_test)\n",
        "top10_topic_3_sum = np.sum(topic3_t10_values)\n",
        "print('Top 10 Sum: ',top10_topic_3_sum)\n",
        "print('Entire Topic 1 topic sum: ',all_topic_3_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbtGGJTOSTT9"
      },
      "source": [
        "####Top 10 Topics for Topic 3 taken from Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_nA1FyhSTT9"
      },
      "source": [
        "best_topics_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfwkrXalSTT-"
      },
      "source": [
        "topic_3_t10_resnet = []\n",
        "for i in best_topics_resnet:\n",
        "  topic_3_t10_resnet.append(topic_3_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0zSJV7xSTT-"
      },
      "source": [
        "topic_3_t10_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKkORDyoY9c9"
      },
      "source": [
        "[0.0049747974,\n",
        " 0.004279796,\n",
        " 0.0,\n",
        " 0.0034174433,\n",
        " 0.0007338119,\n",
        " 0.0049685836,\n",
        " 0.004004097,\n",
        " 0.0,\n",
        " 0.0,\n",
        " 0.001525932]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVx6WryvSTT-"
      },
      "source": [
        "all_topic_3_sum = np.sum(topic_3_test)\n",
        "top10_topic_3_sum_resnet = np.sum(topic_3_t10_resnet)\n",
        "print('Top 10 Sum: ',top10_topic_3_sum_resnet)\n",
        "print('Entire Topic 1 topic sum: ',all_topic_3_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlw0G3UNmMl"
      },
      "source": [
        "###Topic 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmEif25gNnl3"
      },
      "source": [
        "#Topic 4\n",
        "get_t4_output = K.function([cent_model.input],[cent_model.get_layer('topic_4').output])\n",
        "topic_4 = get_t4_output(x)[0]\n",
        "# print('Topic 4:',topic_4)\n",
        "top_4 = np.divide(topic_4,resnet)\n",
        "# print('Topic 4 Values',top_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEmHdIzWFkXU"
      },
      "source": [
        "best_topics_topic_4 = np.argsort(topic_4, axis=1)[:,-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJTki2byFqgT"
      },
      "source": [
        "n=10\n",
        "topic_4_test = np.squeeze(topic_4)\n",
        "best_topics_topic_4 = np.argsort(topic_4_test)[-n:]\n",
        "best_topics_topic_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJPXWzDh5R1j"
      },
      "source": [
        "top_4_hist = top_4.squeeze()\n",
        "top_4_hist = pd.DataFrame(top_4_hist)\n",
        "top_4_hist\n",
        "plt.plot(top_4_hist)\n",
        "plt.title('Topic 4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu70Pn51R3rz"
      },
      "source": [
        "####Top 10 Topic Indexes from Topic 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4KlldpVR3rz"
      },
      "source": [
        "topic4_t10_values = []\n",
        "for i in best_topics_topic_1:\n",
        "  topic4_t10_values.append(topic_4_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyuqHaZ-R3rz"
      },
      "source": [
        "topic4_t10_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbMgY_2BaDFG"
      },
      "source": [
        "[0.0072678076,\n",
        " 0.0,\n",
        " 0.015088149,\n",
        " 0.006569425,\n",
        " 0.0035689166,\n",
        " 0.009058334,\n",
        " 0.002467542,\n",
        " 0.010582676,\n",
        " 0.0030508689,\n",
        " 0.0007867259]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYrQmzQSR3rz"
      },
      "source": [
        "all_topic_4_sum = np.sum(topic_4_test)\n",
        "top10_topic_4_sum = np.sum(topic4_t10_values)\n",
        "print('Top 10 Sum for Topic 1: ',top10_topic_4_sum)\n",
        "print('Entire Topic 1 topic sum: ',all_topic_4_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zvd7c-3Sb59"
      },
      "source": [
        "####Top 10 Topics for Topic 4 taken from Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSYDKzuTSb59"
      },
      "source": [
        "best_topics_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0COGIa3MSb5-"
      },
      "source": [
        "topic_4_t10_resnet = []\n",
        "for i in best_topics_resnet:\n",
        "  topic_4_t10_resnet.append(topic_4_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOGZGAAsSb5-"
      },
      "source": [
        "topic_4_t10_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAHF21UbbXJ0"
      },
      "source": [
        "[0.013382193,\n",
        " 0.0045781448,\n",
        " 0.0,\n",
        " 0.006569425,\n",
        " 0.0016026746,\n",
        " 0.0022247692,\n",
        " 0.008066697,\n",
        " 0.0,\n",
        " 0.006562537,\n",
        " 0.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypYdLpMMSb5-"
      },
      "source": [
        "all_topic_4_sum = np.sum(topic_4_test)\n",
        "top10_topic_4_sum_resnet = np.sum(topic_4_t10_resnet)\n",
        "print('Top 10 values from Resnet for Topic 4: ',top10_topic_4_sum_resnet)\n",
        "print('Entire Topic 4 topic sum: ',all_topic_4_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUtjSjaXHjBQ"
      },
      "source": [
        "###Declaration for data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsiRYN6EHd_o"
      },
      "source": [
        "def create_numpy_img(image):\n",
        "  img = keras.preprocessing.image.load_img(\n",
        "      image, target_size=(224,224,3)\n",
        "  )\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = img_array/255.\n",
        "  img_array = tf.expand_dims(img_array, 0)\n",
        "  x= img_array\n",
        "  \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JadQVKAGSb"
      },
      "source": [
        "###Declaration for creating image arrays with images from a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cq3d-xaARmO"
      },
      "source": [
        "def create_numpy_img_mul(imageList):\n",
        "  img_list = []\n",
        "  for image in imageList:\n",
        "    print('Image: ',image)\n",
        "    img = keras.preprocessing.image.load_img(\n",
        "        image, target_size=(224,224,3)\n",
        "    )\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = img_array/255.\n",
        "    # img_array = tf.expand_dims(img_array, 0)\n",
        "    img_list.append(img_array)\n",
        "  x = tf.stack(img_list)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWiskSPnbd11"
      },
      "source": [
        "##Creating a Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN3TWaPBzHMp"
      },
      "source": [
        "test_data = pd.read_csv('/content/gdrive/MyDrive/data/train (4).csv')\n",
        "np.unique(test_data['Label'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlXlYIhUm5vy"
      },
      "source": [
        "test_data = pd.read_csv('/content/train_p20_final.csv')\n",
        "test_data['Image'] = test_data['Image'].apply(lambda x: x.replace('/data/s4133366/places365_20','/content/gdrive/MyDrive/places365_sampled')) \n",
        "test_data = test_data[test_data['Label']=='volcano']#'underwater-ocean_deep'\n",
        "test_data = test_data.sample(5)\n",
        "images = list(test_data['Image'])\n",
        "images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqJFFQ7PC6_Z"
      },
      "source": [
        "type(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C-IxAZZDD4M"
      },
      "source": [
        "imgg = create_numpy_img_mul(images)\n",
        "np.save('office.npy',imgg)\n",
        "files.download('office.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiuvnqxXEh33"
      },
      "source": [
        "np.save('office.npy',imgg)\n",
        "files.download('office.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i56jGLaaCGEQ"
      },
      "source": [
        "len(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckTwL4I8nlWv"
      },
      "source": [
        "images = list(test_data['Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdkJrdSXn3Km"
      },
      "source": [
        "images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJFUbsWQbrer"
      },
      "source": [
        "def create_Topic_data(image_list,number):\n",
        "\n",
        "  n = number\n",
        "  sum_resnet_total = []\n",
        "  top10_resnet_total = []\n",
        "  \n",
        "  #Collecting the best performing indexes\n",
        "  best_topic_resnet_list = []\n",
        "  best_topics_topic1_list = []\n",
        "  best_topics_topic2_list = []\n",
        "  best_topics_topic3_list = []\n",
        "  best_topics_topic4_list = []\n",
        "  all_topic_1_sum_list = []\n",
        "  all_topic_2_sum_list = []\n",
        "  all_topic_3_sum_list = []\n",
        "  all_topic_4_sum_list = []\n",
        "\n",
        "  top10_topic_1_total = []\n",
        "  top10_topic_1_total_from_resnet = []\n",
        "\n",
        "  top10_topic_2_total = []\n",
        "  top10_topic_2_total_from_resnet = []\n",
        "\n",
        "  top10_topic_3_total = []\n",
        "  top10_topic_3_total_from_resnet = []\n",
        "\n",
        "  top10_topic_4_total = []\n",
        "  top10_topic_4_total_from_resnet = []\n",
        "  class_name = image_list[0]\n",
        "  class_name = class_name.split('/')[-2]\n",
        "  print('Class: ',class_name)\n",
        "  for image in image_list:\n",
        "    print(image)\n",
        "    data = create_numpy_img(image)\n",
        "    x = data\n",
        "\n",
        "    #Getting the Resnet output\n",
        "    resnet = get_base_output(x)[0]\n",
        "    resnet_test = np.squeeze(resnet)\n",
        "    best_topics_resnet = np.argsort(resnet_test)[-n:]\n",
        "    best_topic_resnet_list.append(best_topics_resnet)\n",
        "    resnet_topic_t10_values = []\n",
        "    for i in best_topics_resnet:\n",
        "      resnet_topic_t10_values.append(resnet_test[i])\n",
        "    all_resnet_sum = np.sum(resnet_test)\n",
        "    top10_resnet_sum = np.sum(resnet_topic_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_resnet_sum)\n",
        "    # print('Entire Resnet topics sum: ',all_resnet_sum)  \n",
        "    \n",
        "    #Adding to list\n",
        "    top10_resnet_total.append(top10_resnet_sum)\n",
        "    sum_resnet_total.append(all_resnet_sum)\n",
        "    \n",
        "    #Topic 1\n",
        "    topic_1 = get_t1_output(x)[0]\n",
        "\n",
        "    topic_1_test = np.squeeze(topic_1)\n",
        "    best_topics_topic_1 = np.argsort(topic_1_test)[-n:]\n",
        "    best_topics_topic1_list.append(best_topics_topic_1)\n",
        "\n",
        "    topic1_t10_values = []\n",
        "    for i in best_topics_topic_1:\n",
        "      topic1_t10_values.append(topic_1_test[i])\n",
        "\n",
        "    all_topic_1_sum = np.sum(topic_1_test)\n",
        "    top10_topic_1_sum = np.sum(topic1_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_1_total.append(top10_topic_1_sum)\n",
        "\n",
        "    topic_1_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_1_t10_resnet.append(topic_1_test[i])\n",
        "\n",
        "    all_topic_1_sum = np.sum(topic_1_test)\n",
        "    top10_topic_1_sum_resnet = np.sum(topic_1_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_1_sum_list.append(all_topic_1_sum)\n",
        "\n",
        "    top10_topic_1_total_from_resnet.append(top10_topic_1_sum_resnet)\n",
        "\n",
        "    #Topic 2\n",
        "    topic_2 = get_t2_output(x)\n",
        "    topic_2_test = np.squeeze(topic_2)\n",
        "    best_topics_topic_2 = np.argsort(topic_2_test)[-n:]\n",
        "    best_topics_topic2_list.append(best_topics_topic_2)\n",
        "\n",
        "    topic2_t10_values = []\n",
        "    for i in best_topics_topic_2:\n",
        "      topic2_t10_values.append(topic_2_test[i])\n",
        "\n",
        "    all_topic_2_sum = np.sum(topic_2_test)\n",
        "    top10_topic_2_sum = np.sum(topic2_t10_values)\n",
        "    # print('Top 10 Sum from Topic 2: ',top10_topic_2_sum)\n",
        "    # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "    top10_topic_2_total.append(top10_topic_2_sum)\n",
        "\n",
        "    topic_2_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_2_t10_resnet.append(topic_2_test[i])\n",
        "\n",
        "    all_topic_2_sum = np.sum(topic_2_test)\n",
        "    top10_topic_2_sum_resnet = np.sum(topic_2_t10_resnet)\n",
        "    # print('Sum of Top 10 indexes from Resnet: ',top10_topic_2_sum_resnet)\n",
        "    # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "    all_topic_2_sum_list.append(all_topic_2_sum)\n",
        "\n",
        "    top10_topic_2_total_from_resnet.append(top10_topic_2_sum_resnet)\n",
        "\n",
        "    #Topic 3\n",
        "    topic_3 = get_t3_output(x)[0]\n",
        "    topic_3_test = np.squeeze(topic_3)\n",
        "    best_topics_topic_3 = np.argsort(topic_3_test)[-n:]\n",
        "    best_topics_topic3_list.append(best_topics_topic_3)\n",
        "\n",
        "    topic3_t10_values = []\n",
        "    for i in best_topics_topic_3:\n",
        "      topic3_t10_values.append(topic_3_test[i])\n",
        "\n",
        "    all_topic_3_sum = np.sum(topic_3_test)\n",
        "    top10_topic_3_sum = np.sum(topic3_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_3_sum)\n",
        "    # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "    top10_topic_3_total.append(top10_topic_3_sum)\n",
        "\n",
        "    topic_3_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_3_t10_resnet.append(topic_3_test[i])\n",
        "\n",
        "    all_topic_3_sum = np.sum(topic_3_test)\n",
        "    top10_topic_3_sum_resnet = np.sum(topic_3_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_3_sum_resnet)\n",
        "    # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "    all_topic_3_sum_list.append(all_topic_3_sum)\n",
        "\n",
        "\n",
        "    top10_topic_3_total_from_resnet.append(top10_topic_3_sum_resnet)\n",
        "\n",
        "    #Topic 4\n",
        "    topic_4 = get_t4_output(x)[0]\n",
        "    topic_4_test = np.squeeze(topic_4)\n",
        "    best_topics_topic_4 = np.argsort(topic_4_test)[-n:]\n",
        "    best_topics_topic4_list.append(best_topics_topic_4)\n",
        "    topic4_t10_values = []\n",
        "    for i in best_topics_topic_1:\n",
        "      topic4_t10_values.append(topic_4_test[i])\n",
        "\n",
        "    all_topic_4_sum = np.sum(topic_4_test)\n",
        "    top10_topic_4_sum = np.sum(topic4_t10_values)\n",
        "    # print('Top 10 Sum for Topic 4: ',top10_topic_4_sum)\n",
        "    # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "\n",
        "    top10_topic_4_total.append(top10_topic_4_sum)\n",
        "\n",
        "    topic_4_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_4_t10_resnet.append(topic_4_test[i])\n",
        "\n",
        "    all_topic_4_sum = np.sum(topic_4_test)\n",
        "    top10_topic_4_sum_resnet = np.sum(topic_4_t10_resnet)\n",
        "    # print('Top 10 values from Resnet for Topic 4: ',top10_topic_4_sum_resnet)\n",
        "    # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "    all_topic_4_sum_list.append(all_topic_4_sum)\n",
        "\n",
        "    top10_topic_4_total_from_resnet.append(top10_topic_4_sum_resnet)\n",
        "  df_sum_all_topics = pd.DataFrame(list(zip(sum_resnet_total, all_topic_1_sum_list, all_topic_2_sum_list,all_topic_3_sum_list,all_topic_4_sum_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4'])\n",
        "  df_sum_all_topics['Class'] = class_name\n",
        "  df_sum_all_topics.to_csv('sum_topics_'+class_name+'_imp1.csv')\n",
        "  df_best_topics = pd.DataFrame(list(zip(best_topic_resnet_list, best_topics_topic1_list, best_topics_topic2_list,best_topics_topic3_list,best_topics_topic4_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4'])\n",
        "  df_best_topics['Class'] = class_name\n",
        "  df_best_topics.to_csv('top10_topics_'+class_name+'_imp1.csv')\n",
        "\n",
        "  df_resnet_1 = pd.DataFrame(top10_topic_1_total_from_resnet)\n",
        "  df_resnet_1['Topic'] = 'Topic 1'\n",
        "  df_resnet_2 = pd.DataFrame(top10_topic_2_total_from_resnet)\n",
        "  df_resnet_2['Topic'] = 'Topic 2'\n",
        "  df_resnet_3 = pd.DataFrame(top10_topic_3_total_from_resnet)\n",
        "  df_resnet_3['Topic'] = 'Topic 3'\n",
        "  df_resnet_4 = pd.DataFrame(top10_topic_4_total_from_resnet)\n",
        "  df_resnet_4['Topic'] = 'Topic 4'\n",
        "  \n",
        "  df_resnet = pd.concat([df_resnet_1,df_resnet_2,df_resnet_3,df_resnet_4])\n",
        "  \n",
        "  df_topic_1 = pd.DataFrame(top10_topic_1_total)\n",
        "  df_topic_1['Topic'] = 'Topic 1'\n",
        "  df_topic_2 = pd.DataFrame(top10_topic_2_total)\n",
        "  df_topic_2['Topic'] = 'Topic 2'\n",
        "  df_topic_3 = pd.DataFrame(top10_topic_3_total)\n",
        "  df_topic_3['Topic'] = 'Topic 3'\n",
        "  df_topic_4 = pd.DataFrame(top10_topic_4_total)\n",
        "  df_topic_4['Topic'] = 'Topic 4'\n",
        "\n",
        "  df_topics = pd.concat([df_topic_1,df_topic_2,df_topic_3,df_topic_4])\n",
        "  df_topics.columns = ['Sum','Topic']\n",
        "  df_resnet.columns = ['Sum','Topic']\n",
        "\n",
        "\n",
        "  #Plotting Figures\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_resnet,orient='v')\n",
        "  plt.title('Top 10 Objects from ResNet Softmax for Topics =4 for '+class_name)\n",
        "  plt.savefig('imp4_top10_resnet_'+class_name+'_imp1.png')\n",
        "  plt.clf()\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_topics,orient='v')\n",
        "  plt.title('Top 10 Objects from Dense Topic Layers for Topics =4 for '+class_name)\n",
        "  plt.savefig('imp4_top10_topiclayer_'+class_name+'_imp1.png')\n",
        "  plt.clf()\n",
        "\n",
        "  return df_resnet,df_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1kmIWJmIzg"
      },
      "source": [
        "##Creating declaration for 9 topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69L7t8hsmIPm"
      },
      "source": [
        "def create_Topic_data_k9(image_list,number):\n",
        "\n",
        "  n = number\n",
        "  sum_resnet_total = []\n",
        "  top10_resnet_total = []\n",
        "  \n",
        "  #Collecting the best performing indexes\n",
        "  best_topic_resnet_list = []\n",
        "  best_topics_topic1_list = []\n",
        "  best_topics_topic2_list = []\n",
        "  best_topics_topic3_list = []\n",
        "  best_topics_topic4_list = []\n",
        "  best_topics_topic5_list = []\n",
        "  best_topics_topic6_list = []\n",
        "  best_topics_topic7_list = []\n",
        "  best_topics_topic8_list = []\n",
        "  best_topics_topic9_list = []\n",
        "\n",
        "  all_topic_1_sum_list = []\n",
        "  all_topic_2_sum_list = []\n",
        "  all_topic_3_sum_list = []\n",
        "  all_topic_4_sum_list = []\n",
        "  all_topic_5_sum_list = []\n",
        "  all_topic_6_sum_list = []\n",
        "  all_topic_7_sum_list = []\n",
        "  all_topic_8_sum_list = []\n",
        "  all_topic_9_sum_list = []\n",
        "\n",
        "  top10_topic_1_total = []\n",
        "  top10_topic_1_total_from_resnet = []\n",
        "\n",
        "  top10_topic_2_total = []\n",
        "  top10_topic_2_total_from_resnet = []\n",
        "\n",
        "  top10_topic_3_total = []\n",
        "  top10_topic_3_total_from_resnet = []\n",
        "\n",
        "  top10_topic_4_total = []\n",
        "  top10_topic_4_total_from_resnet = []\n",
        "\n",
        "  top10_topic_5_total = []\n",
        "  top10_topic_5_total_from_resnet = []\n",
        "\n",
        "  top10_topic_6_total = []\n",
        "  top10_topic_6_total_from_resnet = []\n",
        "\n",
        "  top10_topic_7_total = []\n",
        "  top10_topic_7_total_from_resnet = []\n",
        "\n",
        "  top10_topic_8_total = []\n",
        "  top10_topic_8_total_from_resnet = []\n",
        "\n",
        "  top10_topic_9_total = []\n",
        "  top10_topic_9_total_from_resnet = []\n",
        "\n",
        "  class_name = image_list[0]\n",
        "  class_name = class_name.split('/')[-2]\n",
        "  print('Class: ',class_name)\n",
        "  for image in image_list:\n",
        "    print(image)\n",
        "    data = create_numpy_img(image)\n",
        "    x = data\n",
        "\n",
        "    #Getting the Resnet output\n",
        "    resnet = get_base_output(x)[0]\n",
        "    resnet_test = np.squeeze(resnet)\n",
        "    best_topics_resnet = np.argsort(resnet_test)[-n:]\n",
        "    best_topic_resnet_list.append(best_topics_resnet)\n",
        "    resnet_topic_t10_values = []\n",
        "    for i in best_topics_resnet:\n",
        "      resnet_topic_t10_values.append(resnet_test[i])\n",
        "    all_resnet_sum = np.sum(resnet_test)\n",
        "    top10_resnet_sum = np.sum(resnet_topic_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_resnet_sum)\n",
        "    # print('Entire Resnet topics sum: ',all_resnet_sum)  \n",
        "    \n",
        "    #Adding to list\n",
        "    top10_resnet_total.append(top10_resnet_sum)\n",
        "    sum_resnet_total.append(all_resnet_sum)\n",
        "    \n",
        "    #Topic 1\n",
        "    topic_1 = get_t1_output(x)[0]\n",
        "\n",
        "    topic_1_test = np.squeeze(topic_1)\n",
        "    best_topics_topic_1 = np.argsort(topic_1_test)[-n:]\n",
        "    best_topics_topic1_list.append(best_topics_topic_1)\n",
        "\n",
        "    topic1_t10_values = []\n",
        "    for i in best_topics_topic_1:\n",
        "      topic1_t10_values.append(topic_1_test[i])\n",
        "\n",
        "    all_topic_1_sum = np.sum(topic_1_test)\n",
        "    top10_topic_1_sum = np.sum(topic1_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_1_total.append(top10_topic_1_sum)\n",
        "\n",
        "    topic_1_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_1_t10_resnet.append(topic_1_test[i])\n",
        "\n",
        "    all_topic_1_sum = np.sum(topic_1_test)\n",
        "    top10_topic_1_sum_resnet = np.sum(topic_1_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_1_sum_list.append(all_topic_1_sum)\n",
        "\n",
        "    top10_topic_1_total_from_resnet.append(top10_topic_1_sum_resnet)\n",
        "\n",
        "    #Topic 2\n",
        "    topic_2 = get_t2_output(x)\n",
        "    topic_2_test = np.squeeze(topic_2)\n",
        "    best_topics_topic_2 = np.argsort(topic_2_test)[-n:]\n",
        "    best_topics_topic2_list.append(best_topics_topic_2)\n",
        "\n",
        "    topic2_t10_values = []\n",
        "    for i in best_topics_topic_2:\n",
        "      topic2_t10_values.append(topic_2_test[i])\n",
        "\n",
        "    all_topic_2_sum = np.sum(topic_2_test)\n",
        "    top10_topic_2_sum = np.sum(topic2_t10_values)\n",
        "    # print('Top 10 Sum from Topic 2: ',top10_topic_2_sum)\n",
        "    # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "    top10_topic_2_total.append(top10_topic_2_sum)\n",
        "\n",
        "    topic_2_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_2_t10_resnet.append(topic_2_test[i])\n",
        "\n",
        "    all_topic_2_sum = np.sum(topic_2_test)\n",
        "    top10_topic_2_sum_resnet = np.sum(topic_2_t10_resnet)\n",
        "    # print('Sum of Top 10 indexes from Resnet: ',top10_topic_2_sum_resnet)\n",
        "    # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "    all_topic_2_sum_list.append(all_topic_2_sum)\n",
        "\n",
        "    top10_topic_2_total_from_resnet.append(top10_topic_2_sum_resnet)\n",
        "\n",
        "    #Topic 3\n",
        "    topic_3 = get_t3_output(x)[0]\n",
        "    topic_3_test = np.squeeze(topic_3)\n",
        "    best_topics_topic_3 = np.argsort(topic_3_test)[-n:]\n",
        "    best_topics_topic3_list.append(best_topics_topic_3)\n",
        "\n",
        "    topic3_t10_values = []\n",
        "    for i in best_topics_topic_3:\n",
        "      topic3_t10_values.append(topic_3_test[i])\n",
        "\n",
        "    all_topic_3_sum = np.sum(topic_3_test)\n",
        "    top10_topic_3_sum = np.sum(topic3_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_3_sum)\n",
        "    # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "    top10_topic_3_total.append(top10_topic_3_sum)\n",
        "\n",
        "    topic_3_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_3_t10_resnet.append(topic_3_test[i])\n",
        "\n",
        "    all_topic_3_sum = np.sum(topic_3_test)\n",
        "    top10_topic_3_sum_resnet = np.sum(topic_3_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_3_sum_resnet)\n",
        "    # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "    all_topic_3_sum_list.append(all_topic_3_sum)\n",
        "\n",
        "\n",
        "    top10_topic_3_total_from_resnet.append(top10_topic_3_sum_resnet)\n",
        "\n",
        "    #Topic 4\n",
        "    topic_4 = get_t4_output(x)[0]\n",
        "    topic_4_test = np.squeeze(topic_4)\n",
        "    best_topics_topic_4 = np.argsort(topic_4_test)[-n:]\n",
        "    best_topics_topic4_list.append(best_topics_topic_4)\n",
        "    topic4_t10_values = []\n",
        "    for i in best_topics_topic_1:\n",
        "      topic4_t10_values.append(topic_4_test[i])\n",
        "\n",
        "    all_topic_4_sum = np.sum(topic_4_test)\n",
        "    top10_topic_4_sum = np.sum(topic4_t10_values)\n",
        "    # print('Top 10 Sum for Topic 4: ',top10_topic_4_sum)\n",
        "    # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "\n",
        "    top10_topic_4_total.append(top10_topic_4_sum)\n",
        "\n",
        "    topic_4_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_4_t10_resnet.append(topic_4_test[i])\n",
        "\n",
        "    all_topic_4_sum = np.sum(topic_4_test)\n",
        "    top10_topic_4_sum_resnet = np.sum(topic_4_t10_resnet)\n",
        "    # print('Top 10 values from Resnet for Topic 4: ',top10_topic_4_sum_resnet)\n",
        "    # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "    all_topic_4_sum_list.append(all_topic_4_sum)\n",
        "\n",
        "    top10_topic_4_total_from_resnet.append(top10_topic_4_sum_resnet)\n",
        "\n",
        "    #Topic 5\n",
        "    topic_5 = get_t5_output(x)[0]\n",
        "\n",
        "    topic_5_test = np.squeeze(topic_1)\n",
        "    best_topics_topic_5 = np.argsort(topic_5_test)[-n:]\n",
        "    best_topics_topic5_list.append(best_topics_topic_5)\n",
        "\n",
        "    topic5_t10_values = []\n",
        "    for i in best_topics_topic_5:\n",
        "      topic5_t10_values.append(topic_5_test[i])\n",
        "\n",
        "    all_topic_5_sum = np.sum(topic_5_test)\n",
        "    top10_topic_5_sum = np.sum(topic5_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_5_total.append(top10_topic_5_sum)\n",
        "\n",
        "    topic_5_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_5_t10_resnet.append(topic_5_test[i])\n",
        "\n",
        "    all_topic_5_sum = np.sum(topic_5_test)\n",
        "    top10_topic_5_sum_resnet = np.sum(topic_5_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_5_sum_list.append(all_topic_5_sum)\n",
        "\n",
        "    top10_topic_5_total_from_resnet.append(top10_topic_5_sum_resnet)\n",
        "\n",
        "    #Topic 6\n",
        "        \n",
        "    topic_6 = get_t6_output(x)[0]\n",
        "\n",
        "    topic_6_test = np.squeeze(topic_6)\n",
        "    best_topics_topic_6 = np.argsort(topic_6_test)[-n:]\n",
        "    best_topics_topic6_list.append(best_topics_topic_6)\n",
        "\n",
        "    topic6_t10_values = []\n",
        "    for i in best_topics_topic_6:\n",
        "      topic6_t10_values.append(topic_6_test[i])\n",
        "\n",
        "    all_topic_6_sum = np.sum(topic_6_test)\n",
        "    top10_topic_6_sum = np.sum(topic6_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_6_total.append(top10_topic_6_sum)\n",
        "\n",
        "    topic_6_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_6_t10_resnet.append(topic_6_test[i])\n",
        "\n",
        "    all_topic_6_sum = np.sum(topic_6_test)\n",
        "    top10_topic_6_sum_resnet = np.sum(topic_6_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_6_sum_list.append(all_topic_6_sum)\n",
        "\n",
        "    top10_topic_6_total_from_resnet.append(top10_topic_6_sum_resnet)\n",
        "\n",
        "    #Topic 7\n",
        "\n",
        "    \n",
        "    topic_7 = get_t7_output(x)[0]\n",
        "\n",
        "    topic_7_test = np.squeeze(topic_7)\n",
        "    best_topics_topic_7 = np.argsort(topic_7_test)[-n:]\n",
        "    best_topics_topic7_list.append(best_topics_topic_7)\n",
        "\n",
        "    topic7_t10_values = []\n",
        "    for i in best_topics_topic_7:\n",
        "      topic7_t10_values.append(topic_7_test[i])\n",
        "\n",
        "    all_topic_7_sum = np.sum(topic_7_test)\n",
        "    top10_topic_7_sum = np.sum(topic7_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_7_total.append(top10_topic_7_sum)\n",
        "\n",
        "    topic_7_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_7_t10_resnet.append(topic_7_test[i])\n",
        "\n",
        "    all_topic_7_sum = np.sum(topic_7_test)\n",
        "    top10_topic_7_sum_resnet = np.sum(topic_7_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_7_sum_list.append(all_topic_7_sum)\n",
        "\n",
        "    top10_topic_7_total_from_resnet.append(top10_topic_7_sum_resnet)\n",
        "  \n",
        "    #Topic 8\n",
        "\n",
        "    topic_8 = get_t8_output(x)[0]\n",
        "\n",
        "    topic_8_test = np.squeeze(topic_8)\n",
        "    best_topics_topic_8 = np.argsort(topic_8_test)[-n:]\n",
        "    best_topics_topic8_list.append(best_topics_topic_8)\n",
        "\n",
        "    topic8_t10_values = []\n",
        "    for i in best_topics_topic_8:\n",
        "      topic8_t10_values.append(topic_8_test[i])\n",
        "\n",
        "    all_topic_8_sum = np.sum(topic_8_test)\n",
        "    top10_topic_8_sum = np.sum(topic8_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_8_total.append(top10_topic_8_sum)\n",
        "\n",
        "    topic_8_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_8_t10_resnet.append(topic_8_test[i])\n",
        "\n",
        "    all_topic_8_sum = np.sum(topic_8_test)\n",
        "    top10_topic_8_sum_resnet = np.sum(topic_8_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_8_sum_list.append(all_topic_8_sum)\n",
        "\n",
        "    top10_topic_8_total_from_resnet.append(top10_topic_8_sum_resnet)\n",
        "\n",
        "    #Topic 9\n",
        "\n",
        "    topic_9 = get_t9_output(x)[0]\n",
        "\n",
        "    topic_9_test = np.squeeze(topic_9)\n",
        "    best_topics_topic_9 = np.argsort(topic_9_test)[-n:]\n",
        "    best_topics_topic9_list.append(best_topics_topic_9)\n",
        "\n",
        "    topic9_t10_values = []\n",
        "    for i in best_topics_topic_9:\n",
        "      topic9_t10_values.append(topic_9_test[i])\n",
        "\n",
        "    all_topic_9_sum = np.sum(topic_9_test)\n",
        "    top10_topic_9_sum = np.sum(topic9_t10_values)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    #Adding to list\n",
        "    top10_topic_9_total.append(top10_topic_9_sum)\n",
        "\n",
        "    topic_9_t10_resnet = []\n",
        "    for i in best_topics_resnet:\n",
        "      topic_9_t10_resnet.append(topic_9_test[i])\n",
        "\n",
        "    all_topic_9_sum = np.sum(topic_9_test)\n",
        "    top10_topic_9_sum_resnet = np.sum(topic_9_t10_resnet)\n",
        "    # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "    # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "    all_topic_9_sum_list.append(all_topic_9_sum)\n",
        "\n",
        "    top10_topic_9_total_from_resnet.append(top10_topic_9_sum_resnet)\n",
        "\n",
        "  df_sum_all_topics = pd.DataFrame(list(zip(sum_resnet_total, all_topic_1_sum_list, all_topic_2_sum_list,all_topic_3_sum_list,all_topic_4_sum_list,all_topic_5_sum_list,all_topic_6_sum_list,all_topic_7_sum_list,all_topic_8_sum_list,all_topic_9_sum_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4','Topic 5','Topic 6','Topic 7','Topic 8','Topic 9'])\n",
        "  df_sum_all_topics['Class'] = class_name\n",
        "  df_sum_all_topics.to_csv('sum_topics_k9_'+class_name+'_imp2.csv')\n",
        "  df_best_topics = pd.DataFrame(list(zip(best_topic_resnet_list, best_topics_topic1_list, best_topics_topic2_list,best_topics_topic3_list,best_topics_topic4_list,best_topics_topic5_list,best_topics_topic6_list,best_topics_topic7_list,best_topics_topic8_list,best_topics_topic9_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4','Topic 5','Topic 6','Topic 7','Topic 8','Topic 9'])\n",
        "  df_best_topics['Class'] = class_name\n",
        "  df_best_topics.to_csv('top10_topics_k9_'+class_name+'_imp2.csv')\n",
        "\n",
        "  df_resnet_1 = pd.DataFrame(top10_topic_1_total_from_resnet)\n",
        "  df_resnet_1['Topic'] = 'Topic 1'\n",
        "  df_resnet_2 = pd.DataFrame(top10_topic_2_total_from_resnet)\n",
        "  df_resnet_2['Topic'] = 'Topic 2'\n",
        "  df_resnet_3 = pd.DataFrame(top10_topic_3_total_from_resnet)\n",
        "  df_resnet_3['Topic'] = 'Topic 3'\n",
        "  df_resnet_4 = pd.DataFrame(top10_topic_4_total_from_resnet)\n",
        "  df_resnet_4['Topic'] = 'Topic 4'\n",
        "  df_resnet_5 = pd.DataFrame(top10_topic_5_total_from_resnet)\n",
        "  df_resnet_5['Topic'] = 'Topic 5'\n",
        "  df_resnet_6 = pd.DataFrame(top10_topic_6_total_from_resnet)\n",
        "  df_resnet_6['Topic'] = 'Topic 6'\n",
        "  df_resnet_7 = pd.DataFrame(top10_topic_7_total_from_resnet)\n",
        "  df_resnet_7['Topic'] = 'Topic 7'\n",
        "  df_resnet_8 = pd.DataFrame(top10_topic_8_total_from_resnet)\n",
        "  df_resnet_8['Topic'] = 'Topic 8'\n",
        "  df_resnet_9 = pd.DataFrame(top10_topic_9_total_from_resnet)\n",
        "  df_resnet_9['Topic'] = 'Topic 9'\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  df_resnet = pd.concat([df_resnet_1,df_resnet_2,df_resnet_3,df_resnet_4,df_resnet_5,df_resnet_6,df_resnet_7,df_resnet_8,df_resnet_9])\n",
        "  \n",
        "  df_topic_1 = pd.DataFrame(top10_topic_1_total)\n",
        "  df_topic_1['Topic'] = 'Topic 1'\n",
        "  df_topic_2 = pd.DataFrame(top10_topic_2_total)\n",
        "  df_topic_2['Topic'] = 'Topic 2'\n",
        "  df_topic_3 = pd.DataFrame(top10_topic_3_total)\n",
        "  df_topic_3['Topic'] = 'Topic 3'\n",
        "  df_topic_4 = pd.DataFrame(top10_topic_4_total)\n",
        "  df_topic_4['Topic'] = 'Topic 4'\n",
        "  df_topic_5 = pd.DataFrame(top10_topic_5_total)\n",
        "  df_topic_5['Topic'] = 'Topic 5'\n",
        "  df_topic_6 = pd.DataFrame(top10_topic_6_total)\n",
        "  df_topic_6['Topic'] = 'Topic 6'\n",
        "  df_topic_7 = pd.DataFrame(top10_topic_7_total)\n",
        "  df_topic_7['Topic'] = 'Topic 7'\n",
        "  df_topic_8 = pd.DataFrame(top10_topic_8_total)\n",
        "  df_topic_8['Topic'] = 'Topic 8'\n",
        "  df_topic_9 = pd.DataFrame(top10_topic_9_total)\n",
        "  df_topic_9['Topic'] = 'Topic 9'\n",
        "\n",
        "  df_topics = pd.concat([df_topic_1,df_topic_2,df_topic_3,df_topic_4,df_topic_5,df_topic_6,df_topic_7,df_topic_8,df_topic_9])\n",
        "  df_topics.columns = ['Sum','Topic']\n",
        "  df_resnet.columns = ['Sum','Topic']\n",
        "\n",
        "\n",
        "  #Plotting Figures\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_resnet,orient='v')\n",
        "  plt.title('Top 10 Objects from ResNet Softmax for Topics =9 for '+class_name)\n",
        "  plt.savefig('k9_top10_resnet_'+class_name+'_imp2.png')\n",
        "  plt.clf()\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_topics,orient='v')\n",
        "  plt.title('Top 10 Objects from Dense Topic Layers for Topics =9 for '+class_name)\n",
        "  plt.savefig('k9_top10_topiclayer_'+class_name+'_imp2.png')\n",
        "  plt.clf()\n",
        "\n",
        "  return df_resnet,df_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBGX5vkrHxk_"
      },
      "source": [
        "def create_Topic_data_mul(image_list,number):\n",
        "\n",
        "  n = number\n",
        "  sum_resnet_total = []\n",
        "  top10_resnet_total = []\n",
        "  \n",
        "  #Collecting the best performing indexes\n",
        "  best_topic_resnet_list = []\n",
        "  best_topics_topic1_list = []\n",
        "  best_topics_topic2_list = []\n",
        "  best_topics_topic3_list = []\n",
        "  best_topics_topic4_list = []\n",
        "  all_topic_1_sum_list = []\n",
        "  all_topic_2_sum_list = []\n",
        "  all_topic_3_sum_list = []\n",
        "  all_topic_4_sum_list = []\n",
        "\n",
        "  top10_topic_1_total = []\n",
        "  top10_topic_1_total_from_resnet = []\n",
        "\n",
        "  top10_topic_2_total = []\n",
        "  top10_topic_2_total_from_resnet = []\n",
        "\n",
        "  top10_topic_3_total = []\n",
        "  top10_topic_3_total_from_resnet = []\n",
        "\n",
        "  top10_topic_4_total = []\n",
        "  top10_topic_4_total_from_resnet = []\n",
        "  class_name = image_list[0]\n",
        "  class_name = class_name.split('/')[-2]\n",
        "  print('Class: ',class_name)\n",
        "  # for image in image_list:\n",
        "  data = create_numpy_img(image)\n",
        "  x = data\n",
        "\n",
        "  #Getting the Resnet output\n",
        "  resnet = get_base_output(x)[0]\n",
        "  resnet_test = np.squeeze(resnet)\n",
        "  best_topics_resnet = np.argsort(resnet_test)[-n:]\n",
        "  best_topic_resnet_list.append(best_topics_resnet)\n",
        "  resnet_topic_t10_values = []\n",
        "  for i in best_topics_resnet:\n",
        "    resnet_topic_t10_values.append(resnet_test[i])\n",
        "  all_resnet_sum = np.sum(resnet_test)\n",
        "  top10_resnet_sum = np.sum(resnet_topic_t10_values)\n",
        "  # print('Top 10 Sum: ',top10_resnet_sum)\n",
        "  # print('Entire Resnet topics sum: ',all_resnet_sum)  \n",
        "  \n",
        "  #Adding to list\n",
        "  top10_resnet_total.append(top10_resnet_sum)\n",
        "  sum_resnet_total.append(all_resnet_sum)\n",
        "  \n",
        "  #Topic 1\n",
        "  topic_1 = get_t1_output(x)[0]\n",
        "\n",
        "  topic_1_test = np.squeeze(topic_1)\n",
        "  best_topics_topic_1 = np.argsort(topic_1_test)[-n:]\n",
        "  best_topics_topic1_list.append(best_topics_topic_1)\n",
        "\n",
        "  topic1_t10_values = []\n",
        "  for i in best_topics_topic_1:\n",
        "    topic1_t10_values.append(topic_1_test[i])\n",
        "\n",
        "  all_topic_1_sum = np.sum(topic_1_test)\n",
        "  top10_topic_1_sum = np.sum(topic1_t10_values)\n",
        "  # print('Top 10 Sum: ',top10_topic_1_sum)\n",
        "  # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "  #Adding to list\n",
        "  top10_topic_1_total.append(top10_topic_1_sum)\n",
        "\n",
        "  topic_1_t10_resnet = []\n",
        "  for i in best_topics_resnet:\n",
        "    topic_1_t10_resnet.append(topic_1_test[i])\n",
        "\n",
        "  all_topic_1_sum = np.sum(topic_1_test)\n",
        "  top10_topic_1_sum_resnet = np.sum(topic_1_t10_resnet)\n",
        "  # print('Top 10 Sum: ',top10_topic_1_sum_resnet)\n",
        "  # print('Entire Topic 1 topic sum: ',all_topic_1_sum)\n",
        "\n",
        "  all_topic_1_sum_list.append(all_topic_1_sum)\n",
        "\n",
        "  top10_topic_1_total_from_resnet.append(top10_topic_1_sum_resnet)\n",
        "\n",
        "  #Topic 2\n",
        "  topic_2 = get_t2_output(x)\n",
        "  topic_2_test = np.squeeze(topic_2)\n",
        "  best_topics_topic_2 = np.argsort(topic_2_test)[-n:]\n",
        "  best_topics_topic2_list.append(best_topics_topic_2)\n",
        "\n",
        "  topic2_t10_values = []\n",
        "  for i in best_topics_topic_2:\n",
        "    topic2_t10_values.append(topic_2_test[i])\n",
        "\n",
        "  all_topic_2_sum = np.sum(topic_2_test)\n",
        "  top10_topic_2_sum = np.sum(topic2_t10_values)\n",
        "  # print('Top 10 Sum from Topic 2: ',top10_topic_2_sum)\n",
        "  # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "  top10_topic_2_total.append(top10_topic_2_sum)\n",
        "\n",
        "  topic_2_t10_resnet = []\n",
        "  for i in best_topics_resnet:\n",
        "    topic_2_t10_resnet.append(topic_2_test[i])\n",
        "\n",
        "  all_topic_2_sum = np.sum(topic_2_test)\n",
        "  top10_topic_2_sum_resnet = np.sum(topic_2_t10_resnet)\n",
        "  # print('Sum of Top 10 indexes from Resnet: ',top10_topic_2_sum_resnet)\n",
        "  # print('Entire Topic 2 topic sum: ',all_topic_2_sum)\n",
        "\n",
        "  all_topic_2_sum_list.append(all_topic_2_sum)\n",
        "\n",
        "  top10_topic_2_total_from_resnet.append(top10_topic_2_sum_resnet)\n",
        "\n",
        "  #Topic 3\n",
        "  topic_3 = get_t3_output(x)[0]\n",
        "  topic_3_test = np.squeeze(topic_3)\n",
        "  best_topics_topic_3 = np.argsort(topic_3_test)[-n:]\n",
        "  best_topics_topic3_list.append(best_topics_topic_3)\n",
        "\n",
        "  topic3_t10_values = []\n",
        "  for i in best_topics_topic_3:\n",
        "    topic3_t10_values.append(topic_3_test[i])\n",
        "\n",
        "  all_topic_3_sum = np.sum(topic_3_test)\n",
        "  top10_topic_3_sum = np.sum(topic3_t10_values)\n",
        "  # print('Top 10 Sum: ',top10_topic_3_sum)\n",
        "  # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "  top10_topic_3_total.append(top10_topic_3_sum)\n",
        "\n",
        "  topic_3_t10_resnet = []\n",
        "  for i in best_topics_resnet:\n",
        "    topic_3_t10_resnet.append(topic_3_test[i])\n",
        "\n",
        "  all_topic_3_sum = np.sum(topic_3_test)\n",
        "  top10_topic_3_sum_resnet = np.sum(topic_3_t10_resnet)\n",
        "  # print('Top 10 Sum: ',top10_topic_3_sum_resnet)\n",
        "  # print('Entire Topic 3 topic sum: ',all_topic_3_sum)\n",
        "\n",
        "  all_topic_3_sum_list.append(all_topic_3_sum)\n",
        "\n",
        "\n",
        "  top10_topic_3_total_from_resnet.append(top10_topic_3_sum_resnet)\n",
        "\n",
        "  #Topic 4\n",
        "  topic_4 = get_t4_output(x)[0]\n",
        "  topic_4_test = np.squeeze(topic_4)\n",
        "  best_topics_topic_4 = np.argsort(topic_4_test)[-n:]\n",
        "  best_topics_topic4_list.append(best_topics_topic_4)\n",
        "  topic4_t10_values = []\n",
        "  for i in best_topics_topic_1:\n",
        "    topic4_t10_values.append(topic_4_test[i])\n",
        "\n",
        "  all_topic_4_sum = np.sum(topic_4_test)\n",
        "  top10_topic_4_sum = np.sum(topic4_t10_values)\n",
        "  # print('Top 10 Sum for Topic 4: ',top10_topic_4_sum)\n",
        "  # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "\n",
        "  top10_topic_4_total.append(top10_topic_4_sum)\n",
        "\n",
        "  topic_4_t10_resnet = []\n",
        "  for i in best_topics_resnet:\n",
        "    topic_4_t10_resnet.append( [i])\n",
        "\n",
        "  all_topic_4_sum = np.sum(topic_4_test)\n",
        "  top10_topic_4_sum_resnet = np.sum(topic_4_t10_resnet)\n",
        "  # print('Top 10 values from Resnet for Topic 4: ',top10_topic_4_sum_resnet)\n",
        "  # print('Entire Topic 4 topic sum: ',all_topic_4_sum)\n",
        "  all_topic_4_sum_list.append(all_topic_4_sum)\n",
        "\n",
        "  top10_topic_4_total_from_resnet.append(top10_topic_4_sum_resnet)\n",
        "  df_sum_all_topics = pd.DataFrame(list(zip(sum_resnet_total, all_topic_1_sum_list, all_topic_2_sum_list,all_topic_3_sum_list,all_topic_4_sum_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4'])\n",
        "  df_sum_all_topics['Class'] = class_name\n",
        "  df_sum_all_topics.to_csv('sum_topics_'+class_name+'_uni.csv')\n",
        "  df_best_topics = pd.DataFrame(list(zip(best_topic_resnet_list, best_topics_topic1_list, best_topics_topic2_list,best_topics_topic3_list,best_topics_topic4_list)),columns=['Resnet','Topic 1','Topic 2','Topic 3', 'Topic 4'])\n",
        "  df_best_topics['Class'] = class_name\n",
        "  df_best_topics.to_csv('top10_topics_'+class_name+'_uni.csv')\n",
        "\n",
        "  df_resnet_1 = pd.DataFrame(top10_topic_1_total_from_resnet)\n",
        "  df_resnet_1['Topic'] = 'Topic 1'\n",
        "  df_resnet_2 = pd.DataFrame(top10_topic_2_total_from_resnet)\n",
        "  df_resnet_2['Topic'] = 'Topic 2'\n",
        "  df_resnet_3 = pd.DataFrame(top10_topic_3_total_from_resnet)\n",
        "  df_resnet_3['Topic'] = 'Topic 3'\n",
        "  df_resnet_4 = pd.DataFrame(top10_topic_4_total_from_resnet)\n",
        "  df_resnet_4['Topic'] = 'Topic 4'\n",
        "  \n",
        "  df_resnet = pd.concat([df_resnet_1,df_resnet_2,df_resnet_3,df_resnet_4])\n",
        "  \n",
        "  df_topic_1 = pd.DataFrame(top10_topic_1_total)\n",
        "  df_topic_1['Topic'] = 'Topic 1'\n",
        "  df_topic_2 = pd.DataFrame(top10_topic_2_total)\n",
        "  df_topic_2['Topic'] = 'Topic 2'\n",
        "  df_topic_3 = pd.DataFrame(top10_topic_3_total)\n",
        "  df_topic_3['Topic'] = 'Topic 3'\n",
        "  df_topic_4 = pd.DataFrame(top10_topic_4_total)\n",
        "  df_topic_4['Topic'] = 'Topic 4'\n",
        "\n",
        "  df_topics = pd.concat([df_topic_1,df_topic_2,df_topic_3,df_topic_4])\n",
        "  df_topics.columns = ['Sum','Topic']\n",
        "  df_resnet.columns = ['Sum','Topic']\n",
        "\n",
        "\n",
        "  #Plotting Figures\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_resnet,orient='v')\n",
        "  plt.title('Top 10 Objects from ResNet Softmax for Topics =4 for '+class_name)\n",
        "  plt.savefig('imp4_top10_resnet_'+class_name+'_uni.png')\n",
        "  plt.clf()\n",
        "  ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_topics,orient='v')\n",
        "  plt.title('Top 10 Objects from Dense Topic Layers for Topics =4 for '+class_name)\n",
        "  plt.savefig('imp4_top10_topiclayer_'+class_name+'_uni.png')\n",
        "  plt.clf()\n",
        "\n",
        "  return df_resnet,df_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBoZZQ_1pqmH"
      },
      "source": [
        "top10_topic_1_total_from_resnet,top10_topic_2_total_from_resnet,top10_topic_3_total_from_resnet,top10_topic_4_total_from_resnet,top10_topic_4_total,top10_topic_2_total,top10_topic_3_total,top10_topic_4_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nPGXTk77cKF"
      },
      "source": [
        "list_re = top10_topic_1_total_from_resnet+top10_topic_2_total_from_resnet+top10_topic_3_total_from_resnet+top10_topic_4_total_from_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1-fJnl7E8sa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phbz27_gdR7x"
      },
      "source": [
        "im = Image.open(images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16uplBjNDPrl"
      },
      "source": [
        "im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c47dqubhkTO4"
      },
      "source": [
        "test = '/content/gdrive/MyDrive/data/train/Office/20171202_163137_000.jpg'\n",
        "class_test = test.split('/')[-2]\n",
        "class_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUPsiW7yCzNL"
      },
      "source": [
        "##Running the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSBWIV8Sblwo"
      },
      "source": [
        "df_resnet,df_topics = create_Topic_data(images,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1GLFegDAk3p"
      },
      "source": [
        "df_best_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2V_KwSW6myV"
      },
      "source": [
        "df_test = pd.DataFrame(df_resnet.to_numpy().flatten())\n",
        "df_test['Topic'] = 'Topic'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxXBIBl84ZC"
      },
      "source": [
        "df_test['Topic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGvta85mj3E"
      },
      "source": [
        "df_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91cb12fWDjts"
      },
      "source": [
        "Vertical Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CPSaBRRDlgx"
      },
      "source": [
        "ax = sn.boxplot(y=\"Sum\", x=\"Topic\", data=df_resnet,orient='v')\n",
        "plt.title('Top 10 Predictions from ResNet Softmax for Topics =4')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja79-92uvtMa"
      },
      "source": [
        "###Box plots for Top 10 Softmax per topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNSgHllrAga"
      },
      "source": [
        "\n",
        "ax_1 = sn.boxplot(x=df_resnet[\"Topic 1\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Softmax for Topic 1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKIqKd9auz4y"
      },
      "source": [
        "\n",
        "ax_2 = sn.boxplot(x=df_resnet[\"Topic 2\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Softmax for Topic 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z082Jwunu97B"
      },
      "source": [
        "\n",
        "ax_3 = sn.boxplot(x=df_resnet[\"Topic 3\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Softmax for Topic 3')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkb8EPhDvHrE"
      },
      "source": [
        "\n",
        "ax_4 = sn.boxplot(x=df_resnet[\"Topic 4\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Softmax for Topic 4')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZURrU6Sev19Z"
      },
      "source": [
        "###Box Plots for Top 10 values for each individual Topic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gIqs967vNfX"
      },
      "source": [
        "ax_1 = sn.boxplot(x=df_topics[\"Topic 1\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Topic 1 Dense Layer')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNi6TGx0v8qN"
      },
      "source": [
        "ax_2 = sn.boxplot(x=df_topics[\"Topic 2\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Topic 2 Dense Layer')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MesDznlVwAq9"
      },
      "source": [
        "ax_3 = sn.boxplot(x=df_topics[\"Topic 3\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Topic 3 Dense Layer')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZydeMpwCUm"
      },
      "source": [
        "ax_4 = sn.boxplot(x=df_topics[\"Topic 4\"])\n",
        "plt.title('Distribution of Weights for top 10 Predictions from Topic 4 Dense Layer')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iii8ocO2rfAS"
      },
      "source": [
        "\n",
        "df_resnet = pd.DataFrame(list(zip(topic1_resnet_list, topic2_resnet_list,topic3_resnet_list,topic4_resnet_list)),\n",
        "               columns =['Topic 1', 'Topic 2','Topic 3','Topic 4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOSdnmNrr8wW"
      },
      "source": [
        "df_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRS8QUFpsDFn"
      },
      "source": [
        "topic_1_list = [0.06528306, 0.060476344, 0.061662868, 0.058149464, 0.06436589]\n",
        "topic_2_list = [0.19646133, 0.20384228, 0.20229053, 0.19908997, 0.18312304]\n",
        "topic_3_list =  [0.20839123, 0.21751055, 0.21491227, 0.21268351, 0.18942289]\n",
        "topic_4_list = [0.06528306, 0.060476344, 0.061662868, 0.058149464, 0.06436589]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVoryhQSsS8o"
      },
      "source": [
        "df_topics = pd.DataFrame(list(zip(topic_1_list, topic_2_list,topic_3_list,topic_4_list)),\n",
        "               columns =['Topic 1', 'Topic 2','Topic 3','Topic 4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igoo-0AhsdLW"
      },
      "source": [
        "df_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs-I3NqZsfys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2KQ_wpFxcw"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyobCQa9ZMx4"
      },
      "source": [
        "data_segs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41YENOVUvAT"
      },
      "source": [
        "data_segs = pd.read_csv('testing_new.csv')\n",
        "#print(data_segs.head())\n",
        "#for i in range(len(data_segs)):\n",
        "elements = data_segs.iloc[0,1:11]\n",
        "#print(type(elements))\n",
        "#a = int(elements['0'])\n",
        "#b = int(elements['1'])\n",
        "#print(a,b)\n",
        "#check = np.array([int(elements['0']),int(elements['1']),int(elements['2']),int(elements['3']),int(elements['4']),int(elements['5']),int(elements['6']),int(elements['7']),int(elements['8']),int(elements['9'])])\n",
        "#print(check)\n",
        "print(elements)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CEhHhDAqAF5"
      },
      "source": [
        "water = np.load('/content/Water_train_arrays.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ix0So83qwcg"
      },
      "source": [
        "water1 = water[0]\n",
        "water2 = water[1]\n",
        "water3 = water[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izx4r2Zzq-oF"
      },
      "source": [
        "image1 = tf.keras.preprocessing.image.array_to_img(water1)\n",
        "image2 = tf.keras.preprocessing.image.array_to_img(water2)\n",
        "image3 = tf.keras.preprocessing.image.array_to_img(water3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5QhugN0rkw0"
      },
      "source": [
        "plt.imshow(image3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLVbMpaHrMWS"
      },
      "source": [
        "image."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbn7GQaWOGJg"
      },
      "source": [
        "predictions = np.load('/content/topic_weight_k4_imp_4_4_predictions_tens.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1tQUemwOUnI"
      },
      "source": [
        "y_pred = np.argmax(predictions,axis =1)\n",
        "len(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V8op6rfO3UP"
      },
      "source": [
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir('/content/gdrive/MyDrive/data/train')]))\n",
        "label_dict = dict(list(enumerate(class_names)))\n",
        "labels = label_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV_QgoXPDWd"
      },
      "source": [
        "label_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHiesX_tWHZ7"
      },
      "source": [
        "label_dict = {'Balcony': 0,\n",
        " 'Bathroom': 1,\n",
        " 'Beach': 2,\n",
        " 'Bedroom': 3,\n",
        " 'Buildings': 4,\n",
        " 'Education,science': 5,\n",
        " 'Forest, field, jungle': 6,\n",
        " 'Garden': 7,\n",
        " 'Hospital': 8,\n",
        " 'Kitchen': 9,\n",
        " 'Living room': 10,\n",
        " 'Mountains, hills, desert, sky': 11,\n",
        " 'Museum': 12,\n",
        " 'Noise': 13,\n",
        " 'Office': 14,\n",
        " 'Other rooms': 15,\n",
        " 'Others': 16,\n",
        " 'Pathways': 17,\n",
        " 'Recreation': 18,\n",
        " 'Restaurant,Bar': 19,\n",
        " 'Shop': 20,\n",
        " 'Sport fields': 21,\n",
        " 'Transportation': 22,\n",
        " 'Water': 23}\n",
        "labels = label_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQGeTyhpWKIl"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNzFNAEJT6MQ"
      },
      "source": [
        "test_labels = testing['Label']\n",
        "test_labels = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NBnO__JW1vS"
      },
      "source": [
        "test_labels = test_labels.map(label_dict)\n",
        "test_labels = test_labels.to_list()\n",
        "y_true = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW6FW10RXRHM"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hr5yOodPivg"
      },
      "source": [
        "print(classification_report(y_true, y_pred,target_names=labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi74XBBhb8fD"
      },
      "source": [
        "df_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74frluJXnn0"
      },
      "source": [
        "class_names = np.array(sorted([dir1 for dir1 in os.listdir('/content/gdrive/MyDrive/data/train')]))\n",
        "cm = confusion_matrix(y_true,y_pred,labels =  list(range(0,24)) )\n",
        "\n",
        "df_cm = pd.DataFrame(cm,index = class_names,columns = class_names )\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=False, annot_kws={\"size\": 20},cmap=\"BuPu\") # font size\n",
        "plt.title('Confusion matrix for Topic Model Implementation 4')\n",
        "plt.savefig('imp4_cm.png',bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhxEw6FrPwEg"
      },
      "source": [
        "class_report = classification_report( y_true, y_pred,target_names=labels,output_dict=True)\n",
        "df_cr = pd.DataFrame(class_report).transpose()\n",
        "df_cr.to_csv('cr_imp4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eBOThvZ27Df"
      },
      "source": [
        "tips = sn.load_dataset(\"tips\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnoVVDC6289V"
      },
      "source": [
        "tips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiJjZPs0s5vy"
      },
      "source": [
        "#Creating eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz6KHCcNsyrd"
      },
      "source": [
        "val_data = pd.read_csv('/content/gdrive/MyDrive/data/val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szTsV5p3tj5w"
      },
      "source": [
        "def df_sampler(df_file):\n",
        "  df = pd.read_csv(df_file)\n",
        "  class_names_meta = np.unique(df['Label'])\n",
        "  sorted(class_names_meta)\n",
        "  df_list_names = []\n",
        "  df_list = []\n",
        "  for class_name in class_names_meta:\n",
        "    class_name_rep = class_name.replace(\" \",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\"/\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\" \",\"\")\n",
        "    class_name_rep = class_name_rep.replace(\",_\",\"_\")\n",
        "    class_name_rep = class_name_rep.replace(\",\",\"_\")\n",
        "    #print(class_name_rep)\n",
        "    h = \"df_\" + str(class_name_rep)\n",
        "    vars()[h] = df[df['Label'] == class_name]\n",
        "    #vars()[h] = vars()[h].sample(amount)\n",
        "    print(\"Created: \",h,' ',len(vars()[h]))\n",
        "    df_list.append(vars()[h])\n",
        "    df_list_names.append(h)\n",
        "  #final = pd.concat(df_list)\n",
        "  return df_list,df_list_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO5IIk_n1kL7"
      },
      "source": [
        "Created:  df_Balcony   45\n",
        "Created:  df_Bathroom   127\n",
        "Created:  df_Beach   191\n",
        "Created:  df_Bedroom   403\n",
        "Created:  df_Buildings   48\n",
        "Created:  df_Education_science   1839\n",
        "Created:  df_Forest_field_jungle   194\n",
        "Created:  df_Garden   130\n",
        "Created:  df_Hospital   14\n",
        "Created:  df_Kitchen   448\n",
        "Created:  df_Living_room   939\n",
        "Created:  df_Mountains_hills_desert_sky   195\n",
        "Created:  df_Museum   25\n",
        "Created:  df_Noise   1107\n",
        "Created:  df_Office   9013\n",
        "Created:  df_Other_rooms   326\n",
        "Created:  df_Others   72\n",
        "Created:  df_Pathways   2975\n",
        "Created:  df_Recreation   202\n",
        "Created:  df_Restaurant_Bar   2871\n",
        "Created:  df_Shop   380\n",
        "Created:  df_Sport_fields   135\n",
        "Created:  df_Transportation   2823\n",
        "Created:  df_Water   2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC3WwDd21tbB"
      },
      "source": [
        "Created:  df_Balcony   45\n",
        "Created:  df_Bathroom   127\n",
        "Created:  df_Beach   191\n",
        "Created:  df_Bedroom   403\n",
        "Created:  df_Buildings   48\n",
        "Created:  df_Education_science   1839\n",
        "Created:  df_Forest_field_jungle   194\n",
        "Created:  df_Garden   130\n",
        "Created:  df_Hospital   14\n",
        "Created:  df_Kitchen   448\n",
        "Created:  df_Living_room   939\n",
        "Created:  df_Mountains_hills_desert_sky   195\n",
        "Created:  df_Museum   25\n",
        "Created:  df_Noise   1107\n",
        "Created:  df_Office   9013\n",
        "Created:  df_Other_rooms   326\n",
        "Created:  df_Others   72\n",
        "Created:  df_Pathways   2975\n",
        "Created:  df_Recreation   202\n",
        "Created:  df_Restaurant_Bar   2871\n",
        "Created:  df_Shop   380\n",
        "Created:  df_Sport_fields   135\n",
        "Created:  df_Transportation   2823\n",
        "Created:  df_Water   2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSi51DDBx1GW"
      },
      "source": [
        "df_eval_list = [df_Balcony_eval,df_Bathroom_eval,df_Beach_eval,df_Bedroom_eval,df_Buildings_eval,df_Education_science_eval,df_Forest_field_jungle_eval,df_Garden_eval,df_Hospital_eval,df_Kitchen_eval,df_Living_room_eval,df_Mountains_hills_desert_sky_eval,df_Museum_eval,df_Noise_eval,df_Office_eval,df_Other_rooms_eval,df_Others_eval,df_Pathways_eval,df_Recreation_eval,df_Restaurant_Bar_eval,df_Shop_eval,df_Sport_fields_eval,df_Transportation_eval,df_Water_eval]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky_YlErA3Avc"
      },
      "source": [
        "df_val_list = [df_Balcony_val,df_Bathroom_val,df_Beach_val,df_Bedroom_val,df_Buildings_val,df_Education_science_val,df_Forest_field_jungle_val,df_Garden_val,df_Hospital_val,df_Kitchen_val,df_Living_room_val,df_Mountains_hills_desert_sky_val,df_Museum_val,df_Noise_val,df_Office_val,df_Other_rooms_val,df_Others_val,df_Pathways_val,df_Recreation_val,df_Restaurant_Bar_val,df_Shop_val,df_Sport_fields_val,df_Transportation_val,df_Water_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFmg7sEt3aSM"
      },
      "source": [
        "len(final_eval_list),len(final_val_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r81n_82T3eLd"
      },
      "source": [
        "final_val_list = pd.concat(df_val_list)\n",
        "final_val_list.to_csv('val_updated.csv')\n",
        "files.download('val_updated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvfVomds2t49"
      },
      "source": [
        "final_eval_list = pd.concat(df_eval_list)\n",
        "final_eval_list.to_csv('eval.csv')\n",
        "files.download('eval.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAlicO9IvcQd"
      },
      "source": [
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/data/val.csv')\n",
        "class_names_meta = np.unique(df['Label'])\n",
        "sorted(class_names_meta)\n",
        "df_list_names = []\n",
        "df_list = []\n",
        "for class_name in class_names_meta:\n",
        "  class_name_rep = class_name.replace(\" \",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\"/\",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\" \",\"\")\n",
        "  class_name_rep = class_name_rep.replace(\",_\",\"_\")\n",
        "  class_name_rep = class_name_rep.replace(\",\",\"_\")\n",
        "  #print(class_name_rep)\n",
        "  h = \"df_\" + str(class_name_rep)\n",
        "  vars()[h] = df[df['Label'] == class_name]\n",
        "  #vars()[h] = vars()[h].sample(amount)\n",
        "  print(\"Created: \",h,' ',len(vars()[h]))\n",
        "  #df_list.append(vars()[h])\n",
        "  #df_list_names.append(h)\n",
        "#final = pd.concat(df_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA_Lpj-80Bzl"
      },
      "source": [
        "df_Transportation_val,df_Transportation_eval = train_test_split(df_Transportation,test_size = 0.3,shuffle='True',random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcMWC0mYwMrH"
      },
      "source": [
        "df_Water_val,df_Water_eval = train_test_split(df_Water,test_size = 0.3,shuffle='True',random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U44mlF3w_aP"
      },
      "source": [
        "len(df_Kitchen_val),len(df_Kitchen_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAP6BBqSuSBm"
      },
      "source": [
        "df_sampler('/content/gdrive/MyDrive/data/val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT2UszEnvXeg"
      },
      "source": [
        "df_balcony"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6DTGHz17xyo"
      },
      "source": [
        "#Eliminating Dups in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCt4YvYp7_0M"
      },
      "source": [
        "##Inputing the data frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE7WZrr172Yj"
      },
      "source": [
        "eval_test = pd.read_csv('/content/gdrive/MyDrive/Final DataFrames for testing/eval (1).csv')\n",
        "test_test = pd.read_csv('/content/gdrive/MyDrive/Final DataFrames for testing/test (2).csv')\n",
        "train_test = pd.read_csv('/content/gdrive/MyDrive/Final DataFrames for testing/train (6).csv')\n",
        "val_test = pd.read_csv('/content/gdrive/MyDrive/Final DataFrames for testing/val_updated (1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_IdkMD_89l6"
      },
      "source": [
        "def create_photo_col(df):\n",
        "  df['Photo'] = df['Image'].apply(lambda x:x.split('/')[-1])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WayOSi9Wy5"
      },
      "source": [
        "eval_new = create_photo_col(eval_test)\n",
        "test_new = create_photo_col(test_test)\n",
        "train_new = create_photo_col(train_test)\n",
        "val_new = create_photo_col(val_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX19jNxULYFy"
      },
      "source": [
        "train_new = train_new[['Image','Label','Photo']]\n",
        "test_new = test_new[['Image','Label','Photo']]\n",
        "val_new = val_new[['Image','Label','Photo']]\n",
        "eval_new = eval_new[['Image','Label','Photo']]\n",
        "len(train_new),len(test_new),len(val_new),len(eval_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuCMw5XsMMOl"
      },
      "source": [
        "np.unique(train_new['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fXH6vAk_KUe"
      },
      "source": [
        "train_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdwRc5mpAi6R"
      },
      "source": [
        "non_train = pd.concat([eval_new,test_new,val_new])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_RZtWC3WBXm"
      },
      "source": [
        "non_test = pd.concat([eval_new,val_new])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFWZwKX4M3i-"
      },
      "source": [
        "len(non_test),len(test_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNaDzkSmJ17y"
      },
      "source": [
        "len(non_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQYH3HyLNM-r"
      },
      "source": [
        "non_train.to_csv('nontrain.csv')\n",
        "files.download('nontrain.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A69xquK-BRrY"
      },
      "source": [
        "###Common elements between eval and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnQv5mpH9u_8"
      },
      "source": [
        "mergedStuff = pd.merge(eval_new, train_updated, on=['Photo'], how='inner')\n",
        "len(np.unique(mergedStuff['Label_x'])),len(eval_new),len(train_updated),len(mergedStuff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDhev_JwR_mg"
      },
      "source": [
        "mergedStuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyDAP8LjBYb8"
      },
      "source": [
        "###Common elements between val and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcKzRnG8_mfH"
      },
      "source": [
        "mergedStuff_2 = pd.merge(final_user6, train_updated, on=['Photo'], how='inner')\n",
        "len(np.unique(mergedStuff_2['Label_x']))\n",
        "\n",
        "len(mergedStuff_2),len(val_new),len(np.unique(mergedStuff_2['Label_x'])),len(train_updated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ_8MAl46YbG"
      },
      "source": [
        "common_test_train_updated = mergedStuff_2['Photo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNmpmBwrRXxb"
      },
      "source": [
        "mergedStuff_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7097NmWBiOy"
      },
      "source": [
        "###Common elements between test and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNrBo86I_sVV"
      },
      "source": [
        "mergedStuff_3 = pd.merge(test_new, train_updated, on=['Photo'], how='inner')\n",
        "#len(np.unique(mergedStuff_3['Label_x']))\n",
        "len(mergedStuff_3),len(test_new),len(train_updated),len(np.unique(mergedStuff_3['Label_x']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJHkRdtGSS4R"
      },
      "source": [
        "mergedStuff_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv9kI43uScqx"
      },
      "source": [
        "train_updated[train_updated['Label'] == 'Water']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAKEQzrYBlor"
      },
      "source": [
        "###Common elements between eval and val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EawHt81fASQ7"
      },
      "source": [
        "mergedStuff_4 = pd.merge(val_new, eval_new, on=['Photo'], how='inner')\n",
        "# len(np.unique(mergedStuff_4['Label_x']))\n",
        "\n",
        "len(mergedStuff_4),len(val_new),len(np.unique(mergedStuff_4['Label_x'])),len(eval_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWFZBxVyC2nc"
      },
      "source": [
        "mergedStuff_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaMtSLEOGGzk"
      },
      "source": [
        "###Common elements between eval and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qBdUC4OGGzl"
      },
      "source": [
        "mergedStuff_5 = pd.merge(test_new, eval_new, on=['Photo'], how='inner')\n",
        "# len(np.unique(mergedStuff_4['Label_x']))\n",
        "\n",
        "len(mergedStuff_5),len(test_new),len(np.unique(mergedStuff_5['Label_x'])),len(eval_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6RZtjHGGzl"
      },
      "source": [
        "mergedStuff_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csW_UCbQIJYh"
      },
      "source": [
        "###Common elements between val and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCSbcdTIJYh"
      },
      "source": [
        "mergedStuff_6 = pd.merge(test_new, val_new, on=['Photo'], how='inner')\n",
        "# len(np.unique(mergedStuff_4['Label_x']))\n",
        "\n",
        "len(mergedStuff_6),len(test_new),len(np.unique(mergedStuff_6['Label_x'])),len(val_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO11--1gIJYi"
      },
      "source": [
        "mergedStuff_6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-bWKf8LKKvb"
      },
      "source": [
        "###Common elements between train and non-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKocYgRjKKvb"
      },
      "source": [
        "mergedStuff_7 = pd.merge(final_user6, val_new, on=['Photo'], how='inner')\n",
        "# len(np.unique(mergedStuff_4['Label_x']))\n",
        "\n",
        "len(mergedStuff_7),len(train_new),len(np.unique(mergedStuff_7['Label_x'])),len(np.unique(mergedStuff_7['Label_y'])),len(non_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wt2pIDIKKvc"
      },
      "source": [
        "mergedStuff_7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fFGoc5Ofty"
      },
      "source": [
        "common_train_nontrain = list(mergedStuff_7['Photo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KskwtcAO4mV"
      },
      "source": [
        "len(common_train_nontrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2A1PBkXPcPW"
      },
      "source": [
        "train_updated_1 = train_updated[~train_updated['Photo'].isin(common_test_train_updated)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB18Al_WRtjN"
      },
      "source": [
        "len(np.unique(train_updated_1['Image'])),len(np.unique(train_updated_1['Photo']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCdjh2QZ8nuT"
      },
      "source": [
        "test_training = train_updated[train_updated_1.isin(c)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuL8L-D79JCJ"
      },
      "source": [
        "train_updated_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EWuEJh398MR"
      },
      "source": [
        "train_updated_1[train_updated_1['Photo']=='20160608_152708_000.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAz9H5DA8zsH"
      },
      "source": [
        "train_updated_1[train_updated_1.duplicated(['Photo'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubmAV8Xa7CeR"
      },
      "source": [
        "t = train_updated_1['Photo']\n",
        "c = t[t.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FODLtp5U7JPM"
      },
      "source": [
        "tt = train_updated_1['Image'].apply(lambda x:x.split('/')[-1])\n",
        "a = np.unique(tt)\n",
        "b= np.unique(train_updated_1['Photo'])\n",
        "diff = list(set(a)-set(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW9cMKHa7dvO"
      },
      "source": [
        "len(a),len(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrVV0dTqSwFN"
      },
      "source": [
        "len(np.unique(train_updated_1['Label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnVOtbcyRJ4r"
      },
      "source": [
        "train_updated_1.to_csv('train_updated.csv')\n",
        "files.download('train_updated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOS8c68DWeX_"
      },
      "source": [
        "###Common elements between test and non-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRvUEhtDcxqm"
      },
      "source": [
        "test_new[test_new['Label']=='Water']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUW2925fccvd"
      },
      "source": [
        "non_test[non_test['Label']=='Water']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhNquw1AcjpW"
      },
      "source": [
        "train_updated[train_updated['Label']=='Water']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MViwGC4WeX_"
      },
      "source": [
        "mergedStuff_8 = pd.merge(test_new, non_test, on=['Photo'], how='inner')\n",
        "# len(np.unique(mergedStuff_4['Label_x']))\n",
        "\n",
        "len(mergedStuff_8),len(test_new),len(np.unique(mergedStuff_8['Label_x'])),len(np.unique(mergedStuff_8['Label_y'])),len(non_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_P-U-UUWeYA"
      },
      "source": [
        "mergedStuff_8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JMcQxYkWeYA"
      },
      "source": [
        "common_train_nontrain = list(mergedStuff_7['Photo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlRKFTDjWeYA"
      },
      "source": [
        "len(common_train_nontrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PYJr4NsWeYA"
      },
      "source": [
        "train_updated = train_new[~train_new['Photo'].isin(common_train_nontrain)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMkE4DTaWeYA"
      },
      "source": [
        "len(np.unique(train_updated['Image'])),len(np.unique(train_updated['Photo']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MivyIFOWWeYA"
      },
      "source": [
        "train_updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RE8GTfYWeYB"
      },
      "source": [
        "train_updated.to_csv('train_updated.csv')\n",
        "files.download('train_updated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lwUmLnhnnsm"
      },
      "source": [
        "###Creating Test set from User 06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_uijos2nrQv"
      },
      "source": [
        "test_user_6 = pd.read_csv('test_test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gf-A7B2n70X"
      },
      "source": [
        "# test_user_6 = test_user_6.loc[:]\n",
        "test_user_6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oawwJ_TooHrX"
      },
      "source": [
        "test_user_6.columns = ['ID','Path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPdgoMfgpVw9"
      },
      "source": [
        "test_user_6['Photo'] = test_user_6['Path'].apply(lambda x: x.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpX51YRyprG0"
      },
      "source": [
        "user6_labels = pd.concat(pd.read_excel('/content/user_06.xlsx',sheet_name=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8X_3ZIlqAlh"
      },
      "source": [
        "user6_lab = pd.DataFrame(user6_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R9eB_oIrIo9"
      },
      "source": [
        "user6_lab.to_csv('user6.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-G5dKFNrgkm"
      },
      "source": [
        "user6_lab = pd.DataFrame(user6_lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUM8MT-roxR"
      },
      "source": [
        "user6_csv = pd.read_csv('/content/user66.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQR6qaBsuf6_"
      },
      "source": [
        "user6_csv = user6_csv[['Photo','Class']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD0h9p6dr1FV"
      },
      "source": [
        "user6_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o33-LLZEr9r6"
      },
      "source": [
        "final_user6 = pd.merge(user6_csv,test_user_6,on='Photo',how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJTRaGqvVAh"
      },
      "source": [
        "a=np.unique(final_user6['Class'])\n",
        "b = np.unique(train_updated['Label'])\n",
        "list(set(b)-set(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbS3Ujn1m9S"
      },
      "source": [
        "final_user6['Class'] = final_user6['Class'].apply(lambda x:x.replace('/',','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DySPWiaCxMo6"
      },
      "source": [
        "final_user6['Image'] = '/data/s4133366/data/test'+os.sep+final_user6['Class']+os.sep+final_user6['Photo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7yx29aezsGG"
      },
      "source": [
        "final_user6.columns = ['Photo','Label','ID','Path','Image']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPws1f-b5RBZ"
      },
      "source": [
        "final_user6.to_csv('new_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDD4A3miFhiU"
      },
      "source": [
        "final_user6.groupby('Label').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCni1iNFQs32"
      },
      "source": [
        "#Creating collated plots for metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPtjceFHBw20"
      },
      "source": [
        "a = np.load('/content/history_topic_weight_k4_places_20_imp2.npy',allow_pickle=True).item()\n",
        "b = np.load('/content/history_topic_weight_k4_places_20_imp2_1.npy',allow_pickle=True).item()\n",
        "# c = np.load('/content/history_baseline_places20_latest_2.npy',allow_pickle=True).item()\n",
        "# d = np.load('/content/history_topic_weight_k4_places_365_imp9_final_3.npy',allow_pickle=True).item()\n",
        "# e = np.load('/content/history_topic_weight_k4_places_365_imp9_final_4.npy',allow_pickle=True).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8SPz1Zyg9X9"
      },
      "source": [
        "len(df_a),len(df_b)#,len(df_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgNJj6PbRKd4"
      },
      "source": [
        "df_a = pd.DataFrame(a)\n",
        "df_b = pd.DataFrame(b)\n",
        "#df_c = pd.DataFrame(c)\n",
        "# df_d = pd.DataFrame(d)\n",
        "# df_e = pd.DataFrame(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8gpjOo4RT_x"
      },
      "source": [
        "df_b.index = df_b.index + 100\n",
        "# df_c.index = df_c.index + 100\n",
        "# df_d.index = df_d.index + 130\n",
        "# df_e.index = df_e.index+170"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy5WTnTuIR49"
      },
      "source": [
        "df_c.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsOHg_OrQzPX"
      },
      "source": [
        "test = np.load('/content/results_topic_weight_k4_places_20_imp2_1.npy',allow_pickle=True).item()\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfmiZ629lU9E"
      },
      "source": [
        "len(df_a),len(df_b),len(df_c)#,len(df_d),len(df_e)#170"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylia9mWcCC6b"
      },
      "source": [
        "df_d.index = df_d.index+120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcAQ8ztmHI1H"
      },
      "source": [
        "df_final = pd.concat([df_a,df_b])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNO016NYJftK"
      },
      "source": [
        "len(df_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989q1OoHD2Xx"
      },
      "source": [
        "test = pd.DataFrame(test,index=df_a.index)\n",
        "test = test[:1]\n",
        "test.columns = ['loss','categorical_accuracy','precision','recall']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyeutvOor0Hi"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2RGgF7Yoyus"
      },
      "source": [
        "df_final = df_final.append(test,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrJH7bE2sIaw"
      },
      "source": [
        "df_final =df_final.drop(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAtAWOhZp4-y"
      },
      "source": [
        "df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7f9xglXEPns"
      },
      "source": [
        "b = a.tail(1)\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUbCp-HfrRLM"
      },
      "source": [
        "read_train = df_final[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoTlLF2Xl5WR"
      },
      "source": [
        "read_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y7cdrKLrg-p"
      },
      "source": [
        "read_test = df_final.iloc[200,:]\n",
        "read_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT-XRUt3FSZd"
      },
      "source": [
        "df_final =pd.concat([df_a,df_b,df_c,df_d],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sMGt08NFxZN"
      },
      "source": [
        "read_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_PaBiQG0rt"
      },
      "source": [
        "read_test = b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxQhjQk8FYvo"
      },
      "source": [
        "model_string = 'Topic Model Implementation 2 using Places20'\n",
        "history_path = 'top_model_2_p20_'\n",
        "num = 200\n",
        "plt.plot(read_train['categorical_accuracy']*100)\n",
        "#plt.plot(read_train['val_categorical_accuracy']*100)\n",
        "text = str(round(read_test['categorical_accuracy']*100))+'%'\n",
        "plt.scatter(num,read_test['categorical_accuracy']*100, color='g')\n",
        "plt.text(s = text,x = len(read_train+1),y = read_test['categorical_accuracy']*100, fontsize=9) #\n",
        "\n",
        "plt.title('Model accuracy for '+str(model_string))\n",
        "plt.ylabel('Categorical Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.savefig('accuracy_'+str(history_path)+'.png')\n",
        "plt.clf()\n",
        "\n",
        "# summarize history for loss\n",
        "\n",
        "plt.plot(read_train['loss'])\n",
        "#plt.plot(read_train['val_loss'])\n",
        "text = str(round(read_test['loss']))\n",
        "plt.scatter(num,read_test['loss'], color='g')\n",
        "plt.text(s = text,x = 2+len(read_train+1),y = 2+read_test['loss'], fontsize=9) #\n",
        "\n",
        "plt.title('Model loss for '+str(model_string))\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "#plt.show()\n",
        "plt.savefig('loss_'+str(history_path)+'.png')\n",
        "plt.clf()\n",
        "\n",
        "#summarize history for Precision \n",
        "plt.plot(read_train['precision']*100)\n",
        "#plt.plot(read_train['val_precision']*100)\n",
        "text = str(round(read_test['precision']*100))+'%'\n",
        "plt.scatter(num,read_test['precision']*100, color='g')\n",
        "plt.text(s = text,x = 2+len(read_train+1),y = 2+read_test['precision']*100, fontsize=9) #\n",
        "plt.title('Model precision for '+str(model_string))\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.savefig('precision_'+str(history_path)+'.png')\n",
        "plt.clf()\n",
        "\n",
        "#summarize history for Recall \n",
        "\n",
        "\n",
        "\n",
        "plt.plot(read_train['recall']*100)\n",
        "#plt.plot(read_train['val_recall']*100)\n",
        "text = str(round(read_test['recall']*100))+'%'\n",
        "plt.scatter(num,read_test['recall']*100, color='g')\n",
        "plt.text(s = text,x = 2+len(read_train+1),y = 2+read_test['recall']*100, fontsize=9) #\n",
        "plt.title('Model recall for '+str(model_string))\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('recall_'+str(history_path)+'.png')\n",
        "plt.clf()\n",
        "\n",
        "\n",
        "plt.plot(read_train['lr'])\n",
        "plt.title('Model Learning rate for '+str(model_string))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('lr_'+str(history_path)+'.png')\n",
        "plt.clf()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxD9Y10rXkwH"
      },
      "source": [
        "read_test['recall']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRmAL5fxWI0p"
      },
      "source": [
        "plt.plot(read_train['recall']*100)\n",
        "plt.plot(read_train['val_recall']*100)\n",
        "text = str(round(read_test['recall']*100))+'%'\n",
        "plt.scatter(200,read_test['recall']*100, color='g')\n",
        "plt.text(s = text,x = 2+len(read_train+2),y = 2+read_test['recall']*100, fontsize=9) #\n",
        "plt.title('Model recall for '+str('model_string'))\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val','test'], loc='upper left')\n",
        "# plt.savefig('recall_'+str(history_path)+'.png')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Yv54dSMJSP"
      },
      "source": [
        "read_train['lr'].tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjeV5KMLlYqh"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.AveragePooling2D(pool_size=(7,7))(x)\n",
        "x = tf.keras.layers.Flatten(name =\"flatten\")(x)\n",
        "x = tf.keras.layers.Dense(1024,activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "preds = tf.keras.layers.Dense(365,activation=\"softmax\")(x)\n",
        "\n",
        "# #Activating the model\n",
        "\n",
        "model = tf.keras.Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "# opti = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "# model.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()],loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqjsGdB2ocQL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THrqFMuoz0JX"
      },
      "source": [
        "test = pd.read_csv('/content/test_updated (1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Fv5UJRkCgm"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo4y66nRgJ6d"
      },
      "source": [
        "test.columns = ['ID','Image']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH-yHMBtgRRN"
      },
      "source": [
        "test['Label'] = test['Image'].apply(lambda x:x.split('/')[-2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSyQv1ZQkktw"
      },
      "source": [
        "test.to_csv('test_updated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFLPQ25Wp932"
      },
      "source": [
        "labels = pd.read_csv('/content/short_p365_test (1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxqivPp_qhPr"
      },
      "source": [
        "labels = labels['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na215JDOqlzQ"
      },
      "source": [
        "labels_list = np.unique(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c179OavkqvFO"
      },
      "source": [
        "labels_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKUr3__2q9Hr"
      },
      "source": [
        "range(365)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjBcNNkQqxmk"
      },
      "source": [
        "labels_dict = dict(list(enumerate(labels_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AVk9DyywGbw"
      },
      "source": [
        "inv_map = {v: k for k, v in labels_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ikmn1esTYw"
      },
      "source": [
        "inv_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzspmnDaojqk"
      },
      "source": [
        "#Plotting legible Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BjWfOConnz"
      },
      "source": [
        "df_cm = pd.read_csv('/content/imp2_k9 - Sheet1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdxcJbOAqc4x"
      },
      "source": [
        "df_cm = df_cm.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjv5MuaateZ5"
      },
      "source": [
        "labels = dict(enumerate(df_cm.columns))\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MdsyW4esDYL"
      },
      "source": [
        "df_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m4uA1YMpojv"
      },
      "source": [
        "model_string = 'Topic Model Implementation 1'\n",
        "db_name = 'EgoPlaces with 9 topics'\n",
        "plt.figure(figsize=(20,20))\n",
        "ax= plt.subplot()\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14},cmap=\"BuPu\") # font size\n",
        "plt.title('Confusion matrix for '+model_string+' using '+db_name)\n",
        "plt.yticks(rotation = 0) \n",
        "ax.xaxis.set_ticklabels(df_cm.columns)\n",
        "ax.yaxis.set_ticklabels(df_cm.columns)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "#print('Saving plot in '+cm_path+' ...')\n",
        "plt.savefig('/content/imp1_pred_k9_partial.png',bbox_inches=\"tight\")\n",
        "\n",
        "print('Model predicted ....')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsguaG-iSNFz"
      },
      "source": [
        "#Creating big test data-set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBlARjjrSRWQ"
      },
      "source": [
        "a = pd.read_csv('/content/eval (2).csv')\n",
        "b = pd.read_csv('/content/test_updated (3).csv')\n",
        "c = pd.read_csv('/content/val_updated (2).csv')\n",
        "df_final = pd.concat([a,b,c])\n",
        "df_final = df_final[['Image','Label']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvBwOh8-TY-3"
      },
      "source": [
        "df_final = df_final.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7dIB07vUXIJ"
      },
      "source": [
        "len(np.unique(df_final['Label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXTMmxoWEIH"
      },
      "source": [
        "len(df_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DyqiocFUuu0"
      },
      "source": [
        "df_final.to_csv('test_final.csv')\n",
        "files.download('test_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpEyszf4DrB"
      },
      "source": [
        "#Mapping the objects to imagenet values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sDuRCi_4ISx"
      },
      "source": [
        "imagenet = {0: 'tench, Tinca tinca',\n",
        " 1: 'goldfish, Carassius auratus',\n",
        " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
        " 3: 'tiger shark, Galeocerdo cuvieri',\n",
        " 4: 'hammerhead, hammerhead shark',\n",
        " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
        " 6: 'stingray',\n",
        " 7: 'cock',\n",
        " 8: 'hen',\n",
        " 9: 'ostrich, Struthio camelus',\n",
        " 10: 'brambling, Fringilla montifringilla',\n",
        " 11: 'goldfinch, Carduelis carduelis',\n",
        " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
        " 13: 'junco, snowbird',\n",
        " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
        " 15: 'robin, American robin, Turdus migratorius',\n",
        " 16: 'bulbul',\n",
        " 17: 'jay',\n",
        " 18: 'magpie',\n",
        " 19: 'chickadee',\n",
        " 20: 'water ouzel, dipper',\n",
        " 21: 'kite',\n",
        " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
        " 23: 'vulture',\n",
        " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
        " 25: 'European fire salamander, Salamandra salamandra',\n",
        " 26: 'common newt, Triturus vulgaris',\n",
        " 27: 'eft',\n",
        " 28: 'spotted salamander, Ambystoma maculatum',\n",
        " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
        " 30: 'bullfrog, Rana catesbeiana',\n",
        " 31: 'tree frog, tree-frog',\n",
        " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
        " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
        " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
        " 35: 'mud turtle',\n",
        " 36: 'terrapin',\n",
        " 37: 'box turtle, box tortoise',\n",
        " 38: 'banded gecko',\n",
        " 39: 'common iguana, iguana, Iguana iguana',\n",
        " 40: 'American chameleon, anole, Anolis carolinensis',\n",
        " 41: 'whiptail, whiptail lizard',\n",
        " 42: 'agama',\n",
        " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
        " 44: 'alligator lizard',\n",
        " 45: 'Gila monster, Heloderma suspectum',\n",
        " 46: 'green lizard, Lacerta viridis',\n",
        " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
        " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
        " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
        " 50: 'American alligator, Alligator mississipiensis',\n",
        " 51: 'triceratops',\n",
        " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
        " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
        " 54: 'hognose snake, puff adder, sand viper',\n",
        " 55: 'green snake, grass snake',\n",
        " 56: 'king snake, kingsnake',\n",
        " 57: 'garter snake, grass snake',\n",
        " 58: 'water snake',\n",
        " 59: 'vine snake',\n",
        " 60: 'night snake, Hypsiglena torquata',\n",
        " 61: 'boa constrictor, Constrictor constrictor',\n",
        " 62: 'rock python, rock snake, Python sebae',\n",
        " 63: 'Indian cobra, Naja naja',\n",
        " 64: 'green mamba',\n",
        " 65: 'sea snake',\n",
        " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
        " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
        " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
        " 69: 'trilobite',\n",
        " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
        " 71: 'scorpion',\n",
        " 72: 'black and gold garden spider, Argiope aurantia',\n",
        " 73: 'barn spider, Araneus cavaticus',\n",
        " 74: 'garden spider, Aranea diademata',\n",
        " 75: 'black widow, Latrodectus mactans',\n",
        " 76: 'tarantula',\n",
        " 77: 'wolf spider, hunting spider',\n",
        " 78: 'tick',\n",
        " 79: 'centipede',\n",
        " 80: 'black grouse',\n",
        " 81: 'ptarmigan',\n",
        " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
        " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
        " 84: 'peacock',\n",
        " 85: 'quail',\n",
        " 86: 'partridge',\n",
        " 87: 'African grey, African gray, Psittacus erithacus',\n",
        " 88: 'macaw',\n",
        " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
        " 90: 'lorikeet',\n",
        " 91: 'coucal',\n",
        " 92: 'bee eater',\n",
        " 93: 'hornbill',\n",
        " 94: 'hummingbird',\n",
        " 95: 'jacamar',\n",
        " 96: 'toucan',\n",
        " 97: 'drake',\n",
        " 98: 'red-breasted merganser, Mergus serrator',\n",
        " 99: 'goose',\n",
        " 100: 'black swan, Cygnus atratus',\n",
        " 101: 'tusker',\n",
        " 102: 'echidna, spiny anteater, anteater',\n",
        " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
        " 104: 'wallaby, brush kangaroo',\n",
        " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        " 106: 'wombat',\n",
        " 107: 'jellyfish',\n",
        " 108: 'sea anemone, anemone',\n",
        " 109: 'brain coral',\n",
        " 110: 'flatworm, platyhelminth',\n",
        " 111: 'nematode, nematode worm, roundworm',\n",
        " 112: 'conch',\n",
        " 113: 'snail',\n",
        " 114: 'slug',\n",
        " 115: 'sea slug, nudibranch',\n",
        " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
        " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
        " 118: 'Dungeness crab, Cancer magister',\n",
        " 119: 'rock crab, Cancer irroratus',\n",
        " 120: 'fiddler crab',\n",
        " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
        " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
        " 125: 'hermit crab',\n",
        " 126: 'isopod',\n",
        " 127: 'white stork, Ciconia ciconia',\n",
        " 128: 'black stork, Ciconia nigra',\n",
        " 129: 'spoonbill',\n",
        " 130: 'flamingo',\n",
        " 131: 'little blue heron, Egretta caerulea',\n",
        " 132: 'American egret, great white heron, Egretta albus',\n",
        " 133: 'bittern',\n",
        " 134: 'crane',\n",
        " 135: 'limpkin, Aramus pictus',\n",
        " 136: 'European gallinule, Porphyrio porphyrio',\n",
        " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
        " 138: 'bustard',\n",
        " 139: 'ruddy turnstone, Arenaria interpres',\n",
        " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
        " 141: 'redshank, Tringa totanus',\n",
        " 142: 'dowitcher',\n",
        " 143: 'oystercatcher, oyster catcher',\n",
        " 144: 'pelican',\n",
        " 145: 'king penguin, Aptenodytes patagonica',\n",
        " 146: 'albatross, mollymawk',\n",
        " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
        " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
        " 149: 'dugong, Dugong dugon',\n",
        " 150: 'sea lion',\n",
        " 151: 'Chihuahua',\n",
        " 152: 'Japanese spaniel',\n",
        " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
        " 154: 'Pekinese, Pekingese, Peke',\n",
        " 155: 'Shih-Tzu',\n",
        " 156: 'Blenheim spaniel',\n",
        " 157: 'papillon',\n",
        " 158: 'toy terrier',\n",
        " 159: 'Rhodesian ridgeback',\n",
        " 160: 'Afghan hound, Afghan',\n",
        " 161: 'basset, basset hound',\n",
        " 162: 'beagle',\n",
        " 163: 'bloodhound, sleuthhound',\n",
        " 164: 'bluetick',\n",
        " 165: 'black-and-tan coonhound',\n",
        " 166: 'Walker hound, Walker foxhound',\n",
        " 167: 'English foxhound',\n",
        " 168: 'redbone',\n",
        " 169: 'borzoi, Russian wolfhound',\n",
        " 170: 'Irish wolfhound',\n",
        " 171: 'Italian greyhound',\n",
        " 172: 'whippet',\n",
        " 173: 'Ibizan hound, Ibizan Podenco',\n",
        " 174: 'Norwegian elkhound, elkhound',\n",
        " 175: 'otterhound, otter hound',\n",
        " 176: 'Saluki, gazelle hound',\n",
        " 177: 'Scottish deerhound, deerhound',\n",
        " 178: 'Weimaraner',\n",
        " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
        " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
        " 181: 'Bedlington terrier',\n",
        " 182: 'Border terrier',\n",
        " 183: 'Kerry blue terrier',\n",
        " 184: 'Irish terrier',\n",
        " 185: 'Norfolk terrier',\n",
        " 186: 'Norwich terrier',\n",
        " 187: 'Yorkshire terrier',\n",
        " 188: 'wire-haired fox terrier',\n",
        " 189: 'Lakeland terrier',\n",
        " 190: 'Sealyham terrier, Sealyham',\n",
        " 191: 'Airedale, Airedale terrier',\n",
        " 192: 'cairn, cairn terrier',\n",
        " 193: 'Australian terrier',\n",
        " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
        " 195: 'Boston bull, Boston terrier',\n",
        " 196: 'miniature schnauzer',\n",
        " 197: 'giant schnauzer',\n",
        " 198: 'standard schnauzer',\n",
        " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
        " 200: 'Tibetan terrier, chrysanthemum dog',\n",
        " 201: 'silky terrier, Sydney silky',\n",
        " 202: 'soft-coated wheaten terrier',\n",
        " 203: 'West Highland white terrier',\n",
        " 204: 'Lhasa, Lhasa apso',\n",
        " 205: 'flat-coated retriever',\n",
        " 206: 'curly-coated retriever',\n",
        " 207: 'golden retriever',\n",
        " 208: 'Labrador retriever',\n",
        " 209: 'Chesapeake Bay retriever',\n",
        " 210: 'German short-haired pointer',\n",
        " 211: 'vizsla, Hungarian pointer',\n",
        " 212: 'English setter',\n",
        " 213: 'Irish setter, red setter',\n",
        " 214: 'Gordon setter',\n",
        " 215: 'Brittany spaniel',\n",
        " 216: 'clumber, clumber spaniel',\n",
        " 217: 'English springer, English springer spaniel',\n",
        " 218: 'Welsh springer spaniel',\n",
        " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
        " 220: 'Sussex spaniel',\n",
        " 221: 'Irish water spaniel',\n",
        " 222: 'kuvasz',\n",
        " 223: 'schipperke',\n",
        " 224: 'groenendael',\n",
        " 225: 'malinois',\n",
        " 226: 'briard',\n",
        " 227: 'kelpie',\n",
        " 228: 'komondor',\n",
        " 229: 'Old English sheepdog, bobtail',\n",
        " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
        " 231: 'collie',\n",
        " 232: 'Border collie',\n",
        " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
        " 234: 'Rottweiler',\n",
        " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
        " 236: 'Doberman, Doberman pinscher',\n",
        " 237: 'miniature pinscher',\n",
        " 238: 'Greater Swiss Mountain dog',\n",
        " 239: 'Bernese mountain dog',\n",
        " 240: 'Appenzeller',\n",
        " 241: 'EntleBucher',\n",
        " 242: 'boxer',\n",
        " 243: 'bull mastiff',\n",
        " 244: 'Tibetan mastiff',\n",
        " 245: 'French bulldog',\n",
        " 246: 'Great Dane',\n",
        " 247: 'Saint Bernard, St Bernard',\n",
        " 248: 'Eskimo dog, husky',\n",
        " 249: 'malamute, malemute, Alaskan malamute',\n",
        " 250: 'Siberian husky',\n",
        " 251: 'dalmatian, coach dog, carriage dog',\n",
        " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
        " 253: 'basenji',\n",
        " 254: 'pug, pug-dog',\n",
        " 255: 'Leonberg',\n",
        " 256: 'Newfoundland, Newfoundland dog',\n",
        " 257: 'Great Pyrenees',\n",
        " 258: 'Samoyed, Samoyede',\n",
        " 259: 'Pomeranian',\n",
        " 260: 'chow, chow chow',\n",
        " 261: 'keeshond',\n",
        " 262: 'Brabancon griffon',\n",
        " 263: 'Pembroke, Pembroke Welsh corgi',\n",
        " 264: 'Cardigan, Cardigan Welsh corgi',\n",
        " 265: 'toy poodle',\n",
        " 266: 'miniature poodle',\n",
        " 267: 'standard poodle',\n",
        " 268: 'Mexican hairless',\n",
        " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
        " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
        " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
        " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
        " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
        " 274: 'dhole, Cuon alpinus',\n",
        " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
        " 276: 'hyena, hyaena',\n",
        " 277: 'red fox, Vulpes vulpes',\n",
        " 278: 'kit fox, Vulpes macrotis',\n",
        " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
        " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
        " 281: 'tabby, tabby cat',\n",
        " 282: 'tiger cat',\n",
        " 283: 'Persian cat',\n",
        " 284: 'Siamese cat, Siamese',\n",
        " 285: 'Egyptian cat',\n",
        " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
        " 287: 'lynx, catamount',\n",
        " 288: 'leopard, Panthera pardus',\n",
        " 289: 'snow leopard, ounce, Panthera uncia',\n",
        " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
        " 291: 'lion, king of beasts, Panthera leo',\n",
        " 292: 'tiger, Panthera tigris',\n",
        " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
        " 294: 'brown bear, bruin, Ursus arctos',\n",
        " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
        " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
        " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
        " 298: 'mongoose',\n",
        " 299: 'meerkat, mierkat',\n",
        " 300: 'tiger beetle',\n",
        " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        " 302: 'ground beetle, carabid beetle',\n",
        " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
        " 304: 'leaf beetle, chrysomelid',\n",
        " 305: 'dung beetle',\n",
        " 306: 'rhinoceros beetle',\n",
        " 307: 'weevil',\n",
        " 308: 'fly',\n",
        " 309: 'bee',\n",
        " 310: 'ant, emmet, pismire',\n",
        " 311: 'grasshopper, hopper',\n",
        " 312: 'cricket',\n",
        " 313: 'walking stick, walkingstick, stick insect',\n",
        " 314: 'cockroach, roach',\n",
        " 315: 'mantis, mantid',\n",
        " 316: 'cicada, cicala',\n",
        " 317: 'leafhopper',\n",
        " 318: 'lacewing, lacewing fly',\n",
        " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
        " 320: 'damselfly',\n",
        " 321: 'admiral',\n",
        " 322: 'ringlet, ringlet butterfly',\n",
        " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        " 324: 'cabbage butterfly',\n",
        " 325: 'sulphur butterfly, sulfur butterfly',\n",
        " 326: 'lycaenid, lycaenid butterfly',\n",
        " 327: 'starfish, sea star',\n",
        " 328: 'sea urchin',\n",
        " 329: 'sea cucumber, holothurian',\n",
        " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
        " 331: 'hare',\n",
        " 332: 'Angora, Angora rabbit',\n",
        " 333: 'hamster',\n",
        " 334: 'porcupine, hedgehog',\n",
        " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
        " 336: 'marmot',\n",
        " 337: 'beaver',\n",
        " 338: 'guinea pig, Cavia cobaya',\n",
        " 339: 'sorrel',\n",
        " 340: 'zebra',\n",
        " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
        " 342: 'wild boar, boar, Sus scrofa',\n",
        " 343: 'warthog',\n",
        " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
        " 345: 'ox',\n",
        " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
        " 347: 'bison',\n",
        " 348: 'ram, tup',\n",
        " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        " 350: 'ibex, Capra ibex',\n",
        " 351: 'hartebeest',\n",
        " 352: 'impala, Aepyceros melampus',\n",
        " 353: 'gazelle',\n",
        " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
        " 355: 'llama',\n",
        " 356: 'weasel',\n",
        " 357: 'mink',\n",
        " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
        " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
        " 360: 'otter',\n",
        " 361: 'skunk, polecat, wood pussy',\n",
        " 362: 'badger',\n",
        " 363: 'armadillo',\n",
        " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
        " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        " 366: 'gorilla, Gorilla gorilla',\n",
        " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
        " 368: 'gibbon, Hylobates lar',\n",
        " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
        " 370: 'guenon, guenon monkey',\n",
        " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
        " 372: 'baboon',\n",
        " 373: 'macaque',\n",
        " 374: 'langur',\n",
        " 375: 'colobus, colobus monkey',\n",
        " 376: 'proboscis monkey, Nasalis larvatus',\n",
        " 377: 'marmoset',\n",
        " 378: 'capuchin, ringtail, Cebus capucinus',\n",
        " 379: 'howler monkey, howler',\n",
        " 380: 'titi, titi monkey',\n",
        " 381: 'spider monkey, Ateles geoffroyi',\n",
        " 382: 'squirrel monkey, Saimiri sciureus',\n",
        " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
        " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
        " 385: 'Indian elephant, Elephas maximus',\n",
        " 386: 'African elephant, Loxodonta africana',\n",
        " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
        " 389: 'barracouta, snoek',\n",
        " 390: 'eel',\n",
        " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
        " 392: 'rock beauty, Holocanthus tricolor',\n",
        " 393: 'anemone fish',\n",
        " 394: 'sturgeon',\n",
        " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
        " 396: 'lionfish',\n",
        " 397: 'puffer, pufferfish, blowfish, globefish',\n",
        " 398: 'abacus',\n",
        " 399: 'abaya',\n",
        " 400: \"academic gown, academic robe, judge's robe\",\n",
        " 401: 'accordion, piano accordion, squeeze box',\n",
        " 402: 'acoustic guitar',\n",
        " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
        " 404: 'airliner',\n",
        " 405: 'airship, dirigible',\n",
        " 406: 'altar',\n",
        " 407: 'ambulance',\n",
        " 408: 'amphibian, amphibious vehicle',\n",
        " 409: 'analog clock',\n",
        " 410: 'apiary, bee house',\n",
        " 411: 'apron',\n",
        " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
        " 413: 'assault rifle, assault gun',\n",
        " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        " 415: 'bakery, bakeshop, bakehouse',\n",
        " 416: 'balance beam, beam',\n",
        " 417: 'balloon',\n",
        " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
        " 419: 'Band Aid',\n",
        " 420: 'banjo',\n",
        " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
        " 422: 'barbell',\n",
        " 423: 'barber chair',\n",
        " 424: 'barbershop',\n",
        " 425: 'barn',\n",
        " 426: 'barometer',\n",
        " 427: 'barrel, cask',\n",
        " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
        " 429: 'baseball',\n",
        " 430: 'basketball',\n",
        " 431: 'bassinet',\n",
        " 432: 'bassoon',\n",
        " 433: 'bathing cap, swimming cap',\n",
        " 434: 'bath towel',\n",
        " 435: 'bathtub, bathing tub, bath, tub',\n",
        " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        " 437: 'beacon, lighthouse, beacon light, pharos',\n",
        " 438: 'beaker',\n",
        " 439: 'bearskin, busby, shako',\n",
        " 440: 'beer bottle',\n",
        " 441: 'beer glass',\n",
        " 442: 'bell cote, bell cot',\n",
        " 443: 'bib',\n",
        " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
        " 445: 'bikini, two-piece',\n",
        " 446: 'binder, ring-binder',\n",
        " 447: 'binoculars, field glasses, opera glasses',\n",
        " 448: 'birdhouse',\n",
        " 449: 'boathouse',\n",
        " 450: 'bobsled, bobsleigh, bob',\n",
        " 451: 'bolo tie, bolo, bola tie, bola',\n",
        " 452: 'bonnet, poke bonnet',\n",
        " 453: 'bookcase',\n",
        " 454: 'bookshop, bookstore, bookstall',\n",
        " 455: 'bottlecap',\n",
        " 456: 'bow',\n",
        " 457: 'bow tie, bow-tie, bowtie',\n",
        " 458: 'brass, memorial tablet, plaque',\n",
        " 459: 'brassiere, bra, bandeau',\n",
        " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
        " 461: 'breastplate, aegis, egis',\n",
        " 462: 'broom',\n",
        " 463: 'bucket, pail',\n",
        " 464: 'buckle',\n",
        " 465: 'bulletproof vest',\n",
        " 466: 'bullet train, bullet',\n",
        " 467: 'butcher shop, meat market',\n",
        " 468: 'cab, hack, taxi, taxicab',\n",
        " 469: 'caldron, cauldron',\n",
        " 470: 'candle, taper, wax light',\n",
        " 471: 'cannon',\n",
        " 472: 'canoe',\n",
        " 473: 'can opener, tin opener',\n",
        " 474: 'cardigan',\n",
        " 475: 'car mirror',\n",
        " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
        " 477: \"carpenter's kit, tool kit\",\n",
        " 478: 'carton',\n",
        " 479: 'car wheel',\n",
        " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        " 481: 'cassette',\n",
        " 482: 'cassette player',\n",
        " 483: 'castle',\n",
        " 484: 'catamaran',\n",
        " 485: 'CD player',\n",
        " 486: 'cello, violoncello',\n",
        " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
        " 488: 'chain',\n",
        " 489: 'chainlink fence',\n",
        " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
        " 491: 'chain saw, chainsaw',\n",
        " 492: 'chest',\n",
        " 493: 'chiffonier, commode',\n",
        " 494: 'chime, bell, gong',\n",
        " 495: 'china cabinet, china closet',\n",
        " 496: 'Christmas stocking',\n",
        " 497: 'church, church building',\n",
        " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
        " 499: 'cleaver, meat cleaver, chopper',\n",
        " 500: 'cliff dwelling',\n",
        " 501: 'cloak',\n",
        " 502: 'clog, geta, patten, sabot',\n",
        " 503: 'cocktail shaker',\n",
        " 504: 'coffee mug',\n",
        " 505: 'coffeepot',\n",
        " 506: 'coil, spiral, volute, whorl, helix',\n",
        " 507: 'combination lock',\n",
        " 508: 'computer keyboard, keypad',\n",
        " 509: 'confectionery, confectionary, candy store',\n",
        " 510: 'container ship, containership, container vessel',\n",
        " 511: 'convertible',\n",
        " 512: 'corkscrew, bottle screw',\n",
        " 513: 'cornet, horn, trumpet, trump',\n",
        " 514: 'cowboy boot',\n",
        " 515: 'cowboy hat, ten-gallon hat',\n",
        " 516: 'cradle',\n",
        " 517: 'crane',\n",
        " 518: 'crash helmet',\n",
        " 519: 'crate',\n",
        " 520: 'crib, cot',\n",
        " 521: 'Crock Pot',\n",
        " 522: 'croquet ball',\n",
        " 523: 'crutch',\n",
        " 524: 'cuirass',\n",
        " 525: 'dam, dike, dyke',\n",
        " 526: 'desk',\n",
        " 527: 'desktop computer',\n",
        " 528: 'dial telephone, dial phone',\n",
        " 529: 'diaper, nappy, napkin',\n",
        " 530: 'digital clock',\n",
        " 531: 'digital watch',\n",
        " 532: 'dining table, board',\n",
        " 533: 'dishrag, dishcloth',\n",
        " 534: 'dishwasher, dish washer, dishwashing machine',\n",
        " 535: 'disk brake, disc brake',\n",
        " 536: 'dock, dockage, docking facility',\n",
        " 537: 'dogsled, dog sled, dog sleigh',\n",
        " 538: 'dome',\n",
        " 539: 'doormat, welcome mat',\n",
        " 540: 'drilling platform, offshore rig',\n",
        " 541: 'drum, membranophone, tympan',\n",
        " 542: 'drumstick',\n",
        " 543: 'dumbbell',\n",
        " 544: 'Dutch oven',\n",
        " 545: 'electric fan, blower',\n",
        " 546: 'electric guitar',\n",
        " 547: 'electric locomotive',\n",
        " 548: 'entertainment center',\n",
        " 549: 'envelope',\n",
        " 550: 'espresso maker',\n",
        " 551: 'face powder',\n",
        " 552: 'feather boa, boa',\n",
        " 553: 'file, file cabinet, filing cabinet',\n",
        " 554: 'fireboat',\n",
        " 555: 'fire engine, fire truck',\n",
        " 556: 'fire screen, fireguard',\n",
        " 557: 'flagpole, flagstaff',\n",
        " 558: 'flute, transverse flute',\n",
        " 559: 'folding chair',\n",
        " 560: 'football helmet',\n",
        " 561: 'forklift',\n",
        " 562: 'fountain',\n",
        " 563: 'fountain pen',\n",
        " 564: 'four-poster',\n",
        " 565: 'freight car',\n",
        " 566: 'French horn, horn',\n",
        " 567: 'frying pan, frypan, skillet',\n",
        " 568: 'fur coat',\n",
        " 569: 'garbage truck, dustcart',\n",
        " 570: 'gasmask, respirator, gas helmet',\n",
        " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
        " 572: 'goblet',\n",
        " 573: 'go-kart',\n",
        " 574: 'golf ball',\n",
        " 575: 'golfcart, golf cart',\n",
        " 576: 'gondola',\n",
        " 577: 'gong, tam-tam',\n",
        " 578: 'gown',\n",
        " 579: 'grand piano, grand',\n",
        " 580: 'greenhouse, nursery, glasshouse',\n",
        " 581: 'grille, radiator grille',\n",
        " 582: 'grocery store, grocery, food market, market',\n",
        " 583: 'guillotine',\n",
        " 584: 'hair slide',\n",
        " 585: 'hair spray',\n",
        " 586: 'half track',\n",
        " 587: 'hammer',\n",
        " 588: 'hamper',\n",
        " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
        " 590: 'hand-held computer, hand-held microcomputer',\n",
        " 591: 'handkerchief, hankie, hanky, hankey',\n",
        " 592: 'hard disc, hard disk, fixed disk',\n",
        " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
        " 594: 'harp',\n",
        " 595: 'harvester, reaper',\n",
        " 596: 'hatchet',\n",
        " 597: 'holster',\n",
        " 598: 'home theater, home theatre',\n",
        " 599: 'honeycomb',\n",
        " 600: 'hook, claw',\n",
        " 601: 'hoopskirt, crinoline',\n",
        " 602: 'horizontal bar, high bar',\n",
        " 603: 'horse cart, horse-cart',\n",
        " 604: 'hourglass',\n",
        " 605: 'iPod',\n",
        " 606: 'iron, smoothing iron',\n",
        " 607: \"jack-o'-lantern\",\n",
        " 608: 'jean, blue jean, denim',\n",
        " 609: 'jeep, landrover',\n",
        " 610: 'jersey, T-shirt, tee shirt',\n",
        " 611: 'jigsaw puzzle',\n",
        " 612: 'jinrikisha, ricksha, rickshaw',\n",
        " 613: 'joystick',\n",
        " 614: 'kimono',\n",
        " 615: 'knee pad',\n",
        " 616: 'knot',\n",
        " 617: 'lab coat, laboratory coat',\n",
        " 618: 'ladle',\n",
        " 619: 'lampshade, lamp shade',\n",
        " 620: 'laptop, laptop computer',\n",
        " 621: 'lawn mower, mower',\n",
        " 622: 'lens cap, lens cover',\n",
        " 623: 'letter opener, paper knife, paperknife',\n",
        " 624: 'library',\n",
        " 625: 'lifeboat',\n",
        " 626: 'lighter, light, igniter, ignitor',\n",
        " 627: 'limousine, limo',\n",
        " 628: 'liner, ocean liner',\n",
        " 629: 'lipstick, lip rouge',\n",
        " 630: 'Loafer',\n",
        " 631: 'lotion',\n",
        " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
        " 633: \"loupe, jeweler's loupe\",\n",
        " 634: 'lumbermill, sawmill',\n",
        " 635: 'magnetic compass',\n",
        " 636: 'mailbag, postbag',\n",
        " 637: 'mailbox, letter box',\n",
        " 638: 'maillot',\n",
        " 639: 'maillot, tank suit',\n",
        " 640: 'manhole cover',\n",
        " 641: 'maraca',\n",
        " 642: 'marimba, xylophone',\n",
        " 643: 'mask',\n",
        " 644: 'matchstick',\n",
        " 645: 'maypole',\n",
        " 646: 'maze, labyrinth',\n",
        " 647: 'measuring cup',\n",
        " 648: 'medicine chest, medicine cabinet',\n",
        " 649: 'megalith, megalithic structure',\n",
        " 650: 'microphone, mike',\n",
        " 651: 'microwave, microwave oven',\n",
        " 652: 'military uniform',\n",
        " 653: 'milk can',\n",
        " 654: 'minibus',\n",
        " 655: 'miniskirt, mini',\n",
        " 656: 'minivan',\n",
        " 657: 'missile',\n",
        " 658: 'mitten',\n",
        " 659: 'mixing bowl',\n",
        " 660: 'mobile home, manufactured home',\n",
        " 661: 'Model T',\n",
        " 662: 'modem',\n",
        " 663: 'monastery',\n",
        " 664: 'monitor',\n",
        " 665: 'moped',\n",
        " 666: 'mortar',\n",
        " 667: 'mortarboard',\n",
        " 668: 'mosque',\n",
        " 669: 'mosquito net',\n",
        " 670: 'motor scooter, scooter',\n",
        " 671: 'mountain bike, all-terrain bike, off-roader',\n",
        " 672: 'mountain tent',\n",
        " 673: 'mouse, computer mouse',\n",
        " 674: 'mousetrap',\n",
        " 675: 'moving van',\n",
        " 676: 'muzzle',\n",
        " 677: 'nail',\n",
        " 678: 'neck brace',\n",
        " 679: 'necklace',\n",
        " 680: 'nipple',\n",
        " 681: 'notebook, notebook computer',\n",
        " 682: 'obelisk',\n",
        " 683: 'oboe, hautboy, hautbois',\n",
        " 684: 'ocarina, sweet potato',\n",
        " 685: 'odometer, hodometer, mileometer, milometer',\n",
        " 686: 'oil filter',\n",
        " 687: 'organ, pipe organ',\n",
        " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
        " 689: 'overskirt',\n",
        " 690: 'oxcart',\n",
        " 691: 'oxygen mask',\n",
        " 692: 'packet',\n",
        " 693: 'paddle, boat paddle',\n",
        " 694: 'paddlewheel, paddle wheel',\n",
        " 695: 'padlock',\n",
        " 696: 'paintbrush',\n",
        " 697: \"pajama, pyjama, pj's, jammies\",\n",
        " 698: 'palace',\n",
        " 699: 'panpipe, pandean pipe, syrinx',\n",
        " 700: 'paper towel',\n",
        " 701: 'parachute, chute',\n",
        " 702: 'parallel bars, bars',\n",
        " 703: 'park bench',\n",
        " 704: 'parking meter',\n",
        " 705: 'passenger car, coach, carriage',\n",
        " 706: 'patio, terrace',\n",
        " 707: 'pay-phone, pay-station',\n",
        " 708: 'pedestal, plinth, footstall',\n",
        " 709: 'pencil box, pencil case',\n",
        " 710: 'pencil sharpener',\n",
        " 711: 'perfume, essence',\n",
        " 712: 'Petri dish',\n",
        " 713: 'photocopier',\n",
        " 714: 'pick, plectrum, plectron',\n",
        " 715: 'pickelhaube',\n",
        " 716: 'picket fence, paling',\n",
        " 717: 'pickup, pickup truck',\n",
        " 718: 'pier',\n",
        " 719: 'piggy bank, penny bank',\n",
        " 720: 'pill bottle',\n",
        " 721: 'pillow',\n",
        " 722: 'ping-pong ball',\n",
        " 723: 'pinwheel',\n",
        " 724: 'pirate, pirate ship',\n",
        " 725: 'pitcher, ewer',\n",
        " 726: \"plane, carpenter's plane, woodworking plane\",\n",
        " 727: 'planetarium',\n",
        " 728: 'plastic bag',\n",
        " 729: 'plate rack',\n",
        " 730: 'plow, plough',\n",
        " 731: \"plunger, plumber's helper\",\n",
        " 732: 'Polaroid camera, Polaroid Land camera',\n",
        " 733: 'pole',\n",
        " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        " 735: 'poncho',\n",
        " 736: 'pool table, billiard table, snooker table',\n",
        " 737: 'pop bottle, soda bottle',\n",
        " 738: 'pot, flowerpot',\n",
        " 739: \"potter's wheel\",\n",
        " 740: 'power drill',\n",
        " 741: 'prayer rug, prayer mat',\n",
        " 742: 'printer',\n",
        " 743: 'prison, prison house',\n",
        " 744: 'projectile, missile',\n",
        " 745: 'projector',\n",
        " 746: 'puck, hockey puck',\n",
        " 747: 'punching bag, punch bag, punching ball, punchball',\n",
        " 748: 'purse',\n",
        " 749: 'quill, quill pen',\n",
        " 750: 'quilt, comforter, comfort, puff',\n",
        " 751: 'racer, race car, racing car',\n",
        " 752: 'racket, racquet',\n",
        " 753: 'radiator',\n",
        " 754: 'radio, wireless',\n",
        " 755: 'radio telescope, radio reflector',\n",
        " 756: 'rain barrel',\n",
        " 757: 'recreational vehicle, RV, R.V.',\n",
        " 758: 'reel',\n",
        " 759: 'reflex camera',\n",
        " 760: 'refrigerator, icebox',\n",
        " 761: 'remote control, remote',\n",
        " 762: 'restaurant, eating house, eating place, eatery',\n",
        " 763: 'revolver, six-gun, six-shooter',\n",
        " 764: 'rifle',\n",
        " 765: 'rocking chair, rocker',\n",
        " 766: 'rotisserie',\n",
        " 767: 'rubber eraser, rubber, pencil eraser',\n",
        " 768: 'rugby ball',\n",
        " 769: 'rule, ruler',\n",
        " 770: 'running shoe',\n",
        " 771: 'safe',\n",
        " 772: 'safety pin',\n",
        " 773: 'saltshaker, salt shaker',\n",
        " 774: 'sandal',\n",
        " 775: 'sarong',\n",
        " 776: 'sax, saxophone',\n",
        " 777: 'scabbard',\n",
        " 778: 'scale, weighing machine',\n",
        " 779: 'school bus',\n",
        " 780: 'schooner',\n",
        " 781: 'scoreboard',\n",
        " 782: 'screen, CRT screen',\n",
        " 783: 'screw',\n",
        " 784: 'screwdriver',\n",
        " 785: 'seat belt, seatbelt',\n",
        " 786: 'sewing machine',\n",
        " 787: 'shield, buckler',\n",
        " 788: 'shoe shop, shoe-shop, shoe store',\n",
        " 789: 'shoji',\n",
        " 790: 'shopping basket',\n",
        " 791: 'shopping cart',\n",
        " 792: 'shovel',\n",
        " 793: 'shower cap',\n",
        " 794: 'shower curtain',\n",
        " 795: 'ski',\n",
        " 796: 'ski mask',\n",
        " 797: 'sleeping bag',\n",
        " 798: 'slide rule, slipstick',\n",
        " 799: 'sliding door',\n",
        " 800: 'slot, one-armed bandit',\n",
        " 801: 'snorkel',\n",
        " 802: 'snowmobile',\n",
        " 803: 'snowplow, snowplough',\n",
        " 804: 'soap dispenser',\n",
        " 805: 'soccer ball',\n",
        " 806: 'sock',\n",
        " 807: 'solar dish, solar collector, solar furnace',\n",
        " 808: 'sombrero',\n",
        " 809: 'soup bowl',\n",
        " 810: 'space bar',\n",
        " 811: 'space heater',\n",
        " 812: 'space shuttle',\n",
        " 813: 'spatula',\n",
        " 814: 'speedboat',\n",
        " 815: \"spider web, spider's web\",\n",
        " 816: 'spindle',\n",
        " 817: 'sports car, sport car',\n",
        " 818: 'spotlight, spot',\n",
        " 819: 'stage',\n",
        " 820: 'steam locomotive',\n",
        " 821: 'steel arch bridge',\n",
        " 822: 'steel drum',\n",
        " 823: 'stethoscope',\n",
        " 824: 'stole',\n",
        " 825: 'stone wall',\n",
        " 826: 'stopwatch, stop watch',\n",
        " 827: 'stove',\n",
        " 828: 'strainer',\n",
        " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
        " 830: 'stretcher',\n",
        " 831: 'studio couch, day bed',\n",
        " 832: 'stupa, tope',\n",
        " 833: 'submarine, pigboat, sub, U-boat',\n",
        " 834: 'suit, suit of clothes',\n",
        " 835: 'sundial',\n",
        " 836: 'sunglass',\n",
        " 837: 'sunglasses, dark glasses, shades',\n",
        " 838: 'sunscreen, sunblock, sun blocker',\n",
        " 839: 'suspension bridge',\n",
        " 840: 'swab, swob, mop',\n",
        " 841: 'sweatshirt',\n",
        " 842: 'swimming trunks, bathing trunks',\n",
        " 843: 'swing',\n",
        " 844: 'switch, electric switch, electrical switch',\n",
        " 845: 'syringe',\n",
        " 846: 'table lamp',\n",
        " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
        " 848: 'tape player',\n",
        " 849: 'teapot',\n",
        " 850: 'teddy, teddy bear',\n",
        " 851: 'television, television system',\n",
        " 852: 'tennis ball',\n",
        " 853: 'thatch, thatched roof',\n",
        " 854: 'theater curtain, theatre curtain',\n",
        " 855: 'thimble',\n",
        " 856: 'thresher, thrasher, threshing machine',\n",
        " 857: 'throne',\n",
        " 858: 'tile roof',\n",
        " 859: 'toaster',\n",
        " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
        " 861: 'toilet seat',\n",
        " 862: 'torch',\n",
        " 863: 'totem pole',\n",
        " 864: 'tow truck, tow car, wrecker',\n",
        " 865: 'toyshop',\n",
        " 866: 'tractor',\n",
        " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
        " 868: 'tray',\n",
        " 869: 'trench coat',\n",
        " 870: 'tricycle, trike, velocipede',\n",
        " 871: 'trimaran',\n",
        " 872: 'tripod',\n",
        " 873: 'triumphal arch',\n",
        " 874: 'trolleybus, trolley coach, trackless trolley',\n",
        " 875: 'trombone',\n",
        " 876: 'tub, vat',\n",
        " 877: 'turnstile',\n",
        " 878: 'typewriter keyboard',\n",
        " 879: 'umbrella',\n",
        " 880: 'unicycle, monocycle',\n",
        " 881: 'upright, upright piano',\n",
        " 882: 'vacuum, vacuum cleaner',\n",
        " 883: 'vase',\n",
        " 884: 'vault',\n",
        " 885: 'velvet',\n",
        " 886: 'vending machine',\n",
        " 887: 'vestment',\n",
        " 888: 'viaduct',\n",
        " 889: 'violin, fiddle',\n",
        " 890: 'volleyball',\n",
        " 891: 'waffle iron',\n",
        " 892: 'wall clock',\n",
        " 893: 'wallet, billfold, notecase, pocketbook',\n",
        " 894: 'wardrobe, closet, press',\n",
        " 895: 'warplane, military plane',\n",
        " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
        " 897: 'washer, automatic washer, washing machine',\n",
        " 898: 'water bottle',\n",
        " 899: 'water jug',\n",
        " 900: 'water tower',\n",
        " 901: 'whiskey jug',\n",
        " 902: 'whistle',\n",
        " 903: 'wig',\n",
        " 904: 'window screen',\n",
        " 905: 'window shade',\n",
        " 906: 'Windsor tie',\n",
        " 907: 'wine bottle',\n",
        " 908: 'wing',\n",
        " 909: 'wok',\n",
        " 910: 'wooden spoon',\n",
        " 911: 'wool, woolen, woollen',\n",
        " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
        " 913: 'wreck',\n",
        " 914: 'yawl',\n",
        " 915: 'yurt',\n",
        " 916: 'web site, website, internet site, site',\n",
        " 917: 'comic book',\n",
        " 918: 'crossword puzzle, crossword',\n",
        " 919: 'street sign',\n",
        " 920: 'traffic light, traffic signal, stoplight',\n",
        " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
        " 922: 'menu',\n",
        " 923: 'plate',\n",
        " 924: 'guacamole',\n",
        " 925: 'consomme',\n",
        " 926: 'hot pot, hotpot',\n",
        " 927: 'trifle',\n",
        " 928: 'ice cream, icecream',\n",
        " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
        " 930: 'French loaf',\n",
        " 931: 'bagel, beigel',\n",
        " 932: 'pretzel',\n",
        " 933: 'cheeseburger',\n",
        " 934: 'hotdog, hot dog, red hot',\n",
        " 935: 'mashed potato',\n",
        " 936: 'head cabbage',\n",
        " 937: 'broccoli',\n",
        " 938: 'cauliflower',\n",
        " 939: 'zucchini, courgette',\n",
        " 940: 'spaghetti squash',\n",
        " 941: 'acorn squash',\n",
        " 942: 'butternut squash',\n",
        " 943: 'cucumber, cuke',\n",
        " 944: 'artichoke, globe artichoke',\n",
        " 945: 'bell pepper',\n",
        " 946: 'cardoon',\n",
        " 947: 'mushroom',\n",
        " 948: 'Granny Smith',\n",
        " 949: 'strawberry',\n",
        " 950: 'orange',\n",
        " 951: 'lemon',\n",
        " 952: 'fig',\n",
        " 953: 'pineapple, ananas',\n",
        " 954: 'banana',\n",
        " 955: 'jackfruit, jak, jack',\n",
        " 956: 'custard apple',\n",
        " 957: 'pomegranate',\n",
        " 958: 'hay',\n",
        " 959: 'carbonara',\n",
        " 960: 'chocolate sauce, chocolate syrup',\n",
        " 961: 'dough',\n",
        " 962: 'meat loaf, meatloaf',\n",
        " 963: 'pizza, pizza pie',\n",
        " 964: 'potpie',\n",
        " 965: 'burrito',\n",
        " 966: 'red wine',\n",
        " 967: 'espresso',\n",
        " 968: 'cup',\n",
        " 969: 'eggnog',\n",
        " 970: 'alp',\n",
        " 971: 'bubble',\n",
        " 972: 'cliff, drop, drop-off',\n",
        " 973: 'coral reef',\n",
        " 974: 'geyser',\n",
        " 975: 'lakeside, lakeshore',\n",
        " 976: 'promontory, headland, head, foreland',\n",
        " 977: 'sandbar, sand bar',\n",
        " 978: 'seashore, coast, seacoast, sea-coast',\n",
        " 979: 'valley, vale',\n",
        " 980: 'volcano',\n",
        " 981: 'ballplayer, baseball player',\n",
        " 982: 'groom, bridegroom',\n",
        " 983: 'scuba diver',\n",
        " 984: 'rapeseed',\n",
        " 985: 'daisy',\n",
        " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
        " 987: 'corn',\n",
        " 988: 'acorn',\n",
        " 989: 'hip, rose hip, rosehip',\n",
        " 990: 'buckeye, horse chestnut, conker',\n",
        " 991: 'coral fungus',\n",
        " 992: 'agaric',\n",
        " 993: 'gyromitra',\n",
        " 994: 'stinkhorn, carrion fungus',\n",
        " 995: 'earthstar',\n",
        " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
        " 997: 'bolete',\n",
        " 998: 'ear, spike, capitulum',\n",
        " 999: 'toilet tissue, toilet paper, bathroom tissue'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i98eVjcL4S3e"
      },
      "source": [
        "df_off_resnet = pd.read_csv('/content/Office_Topic 2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuuTI49P4hmr"
      },
      "source": [
        "df_off_resnet['Object'] = df_off_resnet['Object'].map(imagenet) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM05J5fe4_H1"
      },
      "source": [
        "df_off_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwRds9D55kK_"
      },
      "source": [
        "def create_plot_topics(filename,place_name,topic_name):\n",
        "  df_off_resnet = pd.read_csv(filename)\n",
        "  df_off_resnet['Object'] = df_off_resnet['Object'].map(imagenet) \n",
        "  topic = topic_name\n",
        "  place = place_name\n",
        "  topic = topic.replace(' ','')\n",
        "  image_name = place+'_'+topic+'.png'\n",
        "  df_off_resnet.Object.value_counts()[:10].sort_values().plot(kind = 'barh')\n",
        "  plt.xlabel('Number of Images')\n",
        "  plt.ylabel('Objects')\n",
        "  plt.title('Top 10 Objects highlighted by '+topic+' Layer for Place: '+place)\n",
        "  #plt.xticks([])\n",
        "  #plt.show()\n",
        "  plt.savefig(image_name,bbox_inches=\"tight\")\n",
        "  files.download(image_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv5JWVOJ8bVE"
      },
      "source": [
        "create_plot_topics('/content/Transportation_Topic 1.csv','Transportation','Topic 1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnIuMVpG9IEe"
      },
      "source": [
        "files.download('/content/Office_Topic 3.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-uyjpEtPKDg"
      },
      "source": [
        "dat = ['Noise']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KBkRc-nOa08"
      },
      "source": [
        "c = pd.read_csv('/content/eval (2).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1heGwWvOjIi"
      },
      "source": [
        "b = pd.read_csv('/content/val_updated (2).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuc-bwkdOoXR"
      },
      "source": [
        "a = pd.read_csv('/content/test_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX6_PPZxO5Fl"
      },
      "source": [
        "a = a[~a['Label'].isin(dat)]\n",
        "a.to_csv('test_nn_final.csv')\n",
        "files.download('test_nn_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSWr1Bb3PfQQ"
      },
      "source": [
        "a[a['Label']=='Noise']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwTY2NFIOkqy"
      },
      "source": [
        "a = pd.read_csv('/content/test_nn_final.csv')\n",
        "len(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQvVD1JI7ptS"
      },
      "source": [
        "#Creating a collage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3OqcugI8FOk"
      },
      "source": [
        "content_list = os.listdir('/content')\n",
        "image_list = []\n",
        "for content in content_list:\n",
        "  if content.endswith('.jpg'):\n",
        "    image_list.append(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBwkIfSE93HR"
      },
      "source": [
        "image_list = sorted(image_list,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONwHFz0d-HVN"
      },
      "source": [
        "imshow(image_list[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxzuyWdJ-hep"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_multiple_img(images, rows = 1, cols=1):\n",
        "    figure, ax = plt.subplots(nrows=rows,ncols=cols )\n",
        "    for ind,title in enumerate(images):\n",
        "        ax.ravel()[ind].imshow(images[title])\n",
        "        ax.ravel()[ind].set_title(title)\n",
        "        ax.ravel()[ind].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSoWc3DAa89"
      },
      "source": [
        "for im in image_list:\n",
        "  image = Image.open(im)\n",
        "\n",
        "  x,y = image.size\n",
        "  new_dimensions = (224, 224) #dimension set here\n",
        "  output = image.resize(new_dimensions, Image.ANTIALIAS)\n",
        "\n",
        "  output_file_name = os.path.join('/content/small',\"small_\" + im)\n",
        "  output.save(output_file_name, \"JPEG\", quality = 95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqplqjp7DM6Q"
      },
      "source": [
        "small  = os.listdir('/content/small')\n",
        "small = '/content/small'+small\n",
        "for sm in small:\n",
        "  files.download(sm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLs27C2PDZwz"
      },
      "source": [
        "# Using TFDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2L4b5J0FIV"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w1 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\"),trainable=True,name = 'weight_topic_1',initializer = 'random_normal'\n",
        "    \n",
        "   \n",
        "    self.w2 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_2',initializer = 'random_normal'\n",
        "    )  \n",
        "    self.w3 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_3',initializer = 'random_normal'\n",
        "    )       \n",
        "    self.w4 = self.add_weight(\n",
        "        shape=(self.units), dtype=\"float32\",trainable=True,name = 'weight_topic_4',initializer = 'random_normal'\n",
        "    )\n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #name = 'tops'\n",
        "    return tf.multiply(input2,self.w1) + tf.multiply(input1, self.w2) + tf.multiply(input3, self.w3)+tf.multiply(input4, self.w4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItXdH4nMKj8H"
      },
      "source": [
        "class Wt_Topics(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Wt_Topics, self).__init__()\n",
        "    self.units = units \n",
        "    # self.name = name      \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    \n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w1 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_1'\n",
        "    )\n",
        "    self.w2 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_2'\n",
        "    )  \n",
        "    self.w3 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_3'\n",
        "    )       \n",
        "    self.w4 = tf.Variable(\n",
        "        initial_value=w_init(shape=(1, self.units), dtype=\"float32\"),\n",
        "        trainable=True,name = 'weight_topic_4'\n",
        "    )\n",
        "  def call(self, input1, input2, input3,input4):\n",
        "    #name = 'tops'\n",
        "    return tf.multiply(input1,self.w1) + tf.multiply(input2, self.w2) + tf.multiply(input3, self.w3)+tf.multiply(input4, self.w4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADrozJBnSq48"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffUIWfg481I9"
      },
      "source": [
        "class Test_Topics(keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Test_Topics, self).__init__()\n",
        "        self.units = units\n",
        "    def build(self,input_shape):\n",
        "        self.w1 = self.add_weight(name='multiply_weight_1', shape=(self.units,),\n",
        "                                  trainable=True,initializer='random_normal')\n",
        "        self.w2 = self.add_weight(name='multiply_weight_2', shape=(self.units,), \n",
        "                                  trainable=True,initializer='random_normal')\n",
        "        self.w3 = self.add_weight(name='multiply_weight_3', shape=(self.units,), \n",
        "                                  trainable=True,initializer='random_normal')\n",
        "        self.w4 = self.add_weight(name='multiply_weight_4', shape=(self.units,), \n",
        "                                  trainable=True,initializer='random_normal')\n",
        "        \n",
        "    def call(self, input1,input2,input3,input4):\n",
        "        #Weights taken individually  \n",
        "        return tf.multiply(input1, self.w1),tf.multiply(input2, self.w2),tf.multiply(input3, self.w3),tf.multiply(input4, self.w4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8EoRLvINR6u"
      },
      "source": [
        "class Topics_New(keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Topics_New, self).__init__()\n",
        "        self.units = units\n",
        "    def build(self,input_shape):\n",
        "        self.w = self.add_weight(name='multiply_weight', shape=(1,self.units), trainable=True,initializer='random_normal')\n",
        "        \n",
        "        #b_init = tf.zeros_initializer()\n",
        "        #self.b = tf.Variable(\n",
        "        #    initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        #)\n",
        "        #self.add = tf.keras.layers.Add()\n",
        "    def call(self, inputs):\n",
        "          #topic = []\n",
        "          #for input in inputs:\n",
        "          #  weights = tf.multiply(input, self.w)\n",
        "          #  topic.append(weights) #+ self.b)\n",
        "        \n",
        "        return tf.keras.layers.Multiply()([inputs,self.w])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XtifS7MZSO_"
      },
      "source": [
        "#Using the output from the resnet model in order to create topics\n",
        "#Defining the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(1024,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "\n",
        "#predictions = Dense(26,activation='softmax')(x)\n",
        "#Using softmax for base classification\n",
        "#model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "wt_add = Test_Topics(1)\n",
        "sum_layer = wt_add(model.output,model.output,model.output,model.output)\n",
        "\n",
        "conc = keras.layers.Concatenate()(sum_layer)\n",
        "#flat = keras.layers.GlobalAveragePooling1D()(conc)\n",
        "#test = keras.layers.GlobalAveragePooling1D()(flat)\n",
        "flat_2 = keras.layers.Flatten(name = 'flat_2')(conc)\n",
        "topic = Dense(128,activation='relu')(flat_2)\n",
        "topic = Dropout(0.2)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)\n",
        "\n",
        "\n",
        "cent_model_c = Model(inputs = base_model.input,outputs = final)\n",
        "opti = tf.keras.optimizers.Adadelta(learning_rate=0.01, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "cent_model_c.compile(optimizer=opti,metrics=[tf.keras.metrics.CategoricalAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()],loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhcIn3RZN8LR"
      },
      "source": [
        "wt_add = Test_Topics(4)([k_1,k_2,k_3,k_4])\n",
        "wt_add.shape\n",
        "tf.reshape(wt_add, shape=[tf.shape(wt_add)[0]*tf.shape(wt_add)[1],4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W20MQlNqOBNC"
      },
      "source": [
        "\n",
        "#  np.random.randn(cent_model_c.layers[-6].get_weights().shape())\n",
        "# np.random.randn(*self.model.layers[layer_index].get_weights().shape\n",
        "#weights_shape = cent_model_c.layers[-6].get_weights()\n",
        "a = [np.random.rand(1),np.random.rand(1),np.random.rand(1),np.random.rand(1)]\n",
        "#weights_shape\n",
        "#a\n",
        "#cent_model_c.layers[-6].set_weights(a)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NguFTVzrmi8"
      },
      "source": [
        "cent_model_c.layers[-6].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR_oNhxIqnzN"
      },
      "source": [
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mJ1vPYNuDWm"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    cent_model_c, to_file='/content/plot_model_2.png', show_shapes=True, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        ")\n",
        "\n",
        "#plot_model(cent_model_c, to_file='/content/plot/plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLO4cJfo4ji1"
      },
      "source": [
        "cent_model_c.layers[-6].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-5Ttm2QDfho"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApzlD98Ogc3h"
      },
      "source": [
        "file_name = 'categories_places365.txt'\n",
        "if not os.access(file_name, os.W_OK):\n",
        "    synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "    os.system('wget ' + synset_url)\n",
        "classes = list()\n",
        "with open(file_name) as class_file:\n",
        "    for line in class_file:\n",
        "        classes.append(line.strip().split(' ')[0][3:])\n",
        "classes = tuple(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suglxRkDka-l"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcCaiEQD3CX"
      },
      "source": [
        "train_df,info = tfds.load('oxford_flowers102',split='train',shuffle_files=True,with_info=True,as_supervised=True)\n",
        "val_df,info = tfds.load('oxford_flowers102',split='validation',shuffle_files=True,with_info=True,as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re5qPuFgGtYl"
      },
      "source": [
        "for image,label in train_df.take(1):\n",
        "  print(image)\n",
        "  print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odCEBKLTE2RY"
      },
      "source": [
        "size = (224,224)\n",
        "train_df = train_df.map(lambda x,y:(tf.image.resize(x,size),tf.one_hot(y,102)))\n",
        "val_df = val_df.map(lambda x, y: (tf.image.resize(x, size), tf.one_hot(y,102)))\n",
        "# info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrFBkNYNGsrA"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lf_9HMnlPkT"
      },
      "source": [
        "# a = tf.keras.utils.to_categorical(, num_classes=4)\n",
        "train_df = train_df.map(lambda x,y: (tf.cast(x/255.,tf.float32),y))\n",
        "val_df = val_df.map(lambda x,y: (tf.cast(x/255.,tf.float32),y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vhU8yzmknSg"
      },
      "source": [
        "for image,labels in train_df.take(1):\n",
        "  print(image)\n",
        "  #labels = tf.one_hot(labels,depth=len(classes))\n",
        "  #print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqIHy-5UTBD"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_ds = train_df.cache('cache_t').batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_df.cache('cache_v').batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivojxW4afcvL"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3yR5EKka0au"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcnxFIm6KOzX"
      },
      "source": [
        "#if not isfile(json_log):\n",
        "\n",
        "json_log = open('topic_k9.json', mode='wt', buffering=1)\n",
        "callbacks = [\n",
        "  ModelCheckpoint(\n",
        "    \n",
        "    filepath='/content/Flowers102',\n",
        "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "    monitor=\"categorical_accuracy\",\n",
        "    verbose=1,\n",
        "    save_freq = 'epoch',mode = 'max'\n",
        "  ),\n",
        "  EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "  ),\n",
        "  ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                          patience=3, min_lr=1e-5,verbose=1),\n",
        "  \n",
        "  LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: json_log.write(\n",
        "                json.dumps({'epoch': epoch, \n",
        "                            'categorical_accuracy': logs['categorical_accuracy'],\n",
        "                            'loss': logs['loss'],\n",
        "                            'topic_layer_weights': list(cent_model_c.layers[-6].get_weights())},cls = NumpyArrayEncoder) + '\\n'),\n",
        "            on_train_end=lambda logs: json_log.close()\n",
        "  )\n",
        "\n",
        "         \n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBk_4SmzqpHM"
      },
      "source": [
        "wt_add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJrzeXL4bsCQ"
      },
      "source": [
        "cent_model_c.layers[-8].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnXWX9VpzrQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCQlPV6acB52"
      },
      "source": [
        "# Create new model on top\n",
        "#cant_model_c = keras.models.load_model('/content/Flowers102')\n",
        "history = cent_model_c.fit(train_ds,epochs=10,verbose = 1,validation_data=val_ds,use_multiprocessing=False,workers = 2,callbacks = callbacks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGGdFOxwWRER"
      },
      "source": [
        "cent_model_c.save('/content/Flowers102')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_8gBH75GDUr"
      },
      "source": [
        "tfds.list_builders(\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qCSehk4_zPO"
      },
      "source": [
        "for l in cent_model_c.layers:\n",
        "    print(l.name, l.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJUao7YBYqt"
      },
      "source": [
        "cent_model_c.layers[-4].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTq1gZJ1vQc"
      },
      "source": [
        "import json\n",
        "# file = \n",
        "\n",
        "# \n",
        "test = []\n",
        "for line in open('/content/json_now_cw_tk4_fds.json','r'):\n",
        "    test.append(json.loads(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4BtUGz43J1I"
      },
      "source": [
        "data = [json.loads(line) for line in open('/content/json_now_cw_tk4_fds.json', 'r')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMKOP4Kg6yVv"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jGKfbuq4KgZ"
      },
      "source": [
        "test_dict = {}\n",
        "test_2 = []\n",
        "for elements in data:\n",
        "  test_dict.update(elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ6sI9Uo5fbD"
      },
      "source": [
        "from functools import reduce\n",
        "abc = reduce(lambda a, b: dict(a, **b), data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w6eF_Ye8WWs"
      },
      "source": [
        "d = {k:v for x in data for k,v in x.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9g25MG6-Y8O"
      },
      "source": [
        "import pandas as pd\n",
        "df_data = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7uElTKs_9B7"
      },
      "source": [
        "text = df_data.head(1)\n",
        "# for el in text:\n",
        "#   for els in el:\n",
        "#     print(els)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv-WajfUAyUx"
      },
      "source": [
        "abc = df_data['topic_layer_weights']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmbJRY7DKYA"
      },
      "source": [
        "pd.DataFrame(abc)\n",
        "abc = abc.map(lambda x:str(x).replace('[[',''))\n",
        "abc = abc.map(lambda x:str(x).replace(']]',''))\n",
        "abc = abc.map(lambda x:str(x).replace('[',''))\n",
        "abc = abc.map(lambda x:str(x).replace(']',''))\n",
        "\n",
        "abc = abc.str.split(',',expand=True)\n",
        "\n",
        "abc.columns = ['Topic 1','Topic 2','Topic 3','Topic 4']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDl3h6nEElzV"
      },
      "source": [
        "abc.columns = ['Topic 1','Topic 2','Topic 3','Topic 4']\n",
        "abc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "colC9r80HASk"
      },
      "source": [
        "testing = pd.merge(df_data,abc,how='inner',on=df_data.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sntbPpJsHdDp"
      },
      "source": [
        "final_data = testing[['batch','categorical_accuracy','loss','Topic 1','Topic 2','Topic 3','Topic 4']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpM28r9NIX6B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZOdxC_ItMX"
      },
      "source": [
        "##Designing a function to convert JSON to a dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLocIGQxXJG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8287jCkKCr6"
      },
      "source": [
        "def topic_col_to_df(frame):\n",
        "  pd.DataFrame(frame)\n",
        "  frame = frame.map(lambda x:str(x).replace('[[',''))\n",
        "  frame = frame.map(lambda x:str(x).replace(']]',''))\n",
        "  frame = frame.map(lambda x:str(x).replace('[',''))\n",
        "  frame = frame.map(lambda x:str(x).replace(']',''))\n",
        "  frame = frame.map(lambda x:str(x).replace(']]]',''))\n",
        "  frame = frame.map(lambda x:str(x).replace('[[[',''))\n",
        "\n",
        "  frame = frame.str.split(',',expand=True)\n",
        "\n",
        "  frame.columns = ['Topic 1','Topic 2','Topic 3','Topic 4']\n",
        "  return frame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNGiyn17Ix_a"
      },
      "source": [
        "def make_topic_df(filename):\n",
        "  data = [json.loads(line) for line in open(filename, 'r')]\n",
        "  df_data = pd.DataFrame(data)\n",
        "  df_data['epoch'] = df_data['epoch']+1 \n",
        "  topic_column = df_data['topic_layer_weights']\n",
        "  topic_df = topic_col_to_df(topic_column)\n",
        "  testing = pd.merge(df_data,topic_df,how='inner',on=df_data.index)\n",
        "  final_data = testing[['epoch','categorical_accuracy','loss','Topic 1','Topic 2','Topic 3','Topic 4']]\n",
        "  return final_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_5ElkcHKlBP"
      },
      "source": [
        "final_data = make_topic_df('/content/MODEL_1_INIT.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8x2-GDHXARy"
      },
      "source": [
        "def plot_topics(df):\n",
        "  topic_list = ['Topic 1','Topic 2','Topic 3','Topic 4']\n",
        "  for topic in topic_list:\n",
        "    weight = []\n",
        "    #Topic Weights\n",
        "    y = df[topic].tolist()\n",
        "    x = df['epoch'].tolist()\n",
        "    for el in y:\n",
        "      \n",
        "      weight.append(float(el))\n",
        "    ax = sn.barplot(x=x, y=weight, data=df)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Topic Weight Value\")\n",
        "    plt.title(\"Weight Distribution for \"+topic)\n",
        "    plt.savefig('weights_'+topic+'.png')#Place in plot dir plt.savefig(os.path.join(plot_dir,'weights_'+topic+'.png')) \n",
        "    plt.close()\n",
        "    print(topic+' Weights plotted...')\n",
        "    #Accuracy\n",
        "    \n",
        "    plt.plot(df['categorical_accuracy'])\n",
        "    plt.ylabel('Categorical Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title('Accuracy with respect to '+topic)\n",
        "    plt.savefig('acc_'+topic+'.png')#Place in plot dir plt.savefig(os.path.join(plot_dir,'acc_'+topic+'.png'))\n",
        "    plt.close()\n",
        "    print(topic+' Accuracy plotted...')\n",
        "    #Loss\n",
        "    plt.plot(df['loss'])\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title('Loss with respect to '+topic)\n",
        "    plt.savefig('loss_'+topic+'.png')#Place in plot dir  plt.savefig(os.path.join(plot_dir,'loss_'+topic+'.png'))\n",
        "    plt.close()\n",
        "    print(topic+' Loss plotted...')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXWyWPr7sYNS"
      },
      "source": [
        "plot_topics(final_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlzAZghLh3W"
      },
      "source": [
        "plt.plot(final_data['categorical_accuracy'])\n",
        "plt.plot(final_data['loss'], color='g', linestyle='--')\n",
        "plt.plot(read_test['categorical_accuracy'])\n",
        "plt.title('model accuracy for fold_'+str(get_fold))\n",
        "plt.ylabel('Categorical Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(os.path.join(folder_path,'accuracy_'+str(history_path)+'.png'))\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zt-DVDlNhrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vmpZnmhOWxZ"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "#sns.set_style('darkgrid')\n",
        "topic_list = ['Topic 1','Topic 2','Topic 3','Topic 4']\n",
        "for topic in topic_list:\n",
        "  weight = []\n",
        "  y = final_data[topic].tolist()\n",
        "  x = final_data['epoch'].tolist()\n",
        "  for el in y:\n",
        "    \n",
        "    weight.append(float(el))\n",
        "  ax = sns.barplot(x=x, y=weight, data=final_data)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Topic Weight Value\")\n",
        "  plt.title(\"Weight Distribution for \"+topic)\n",
        "  plt.savefig('weights_'+topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbcVe_rtXPe"
      },
      "source": [
        "plt.plot(final_data['categorical_accuracy'])\n",
        "#plt.plot(final_data['loss'])\n",
        "plt.ylabel('Categorical Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Accuracy with respect to Topic 1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfmILDUSTec"
      },
      "source": [
        "#test_df = final_data.iloc[:,:]\n",
        "weight = []\n",
        "y = final_data['Topic 1'].tolist()\n",
        "x = final_data['epoch'].tolist()\n",
        "for el in y:\n",
        "  # el = \"{:.7f}\".format(el)\n",
        "  weight.append(float(el))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F58tUkTQ7IT_"
      },
      "source": [
        "#Testing stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxDG9kdv7Nm0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acry3ZBF40BN"
      },
      "source": [
        "##Scrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlWIdiq6_mb9"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = AveragePooling2D(pool_size=(7,7))(x)\n",
        "x = Flatten(name =\"flatten\")(x)\n",
        "x = Dense(128,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "preds = Dense(1000,activation=\"softmax\")(x)\n",
        "\n",
        "#Activating the model\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = Model(inputs = base_model.input,outputs = preds)\n",
        "opti = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.01,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "model.compile(optimizer=opti,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=[keras.metrics.Precision(),keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzRS-CTeAPUN"
      },
      "source": [
        "data = np.array(Image.open('/content/drive/MyDrive/images_routine_users/user_01/user1_01/20171201_115957_000.jpg'))\n",
        "data.resize(224,224,3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ-9dChJDFQ_"
      },
      "source": [
        "img_pil = array_to_img(data)\n",
        "img_arr = img_to_array(img_pil)\n",
        "img_arr = img_arr.reshape((1,224,224,3))\n",
        "predictions = model.predict(img_arr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEZk56vP_BL"
      },
      "source": [
        "def create_topics(predictions):\n",
        "  topics = decode_predictions(predictions,top = 1000)\n",
        "  t = pd.DataFrame(topics)\n",
        "  lis = []\n",
        "  for top in t:\n",
        "    a,b,c = t[top].explode()\n",
        "    lis.append(b)\n",
        "  return lis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfBPxWmkQKfc"
      },
      "source": [
        "tops = []\n",
        "topss = []\n",
        "for i in range(0,4):\n",
        "  predictions = model.predict(img_arr)\n",
        "  tops = create_topics(predictions)\n",
        "  topss.append(tops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaoBd_UrTd1S"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-SntYwRWgJ"
      },
      "source": [
        "y = model.output\n",
        "k_1 = Dense(1000,name = 'topic_1')(y)\n",
        "k_2 = Dense(1000,name = 'topic_2')(y)\n",
        "k_3 = Dense(1000,name = 'topic_3')(y)\n",
        "k_4 = Dense(1000,name = 'topic_4')(y)\n",
        "conc2 = Concatenate(name = 'conc_c')([k_1,k_2,k_3,k_4])\n",
        "flat_2 = Flatten(name = 'flat_c')(conc2)\n",
        "#Adding the Dense layers now\n",
        "topic = Dense(4000,activation='relu')(flat_2)\n",
        "topic = Dropout(0.5)(topic)\n",
        "topic = Dense(4000,activation='relu')(topic)\n",
        "topic = Dropout(0.5)(topic)\n",
        "final = Dense(24,activation='softmax')(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ji0efbSNYnM"
      },
      "source": [
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQukb2ybu6i"
      },
      "source": [
        "cent_model_c = Model(inputs = base_model.input,outputs = final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuCKCeavc3j8"
      },
      "source": [
        "model.compile(optimizer=opti,loss='categorical_crossentropy',metrics=[keras.metrics.Precision(),keras.metrics.CategoricalAccuracy()])\n",
        "cent_model_c.compile(optimizer=opti,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=[tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.Precision()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YqL8DEYvCpJ"
      },
      "source": [
        "layer = cent_model_c.get_layer('topic_1')\n",
        "weights = layer.get_weights()\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXHxsSxJupvu"
      },
      "source": [
        "cent_model_c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV5EJ3zddyng"
      },
      "source": [
        "predi = cent_model_c.predict(img_arr)\n",
        "pre = np.argmax(predi)\n",
        "pre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh2OudkNDQh6"
      },
      "source": [
        "%pip install keras-metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qImZCx0iTNfF"
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzZ5bX9-TZds"
      },
      "source": [
        "train_dir = '/content/gdrive/MyDrive/data/train'\n",
        "val_dir = '/content/gdrive/MyDrive/data/val'\n",
        "test_dir = '/content/gdrive/MyDrive/data/test'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D90-nfMATFn6"
      },
      "source": [
        "\n",
        "\n",
        "#getting the number of files in train and val dataset\n",
        "train_num_files=len([file for file in glob(str(train_dir + '/*/*'))])\n",
        "val_num_files=len([file for file in glob(str(val_dir + '/*/*'))])\n",
        "print(\"No. of files in Train folder: \",train_num_files)\n",
        "print(\"No. of files in Val folder: \",val_num_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0AIxS5EbQkV"
      },
      "source": [
        "#Try using image data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "train = datagen.flow_from_directory('/content/gdrive/MyDrive/data/train',target_size=(224,224),color_mode='rgb',batch_size=151,shuffle=True,class_mode='categorical',seed=42)\n",
        "STEP_SIZE_TRAIN = train.n//train.batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ7i3nVrcmZF"
      },
      "source": [
        "images, labels = next(datagen.flow_from_directory('/content/gdrive/MyDrive/data/train',target_size=(224,224),color_mode='rgb',batch_size=151,shuffle=True,class_mode='categorical',seed=42))\n",
        "\n",
        "#try_img = tf.data.Dataset.from_generator(datagen.flow_from_directory(lambda x:'/content/gdrive/MyDrive/data/train',target_size=(224,224),color_mode='rgb',batch_size=151,shuffle=True,class_mode='categorical',seed=42))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicxOgJge0z9"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((next(datagen.flow_from_directory('/content/gdrive/MyDrive/data/train',target_size=(224,224),color_mode='rgb',batch_size=64,shuffle=True,class_mode='categorical',seed=42))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}